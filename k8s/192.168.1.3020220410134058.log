=~=~=~=~=~=~=~=~=~=~=~= PuTTY log 2022.04.10 13:40:58 =~=~=~=~=~=~=~=~=~=~=~=
login as: root
root@192.168.1.30's password: 
Last login: Fri Apr  8 20:49:51 2022 from 192.168.1.104
]0;root@localhost:~[?1034h[root@localhost ~]# ll
total 172
-rw-------.  1 root root  2227 Mar 21 19:14 anaconda-ks.cfg
-rwx------.  1 root root 11156 Mar 23 22:44 [0m[01;32mget_helm.sh[0m
-rw-r--r--.  1 root root  2320 Mar 21 19:43 initial-setup-ks.cfg
-rw-r--r--.  1 root root  2928 Apr  8 16:47 kube-metricbeat.yml
-rw-r--r--.  1 root root 93490 Mar 22 23:05 kube-prometheus-stack.yml
drwxr-xr-x. 31 root root  4096 Mar 21 21:21 [01;34mkubernetes[0m
drwxr-xr-x.  9 root root  4096 Mar 21 14:30 [01;34mlocal-repo[0m
-rw-r--r--.  1 root root  7565 Apr  8 21:00 Metricbeat-kube-not.yml
-rw-r--r--.  1 root root  9027 Apr  8 21:28 metricbeat-kubernetes.yaml
-rw-r--r--.  1 root root   104 Mar 20 14:31 mongo-configmap.yaml
-rw-r--r--.  1 root root  1114 Mar 20 14:32 mongo-express.yaml
-rw-r--r--.  1 root root   158 Mar 20 14:32 mongo-secret.yaml
-rw-r--r--.  1 root root   862 Mar 20 14:31 mongo.yaml
drwxr-xr-x.  4 root root  4096 Mar 21 21:26 [01;34mprome-grafana[0m
drwxr-xr-x.  4 root root  4096 Mar 21 22:01 [01;34mPrometheus-Grafana[0m
-rw-r--r--.  1 root root  4077 Apr  8 22:23 values-metricbeat.yaml
]0;root@localhost:~[root@localhost ~]# kube [Kctl get po
No resources found in default namespace.
]0;root@localhost:~[root@localhost ~]# kubectl cerate metricbeat-kubernetes.yaml[C metricbeat-kubernetes.yaml- metricbeat-kubernetes.yamlf metricbeat-kubernetes.yaml[C metricbeat-kubernetes.yaml
Error: unknown command "cerate" for "kubectl"

Did you mean this?
	create

Run 'kubectl --help' for usage.
unknown command "cerate" for "kubectl"

Did you mean this?
	create

]0;root@localhost:~[root@localhost ~]# kubectl cerate -f  metricbeat-kubernetes.yaml[1P[1P[1@r[1@e
configmap/metricbeat-daemonset-config created
configmap/metricbeat-daemonset-modules created
daemonset.apps/metricbeat created
clusterrolebinding.rbac.authorization.k8s.io/metricbeat created
rolebinding.rbac.authorization.k8s.io/metricbeat created
rolebinding.rbac.authorization.k8s.io/metricbeat-kubeadm-config created
clusterrole.rbac.authorization.k8s.io/metricbeat created
role.rbac.authorization.k8s.io/metricbeat created
role.rbac.authorization.k8s.io/metricbeat-kubeadm-config created
serviceaccount/metricbeat created
]0;root@localhost:~[root@localhost ~]# kubectl create -f  metricbeat-kubernetes.yaml[1P[1P[1P[1P[1P[1P[1P[1@d[1@e[1@l[1@e[1@t[1@e[1@ 
configmap "metricbeat-daemonset-config" deleted
configmap "metricbeat-daemonset-modules" deleted
daemonset.apps "metricbeat" deleted
clusterrolebinding.rbac.authorization.k8s.io "metricbeat" deleted
rolebinding.rbac.authorization.k8s.io "metricbeat" deleted
rolebinding.rbac.authorization.k8s.io "metricbeat-kubeadm-config" deleted
clusterrole.rbac.authorization.k8s.io "metricbeat" deleted
role.rbac.authorization.k8s.io "metricbeat" deleted
role.rbac.authorization.k8s.io "metricbeat-kubeadm-config" deleted
serviceaccount "metricbeat" deleted
]0;root@localhost:~[root@localhost ~]# rm -rf metricbeat-kubernetes.yaml
]0;root@localhost:~[root@localhost ~]# rm -rf [K[K[K[K[K[K[Kmv kube-metricbeat.yml /home/dhana/
]0;root@localhost:~[root@localhost ~]# ll
total 156
-rw-------.  1 root root  2227 Mar 21 19:14 anaconda-ks.cfg
-rwx------.  1 root root 11156 Mar 23 22:44 [0m[01;32mget_helm.sh[0m
-rw-r--r--.  1 root root  2320 Mar 21 19:43 initial-setup-ks.cfg
-rw-r--r--.  1 root root 93490 Mar 22 23:05 kube-prometheus-stack.yml
drwxr-xr-x. 31 root root  4096 Mar 21 21:21 [01;34mkubernetes[0m
drwxr-xr-x.  9 root root  4096 Mar 21 14:30 [01;34mlocal-repo[0m
-rw-r--r--.  1 root root  7565 Apr  8 21:00 Metricbeat-kube-not.yml
-rw-r--r--.  1 root root   104 Mar 20 14:31 mongo-configmap.yaml
-rw-r--r--.  1 root root  1114 Mar 20 14:32 mongo-express.yaml
-rw-r--r--.  1 root root   158 Mar 20 14:32 mongo-secret.yaml
-rw-r--r--.  1 root root   862 Mar 20 14:31 mongo.yaml
drwxr-xr-x.  4 root root  4096 Mar 21 21:26 [01;34mprome-grafana[0m
drwxr-xr-x.  4 root root  4096 Mar 21 22:01 [01;34mPrometheus-Grafana[0m
-rw-r--r--.  1 root root  4077 Apr  8 22:23 values-metricbeat.yaml
]0;root@localhost:~[root@localhost ~]# llmv kube-metricbeat.yml /home/dhana/[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[2Prm -rf metricbeat-kubernetes.yaml[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[12@kubectl delete -f [C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[Ccrea[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[Kvalues-metricbeat.yaml
error: error validating "values-metricbeat.yaml": error validating data: [apiVersion not set, kind not set]; if you choose to ignore these errors, turn validation off with --validate=false
]0;root@localhost:~[root@localhost ~]# kubectl create -f  values-metricbeat.yaml --validate=false
error: unable to decode "values-metricbeat.yaml": Object 'Kind' is missing in '{"extraEnvs":[],"extraVolumeMounts":[],"extraVolumes":[],"fullnameOverride":"","hostPathRoot":"/var/lib","image":"docker.elastic.co/beats/metricbeat","imagePullPolicy":"IfNotPresent","imagePullSecrets":[],"imageTag":"7.3.0","livenessProbe":{"failureThreshold":3,"initialDelaySeconds":10,"periodSeconds":10,"timeoutSeconds":5},"managedServiceAccount":true,"metricbeatConfig":{"kube-state-metrics-metricbeat.yml":"metricbeat.modules:\n- module: kubernetes\n  enabled: true\n  metricsets:\n    - state_node\n    - state_deployment\n    - state_replicaset\n    - state_pod\n    - state_container\n  period: 10s\n  hosts: [\"${KUBE_STATE_METRICS_HOSTS:kube-state-metrics:8080}\"]\noutput.elasticsearch:\n  #hosts: '${ELASTICSEARCH_HOSTS:elasticsearch-master:9200}'\n   hosts: [\"http://192.168.1.50:9200/\"]\n","metricbeat.yml":"system:\n  hostfs: /hostfs\nmetricbeat.modules:\n- module: kubernetes\n  metricsets:\n    - container\n    - node\n    - pod\n    - system\n    - volume\n  period: 10s\n  hosts: [\"localhost:10255\"]\n  processors:\n  - add_kubernetes_metadata:\n      in_cluster: true\n- module: kubernetes\n  enabled: true\n  metricsets:\n    - event\n- module: system\n  period: 10s\n  metricsets:\n    - cpu\n    - load\n    - memory\n    - network\n    - process\n    - process_summary\n  processes: ['.*']\n  process.include_top_n:\n    by_cpu: 5\n    by_memory: 5\n- module: system\n  period: 1m\n  metricsets:\n    - filesystem\n    - fsstat\n  processors:\n  - drop_event.when.regexp:\n      system.filesystem.mount_point: '^/(sys|cgroup|proc|dev|etc|host|lib)($|/)'\noutput.elasticsearch:\n  #hosts: '${ELASTICSEARCH_HOSTS:elasticsearch-master:9200}'\n   hosts: [\"http://192.168.1.50:9200/\"]\n"},"nameOverride":"","podAnnotations":{},"podSecurityContext":{"privileged":false,"runAsUser":0},"readinessProbe":{"failureThreshold":3,"initialDelaySeconds":10,"periodSeconds":10,"timeoutSeconds":5},"replicas":1,"resources":{"limits":{"cpu":"1000m","memory":"200Mi"},"requests":{"cpu":"100m","memory":"100Mi"}},"secretMounts":[],"serviceAccount":"","terminationGracePeriod":30,"tolerations":[],"updateStrategy":"RollingUpdate"}'
]0;root@localhost:~[root@localhost ~]# kubectl create -f  values-metricbeat.yaml --validate=false[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[Khelm repo all

This command consists of multiple subcommands to interact with chart repositories.

It can be used to add, remove, list, and index chart repositories.

Usage:
  helm repo [command]

Available Commands:
  add         add a chart repository
  index       generate an index file given a directory containing packaged charts
  list        list chart repositories
  remove      remove one or more chart repositories
  update      update information of available charts locally from chart repositories

Flags:
  -h, --help   help for repo

Global Flags:
      --debug                       enable verbose output
      --kube-apiserver string       the address and the port for the Kubernetes API server
      --kube-as-group stringArray   group to impersonate for the operation, this flag can be repeated to specify multiple groups.
      --kube-as-user string         username to impersonate for the operation
      --kube-ca-file string         the certificate authority file for the Kubernetes API server connection
      --kube-context string         name of the kubeconfig context to use
      --kube-token string           bearer token used for authentication
      --kubeconfig string           path to the kubeconfig file
  -n, --namespace string            namespace scope for this request
      --registry-config string      path to the registry config file (default "/root/.config/helm/registry/config.json")
      --repository-cache string     path to the file containing cached repository indexes (default "/root/.cache/helm/repository")
      --repository-config string    path to the file containing repository names and URLs (default "/root/.config/helm/repositories.yaml")

Use "helm repo [command] --help" for more information about a command.
]0;root@localhost:~[root@localhost ~]# helm repo all[Klist
NAME                	URL                                               
bitnami             	https://charts.bitnami.com/bitnami                
prometheus-community	https://prometheus-community.github.io/helm-charts
elastic             	https://helm.elastic.co                           
]0;root@localhost:~[root@localhost ~]# helm repo list[K[Kinstall dhana[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[Khistoe[Krt[Ky
    1      #         values:
    2      #         - e2e-az1
    3      #         - e2e-az2
    4    securityContext:
    5      fsGroup: 65534
    6      runAsGroup: 65534
    7      runAsNonRoot: true
    8      runAsUser: 65534
    9    ## Prometheus-operator image
   10    ##
   11    image:
   12      repository: quay.io/coreos/prometheus-operator
   13      tag: v0.38.1
   14      sha: ""
   15      pullPolicy: IfNotPresent
   16    ## Configmap-reload image to use for reloading configmaps
   17    ##
   18    configmapReloadImage:
   19      repository: docker.io/jimmidyson/configmap-reload
   20      tag: v0.3.0
   21      sha: ""
   22    ## Prometheus-config-reloader image to use for config and rule reloading
   23    ##
   24    prometheusConfigReloaderImage:
   25      repository: quay.io/coreos/prometheus-config-reloader
   26      tag: v0.38.1
   27      sha: ""
   28    ## Set the prometheus config reloader side-car CPU limit
   29    ##
   30    configReloaderCpu: 100m
   31    ## Set the prometheus config reloader side-car memory limit
   32    ##
   33    configReloaderMemory: 25Mi
   34    ## Hyperkube image to use when cleaning up
   35    ##
   36    hyperkubeImage:
   37      repository: k8s.gcr.io/hyperkube
   38      tag: v1.16.12
   39      sha: ""
   40      pullPolicy: IfNotPresent
   41  ## Deploy a Prometheus instance
   42  ##
   43  prometheus:
   44    enabled: true
   45    ## Annotations for Prometheus
   46    ##
   47    annotations: {}
   48    ## Service account for Prometheuses to use.
   49    ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/
   50    ##
   51    serviceAccount:
   52      create: true
   53      name: ""
   54    ## Configuration for Prometheus service
   55    ##
   56    service:
   57      annotations: {}
   58      labels: {}
   59      clusterIP: ""
   60      ## Port for Prometheus Service to listen on
   61      ##
   62      port: 9090
   63      ## To be used with a proxy extraContainer port
   64      targetPort: 9090
   65      ## List of IP addresses at which the Prometheus server service is available
   66      ## Ref: https://kubernetes.io/docs/user-guide/services/#external-ips
   67      ##
   68      externalIPs: []
   69      ## Port to expose on each node
   70      ## Only used if service.type is 'NodePort'
   71      ##
   72      nodePort: 30090
   73      ## Loadbalancer IP
   74      ## Only use if service.type is "loadbalancer"
   75      loadBalancerIP: ""
   76      loadBalancerSourceRanges: []
   77      ## Service type
   78      ##
   79      type: ClusterIP
   80      sessionAffinity: ""
   81    ## Configuration for creating a separate Service for each statefulset Prometheus replica
   82    ##
   83    servicePerReplica:
   84      enabled: false
   85      annotations: {}
   86      ## Port for Prometheus Service per replica to listen on
   87      ##
   88      port: 9090
   89      ## To be used with a proxy extraContainer port
   90      targetPort: 9090
   91      ## Port to expose on each node
   92      ## Only used if servicePerReplica.type is 'NodePort'
   93      ##
   94      nodePort: 30091
   95      ## Loadbalancer source IP ranges
   96      ## Only used if servicePerReplica.type is "loadbalancer"
   97      loadBalancerSourceRanges: []
   98      ## Service type
   99      ##
  100      type: ClusterIP
  101    ## Configure pod disruption budgets for Prometheus
  102    ## ref: https://kubernetes.io/docs/tasks/run-application/configure-pdb/#specifying-a-poddisruptionbudget
  103    ## This configuration is immutable once created and will require the PDB to be deleted to be changed
  104    ## https://github.com/kubernetes/kubernetes/issues/45398
  105    ##
  106    podDisruptionBudget:
  107      enabled: false
  108      minAvailable: 1
  109      maxUnavailable: ""
  110  # Ingress exposes thanos sidecar outside the clsuter
  111    thanosIngress:
  112      enabled: false
  113      annotations: {}
  114      labels: {}
  115      servicePort: 10901
  116      ## Hosts must be provided if Ingress is enabled.
  117      ##
  118      hosts: []
  119        # - thanos-gateway.domain.com
  120      ## Paths to use for ingress rules
  121      ##
  122      paths: []
  123      # - /
  124      ## TLS configuration for Alertmanager Ingress
  125      ## Secret must be manually created in the namespace
  126      ##
  127      tls: []
  128      # - secretName: thanos-gateway-tls
  129      #   hosts:
  130      #   - thanos-gateway.domain.com
  131    ingress:
  132      enabled: false
  133      annotations: {}
  134      labels: {}
  135      ## Hostnames.
  136      ## Must be provided if Ingress is enabled.
  137      ##
  138      # hosts:
  139      #   - prometheus.domain.com
  140      hosts: []
  141      ## Paths to use for ingress rules - one path should match the prometheusSpec.routePrefix
  142      ##
  143      paths: []
  144      # - /
  145      ## TLS configuration for Prometheus Ingress
  146      ## Secret must be manually created in the namespace
  147      ##
  148      tls: []
  149        # - secretName: prometheus-general-tls
  150        #   hosts:
  151        #     - prometheus.example.com
  152    ## Configuration for creating an Ingress that will map to each Prometheus replica service
  153    ## prometheus.servicePerReplica must be enabled
  154    ##
  155    ingressPerReplica:
  156      enabled: false
  157      annotations: {}
  158      labels: {}
  159      ## Final form of the hostname for each per replica ingress is
  160      ## {{ ingressPerReplica.hostPrefix }}-{{ $replicaNumber }}.{{ ingressPerReplica.hostDomain }}
  161      ##
  162      ## Prefix for the per replica ingress that will have `-$replicaNumber`
  163      ## appended to the end
  164      hostPrefix: ""
  165      ## Domain that will be used for the per replica ingress
  166      hostDomain: ""
  167      ## Paths to use for ingress rules
  168      ##
  169      paths: []
  170      # - /
  171      ## Secret name containing the TLS certificate for Prometheus per replica ingress
  172      ## Secret must be manually created in the namespace
  173      tlsSecretName: ""
  174      ## Separated secret for each per replica Ingress. Can be used together with cert-manager
  175      ##
  176      tlsSecretPerReplica:
  177        enabled: false
  178        ## Final form of the secret for each per replica ingress is
  179        ## {{ tlsSecretPerReplica.prefix }}-{{ $replicaNumber }}
  180        ##
  181        prefix: "prometheus"
  182    ## Configure additional options for default pod security policy for Prometheus
  183    ## ref: https://kubernetes.io/docs/concepts/policy/pod-security-policy/
  184    podSecurityPolicy:
  185      allowedCapabilities: []
  186    serviceMonitor:
  187      ## Scrape interval. If not set, the Prometheus default scrape interval is used.
  188      ##
  189      interval: ""
  190      selfMonitor: true
  191      ## scheme: HTTP scheme to use for scraping. Can be used with `tlsConfig` for example if using istio mTLS.
  192      scheme: ""
  193      ## tlsConfig: TLS configuration to use when scraping the endpoint. For example if using istio mTLS.
  194      ## Of type: https://github.com/coreos/prometheus-operator/blob/master/Documentation/api.md#tlsconfig
  195      tlsConfig: {}
  196      bearerTokenFile:
  197      ## metric relabel configs to apply to samples before ingestion.
  198      ##
  199      metricRelabelings: []
  200      # - action: keep
  201      #   regex: 'kube_(daemonset|deployment|pod|namespace|node|statefulset).+'
  202      #   sourceLabels: [__name__]
  203      # relabel configs to apply to samples before ingestion.
  204      ##
  205      relabelings: []
  206      # - sourceLabels: [__meta_kubernetes_pod_node_name]
  207      #   separator: ;
  208      #   regex: ^(.*)$
  209      #   targetLabel: nodename
  210      #   replacement: $1
  211      #   action: replace
  212    ## Settings affecting prometheusSpec
  213    ## ref: https://github.com/coreos/prometheus-operator/blob/master/Documentation/api.md#prometheusspec
  214    ##
  215    prometheusSpec:
  216      ## If true, pass --storage.tsdb.max-block-duration=2h to prometheus. This is already done if using Thanos
  217      ##
  218      disableCompaction: false
  219      ## APIServerConfig
  220      ## ref: https://github.com/coreos/prometheus-operator/blob/master/Documentation/api.md#apiserverconfig
  221      ##
  222      apiserverConfig: {}
  223      ## Interval between consecutive scrapes.
  224      ##
  225      scrapeInterval: ""
  226      ## Interval between consecutive evaluations.
  227      ##
  228      evaluationInterval: ""
  229      ## ListenLocal makes the Prometheus server listen on loopback, so that it does not bind against the Pod IP.
  230      ##
  231      listenLocal: false
  232      ## EnableAdminAPI enables Prometheus the administrative HTTP API which includes functionality such as deleting time series.
  233      ## This is disabled by default.
  234      ## ref: https://prometheus.io/docs/prometheus/latest/querying/api/#tsdb-admin-apis
  235      ##
  236      enableAdminAPI: false
  237      ## Image of Prometheus.
  238      ##
  239      image:
  240        repository: quay.io/prometheus/prometheus
  241        tag: v2.18.2
  242        sha: ""
  243      ## Tolerations for use with node taints
  244      ## ref: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/
  245      ##
  246      tolerations: []
  247      #  - key: "key"
  248      #    operator: "Equal"
  249      #    value: "value"
  250      #    effect: "NoSchedule"
  251      ## Alertmanagers to which alerts will be sent
  252      ## ref: https://github.com/coreos/prometheus-operator/blob/master/Documentation/api.md#alertmanagerendpoints
  253      ##
  254      ## Default configuration will connect to the alertmanager deployed as part of this release
  255      ##
  256      alertingEndpoints: []
  257      # - name: ""
  258      #   namespace: ""
  259      #   port: http
  260      #   scheme: http
  261      #   pathPrefix: ""
  262      #   tlsConfig: {}
  263      #   bearerTokenFile: ""
  264      #   apiVersion: v2
  265      ## External labels to add to any time series or alerts when communicating with external systems
  266      ##
  267      externalLabels: {}
  268      ## Name of the external label used to denote replica name
  269      ##
  270      replicaExternalLabelName: ""
  271      ## If true, the Operator won't add the external label used to denote replica name
  272      ##
  273      replicaExternalLabelNameClear: false
  274      ## Name of the external label used to denote Prometheus instance name
  275      ##
  276      prometheusExternalLabelName: ""
  277      ## If true, the Operator won't add the external label used to denote Prometheus instance name
  278      ##
  279      prometheusExternalLabelNameClear: false
  280      ## External URL at which Prometheus will be reachable.
  281      ##
  282      externalUrl: ""
  283      ## Define which Nodes the Pods are scheduled on.
  284      ## ref: https://kubernetes.io/docs/user-guide/node-selection/
  285      ##
  286      nodeSelector: {}
  287      ## Secrets is a list of Secrets in the same namespace as the Prometheus object, which shall be mounted into the Prometheus Pods.
  288      ## The Secrets are mounted into /etc/prometheus/secrets/. Secrets changes after initial creation of a Prometheus object are not
  289      ## reflected in the running Pods. To change the secrets mounted into the Prometheus Pods, the object must be deleted and recreated
  290      ## with the new list of secrets.
  291      ##
  292      secrets: []
  293      ## ConfigMaps is a list of ConfigMaps in the same namespace as the Prometheus object, which shall be mounted into the Prometheus Pods.
  294      ## The ConfigMaps are mounted into /etc/prometheus/configmaps/.
  295      ##
  296      configMaps: []
  297      ## QuerySpec defines the query command line flags when starting Prometheus.
  298      ## ref: https://github.com/coreos/prometheus-operator/blob/master/Documentation/api.md#queryspec
  299      ##
  300      query: {}
  301      ## Namespaces to be selected for PrometheusRules discovery.
  302      ## If nil, select own namespace. Namespaces to be selected for ServiceMonitor discovery.
  303      ## See https://github.com/coreos/prometheus-operator/blob/master/Documentation/api.md#namespaceselector for usage
  304      ##
  305      ruleNamespaceSelector: {}
  306      ## If true, a nil or {} value for prometheus.prometheusSpec.ruleSelector will cause the
  307      ## prometheus resource to be created with selectors based on values in the helm deployment,
  308      ## which will also match the PrometheusRule resources created
  309      ##
  310      ruleSelectorNilUsesHelmValues: true
  311      ## PrometheusRules to be selected for target discovery.
  312      ## If {}, select all ServiceMonitors
  313      ##
  314      ruleSelector: {}
  315      ## Example which select all prometheusrules resources
  316      ## with label "prometheus" with values any of "example-rules" or "example-rules-2"
  317      # ruleSelector:
  318      #   matchExpressions:
  319      #     - key: prometheus
  320      #       operator: In
  321      #       values:
  322      #         - example-rules
  323      #         - example-rules-2
  324      #
  325      ## Example which select all prometheusrules resources with label "role" set to "example-rules"
  326      # ruleSelector:
  327      #   matchLabels:
  328      #     role: example-rules
  329      ## If true, a nil or {} value for prometheus.prometheusSpec.serviceMonitorSelector will cause the
  330      ## prometheus resource to be created with selectors based on values in the helm deployment,
  331      ## which will also match the servicemonitors created
  332      ##
  333      serviceMonitorSelectorNilUsesHelmValues: true
  334      ## ServiceMonitors to be selected for target discovery.
  335      ## If {}, select all ServiceMonitors
  336      ##
  337      serviceMonitorSelector: {}
  338      ## Example which selects ServiceMonitors with label "prometheus" set to "somelabel"
  339      # serviceMonitorSelector:
  340      #   matchLabels:
  341      #     prometheus: somelabel
  342      ## Namespaces to be selected for ServiceMonitor discovery.
  343      ## See https://github.com/coreos/prometheus-operator/blob/master/Documentation/api.md#namespaceselector for usage
  344      ##
  345      serviceMonitorNamespaceSelector: {}
  346      ## If true, a nil or {} value for prometheus.prometheusSpec.podMonitorSelector will cause the
  347      ## prometheus resource to be created with selectors based on values in the helm deployment,
  348      ## which will also match the podmonitors created
  349      ##
  350      podMonitorSelectorNilUsesHelmValues: true
  351      ## PodMonitors to be selected for target discovery.
  352      ## If {}, select all PodMonitors
  353      ##
  354      podMonitorSelector: {}
  355      ## Example which selects PodMonitors with label "prometheus" set to "somelabel"
  356      # podMonitorSelector:
  357      #   matchLabels:
  358      #     prometheus: somelabel
  359      ## Namespaces to be selected for PodMonitor discovery.
  360      ## See https://github.com/coreos/prometheus-operator/blob/master/Documentation/api.md#namespaceselector for usage
  361      ##
  362      podMonitorNamespaceSelector: {}
  363      ## How long to retain metrics
  364      ##
  365      retention: 10d
  366      ## Maximum size of metrics
  367      ##
  368      retentionSize: ""
  369      ## Enable compression of the write-ahead log using Snappy.
  370      ##
  371      walCompression: false
  372      ## If true, the Operator won't process any Prometheus configuration changes
  373      ##
  374      paused: false
  375      ## Number of Prometheus replicas desired
  376      ##
  377      replicas: 1
  378      ## Log level for Prometheus be configured in
  379      ##
  380      logLevel: info
  381      ## Log format for Prometheus be configured in
  382      ##
  383      logFormat: logfmt
  384      ## Prefix used to register routes, overriding externalUrl route.
  385      ## Useful for proxies that rewrite URLs.
  386      ##
  387      routePrefix: /
  388      ## Standard object’s metadata. More info: https://github.com/kubernetes/community/blob/master/contributors/devel/sig-architecture/api-conventions.md#metadata
  389      ## Metadata Labels and Annotations gets propagated to the prometheus pods.
  390      ##
  391      podMetadata: {}
  392      # labels:
  393      #   app: prometheus
  394      #   k8s-app: prometheus
  395      ## Pod anti-affinity can prevent the scheduler from placing Prometheus replicas on the same node.
  396      ## The default value "soft" means that the scheduler should *prefer* to not schedule two replica pods onto the same node but no guarantee is provided.
  397      ## The value "hard" means that the scheduler is *required* to not schedule two replica pods onto the same node.
  398      ## The value "" will disable pod anti-affinity so that no anti-affinity rules will be configured.
  399      podAntiAffinity: ""
  400      ## If anti-affinity is enabled sets the topologyKey to use for anti-affinity.
  401      ## This can be changed to, for example, failure-domain.beta.kubernetes.io/zone
  402      ##
  403      podAntiAffinityTopologyKey: kubernetes.io/hostname
  404      ## Assign custom affinity rules to the prometheus instance
  405      ## ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/
  406      ##
  407      affinity: {}
  408      # nodeAffinity:
  409      #   requiredDuringSchedulingIgnoredDuringExecution:
  410      #     nodeSelectorTerms:
  411      #     - matchExpressions:
  412      #       - key: kubernetes.io/e2e-az-name
  413      #         operator: In
  414      #         values:
  415      #         - e2e-az1
  416      #         - e2e-az2
  417      ## The remote_read spec configuration for Prometheus.
  418      ## ref: https://github.com/coreos/prometheus-operator/blob/master/Documentation/api.md#remotereadspec
  419      remoteRead: []
  420      # - url: http://remote1/read
  421      ## The remote_write spec configuration for Prometheus.
  422      ## ref: https://github.com/coreos/prometheus-operator/blob/master/Documentation/api.md#remotewritespec
  423      remoteWrite: []
  424      # - url: http://remote1/push
  425      ## Enable/Disable Grafana dashboards provisioning for prometheus remote write feature
  426      remoteWriteDashboards: false
  427      ## Resource limits & requests
  428      ##
  429      resources: {}
  430      # requests:
  431      #   memory: 400Mi
  432      ## Prometheus StorageSpec for persistent data
  433      ## ref: https://github.com/coreos/prometheus-operator/blob/master/Documentation/user-guides/storage.md
  434      ##
  435      storageSpec: {}
  436      #  volumeClaimTemplate:
  437      #    spec:
  438      #      storageClassName: gluster
  439      #      accessModes: ["ReadWriteOnce"]
  440      #      resources:
  441      #        requests:
  442      #          storage: 50Gi
  443      #    selector: {}
  444      # Additional volumes on the output StatefulSet definition.
  445      volumes: []
  446      # Additional VolumeMounts on the output StatefulSet definition.
  447      volumeMounts: []
  448      ## AdditionalScrapeConfigs allows specifying additional Prometheus scrape configurations. Scrape configurations
  449      ## are appended to the configurations generated by the Prometheus Operator. Job configurations must have the form
  450      ## as specified in the official Prometheus documentation:
  451      ## https://prometheus.io/docs/prometheus/latest/configuration/configuration/#scrape_config. As scrape configs are
  452      ## appended, the user is responsible to make sure it is valid. Note that using this feature may expose the possibility
  453      ## to break upgrades of Prometheus. It is advised to review Prometheus release notes to ensure that no incompatible
  454      ## scrape configs are going to break Prometheus after the upgrade.
  455      ##
  456      ## The scrape configuraiton example below will find master nodes, provided they have the name .*mst.*, relabel the
  457      ## port to 2379 and allow etcd scraping provided it is running on all Kubernetes master nodes
  458      ##
  459      additionalScrapeConfigs: []
  460      # - job_name: kube-etcd
  461      #   kubernetes_sd_configs:
  462      #     - role: node
  463      #   scheme: https
  464      #   tls_config:
  465      #     ca_file:   /etc/prometheus/secrets/etcd-client-cert/etcd-ca
  466      #     cert_file: /etc/prometheus/secrets/etcd-client-cert/etcd-client
  467      #     key_file:  /etc/prometheus/secrets/etcd-client-cert/etcd-client-key
  468      #   relabel_configs:
  469      #   - action: labelmap
  470      #     regex: __meta_kubernetes_node_label_(.+)
  471      #   - source_labels: [__address__]
  472      #     action: replace
  473      #     targetLabel: __address__
  474      #     regex: ([^:;]+):(\d+)
  475      #     replacement: ${1}:2379
  476      #   - source_labels: [__meta_kubernetes_node_name]
  477      #     action: keep
  478      #     regex: .*mst.*
  479      #   - source_labels: [__meta_kubernetes_node_name]
  480      #     action: replace
  481      #     targetLabel: node
  482      #     regex: (.*)
  483      #     replacement: ${1}
  484      #   metric_relabel_configs:
  485      #   - regex: (kubernetes_io_hostname|failure_domain_beta_kubernetes_io_region|beta_kubernetes_io_os|beta_kubernetes_io_arch|beta_kubernetes_io_instance_type|failure_domain_beta_kubernetes_io_zone)
  486      #     action: labeldrop
  487      ## If additional scrape configurations are already deployed in a single secret file you can use this section.
  488      ## Expected values are the secret name and key
  489      ## Cannot be used with additionalScrapeConfigs
  490      additionalScrapeConfigsSecret: {}
  491        # enabled: false
  492        # name:
  493        # key:
  494      ## additionalPrometheusSecretsAnnotations allows to add annotations to the kubernetes secret. This can be useful
  495      ## when deploying via spinnaker to disable versioning on the secret, strategy.spinnaker.io/versioned: 'false'
  496      additionalPrometheusSecretsAnnotations: {}
  497      ## AdditionalAlertManagerConfigs allows for manual configuration of alertmanager jobs in the form as specified
  498      ## in the official Prometheus documentation https://prometheus.io/docs/prometheus/latest/configuration/configuration/#<alertmanager_config>.
  499      ## AlertManager configurations specified are appended to the configurations generated by the Prometheus Operator.
  500      ## As AlertManager configs are appended, the user is responsible to make sure it is valid. Note that using this
  501      ## feature may expose the possibility to break upgrades of Prometheus. It is advised to review Prometheus release
  502      ## notes to ensure that no incompatible AlertManager configs are going to break Prometheus after the upgrade.
  503      ##
  504      additionalAlertManagerConfigs: []
  505      # - consul_sd_configs:
  506      #   - server: consul.dev.test:8500
  507      #     scheme: http
  508      #     datacenter: dev
  509      #     tag_separator: ','
  510      #     services:
  511      #       - metrics-prometheus-alertmanager
  512      ## AdditionalAlertRelabelConfigs allows specifying Prometheus alert relabel configurations. Alert relabel configurations specified are appended
  513      ## to the configurations generated by the Prometheus Operator. Alert relabel configurations specified must have the form as specified in the
  514      ## official Prometheus documentation: https://prometheus.io/docs/prometheus/latest/configuration/configuration/#alert_relabel_configs.
  515      ## As alert relabel configs are appended, the user is responsible to make sure it is valid. Note that using this feature may expose the
  516      ## possibility to break upgrades of Prometheus. It is advised to review Prometheus release notes to ensure that no incompatible alert relabel
  517      ## configs are going to break Prometheus after the upgrade.
  518      ##
  519      additionalAlertRelabelConfigs: []
  520      # - separator: ;
  521      #   regex: prometheus_replica
  522      #   replacement: $1
  523      #   action: labeldrop
  524      ## SecurityContext holds pod-level security attributes and common container settings.
  525      ## This defaults to non root user with uid 1000 and gid 2000.
  526      ## https://github.com/coreos/prometheus-operator/blob/master/Documentation/api.md
  527      ##
  528      securityContext:
  529        runAsGroup: 2000
  530        runAsNonRoot: true
  531        runAsUser: 1000
  532        fsGroup: 2000
  533      ## Priority class assigned to the Pods
  534      ##
  535      priorityClassName: ""
  536  ll
  537  mv value.txt  pro-gra.yml
  538  kubectl create -f pro-gra.yml 
  539  kubectl create -f pro-gra.yml --validate=false--validate=false
  540  kubectl create -f pro-gra.yml --validate=false
  541  vim pro-gra.yml 
  542  kubectl create -f pro-gra.yml --validate=false
  543  ll
  544  kubectl create -f pro-gra.yml --validate=false
  545  ll
  546  ll
  547  rm -rf mongodb.yaml
  548  cat mongo.yaml
  549  yum install git
  550  git clone https://github.com/DeekshithSN/kubernetes.git
  551  ll
  552  cd kubernetes/
  553  ll
  554  cd monitoring/
  555  ll
  556  /usr/local/bin/k3s-uninstall.sh
  557  /usr/local/bin/k3s-agent-uninstall.sh
  558  curl -sfL https://get.k3s.io | sh -
  559  k3s 
  560  k3s  kubectl
  561  helm install my-release bitnami/jenkins
  562  netstat -nultp
  563  export KUBECONFIG=/etc/rancher/k3s/k3s.yaml
  564  helm repo update
  565  kubectl get all
  566  helm install my-release bitnami/jenkins
  567  helm uninstall my-release bitnami/jenkins
  568  kubectl get po
  569  kubectl delete pod my-release-jenkins-68f6b4fbdc-7gfg7
  570  kubectl get po
  571  helm install prometheus stable/prometheus-operator
  572  helm install my-release bitnami/kube-prometheus
  573  kubectl get deploy -w --namespace default -l app.kubernetes.io/name=kube-prometheus-operator,app.kubernetes.io/instance=my-release
  574  kubectl get svc
  575  kubectl port-forward --namespace default svc/my-release-kube-prometheus-prometheus 9090:9090
  576  kubectl get svc
  577  helm install my-release bitnami/grafana
  578  helm install grafana bitnami/grafana
  579   echo "Browse to http://127.0.0.1:8080"
  580  kubectl port-forward svc/grafana 8080:3000
  581  elm delete my-release
  582  helm delete my-release
  583  helm delete grafana
  584  helm install my-release bitnami/grafana
  585  echo "Password: $(kubectl get secret my-release-grafana-admin --namespace default -o jsonpath="{.data.GF_SECURITY_ADMIN_PASSWORD}" | base64 --decode)"
  586   kubectl port-forward svc/my-release-grafana 8080:3000
  587   helm uninstall my-release bitnami/grafana
  588  kubectl get po
  589  init 0
  590  kubectl get svc
  591  kubectl get po
  592  kubectl get all
  593  kubectl get svc
  594  kubectl get  pod
  595  #kubectl port-forward prometheus-my-release-kube-prometheus-prometheus-0 90
  596  firewall-cmd --perm --add-port=9090/tcp
  597  firewall-cmd --reload
  598  kubectl port-forward prometheus-my-release-kube-prometheus-prometheus-0 9090
  599  kubectl get  pod
  600  kubectl port-forward prometheus-my-release-kube-prometheus-prometheus-0 9090
  601  netstat -nultp
  602  kubectl get svc
  603  kubectl get all
  604  kubectl port-forward prometheus-my-release-kube-prometheus-prometheus-0 9090
  605  kubectl get all
  606  kubectl get po
  607  kubect delete pod grafana-6cc877d4c7-krwwm
  608  kubectl delete pod grafana-6cc877d4c7-krwwm
  609  kubectl get po
  610  kubectl get svc
  611  kubectl get po
  612  kubectl get deployment
  613  helm uninstall grafana
  614  kubectl get svc
  615  helm uninstall my-release
  616  helm uninstall grafana
  617  kubectl get po
  618  kubectl get all
  619  helm delete grafana
  620  helm delete my-release
  621  kubectl get svc
  622  kubectl delete my-release
  623  kubectl delete grafana
  624  kubectl get pods --namespace=monitoring
  625  ll
  626   cd prome-grafana
  627  mkdir prome-grafana
  628  pwd
  629  cd prome-grafana/
  630  ll
  631  pwd
  632  cd
  633  cd kubernetes/
  634  ll
  635  cd monitoring/
  636  ll
  637  cp kubernetes-grafana  kubernetes-prometheus /root/prome-grafana/
  638  cp -rvf  kubernetes-grafana  kubernetes-prometheus /root/prome-grafana/
  639  ll
  640  cd 
  641  cd prome-grafana/
  642  ll
  643  kubectl create namespace monitoring
  644  #kubectl create -f clusterRole.yaml
  645  cd kubernetes-prometheus/
  646  kubectl create -f clusterRole.yaml
  647  kubectl create -f config-map.yaml
  648  kubectl create -f prometheus-deployment.yaml
  649  kubectl get deployments --namespace=monitoring
  650  kubectl get pods --namespace=monitoring
  651  kubectl port-forward prometheus-deployment-599bbd9457-gnpvp 8080:9090 -n monitoring
  652  kubectl port-forward prometheus-monitoring-3331088907-hm5n1 8080:9090 -n monitoringkubectl get pods --namespace=monitoring
  653  kubectl get pods --namespace=monitoring
  654  kubectl port-forward prometheus-deployment-599bbd9457-gnpvp 8080:9090 -n monitoring
  655  ll
  656  kubectl get svc
  657  kubectl get svc all
  658  kubectl get svc --all
  659  kubectl get svc
  660  kubectl port-forward prometheus-deployment-599bbd9457-gnpvp 8080:9090 -n monitoring
  661  kubectltl get pods
  662  kubectl get pods
  663  kubectl get pods --all-namespace
  664  kubectl get pods -all-namespace
  665  kubectl get pods --help
  666  kubectl get deployments --namespace=monitoring
  667  kubectl delete pod prometheus-deployment\
  668  kubectl delete pod prometheus-deployment
  669  kubectl get pods --namespace=monitoring
  670  kubectl delete pod prometheus-deployment-599bbd9457-gnpvp
  671  kubectl describe prometheus-deployment-599bbd9457-gnpvp
  672  kubectkubectl get pods --namespace=monitoring
  673  kubectl get pods --namespace=monitoring
  674  kubectk get namespace
  675  kubectl get namespace
  676  kubectl delete  namespace monitoring
  677  kubectl get pods --namespace=monitoring
  678  ll
  679  kubectl get pods
  680  $ helm install prometheus stable/prometheus-operator --namespace prometheus
  681   helm install prometheus stable/prometheus-operator --namespace prometheus
  682  $ kubectl create namespace prometheus
  683   kubectl create namespace prometheus
  684  $ helm install prometheus stable/prometheus-operator --namespace prometheus
  685   helm install prometheus stable/prometheus-operator --namespace prometheus
  686  ll
  687  git clone https://github.com/gurpreet0610/Deploy-Prometheus-Grafana-on-Kubernetes.git
  688  ll
  689  cd Deploy-Prometheus-Grafana-on-Kubernetes/
  690  ll
  691  kubectl create -f monitoring-namespace.yaml
  692  kubectl create -f prometheus-cluster-role.yaml
  693  kubectl create -f prometheus-config.yaml
  694  kubectl create -f prometheus-deployment.yaml
  695  kubectl create -f prometheus-pvc-nfs.yaml
  696  kubectl create -f prometheus-pv-nfs.yaml
  697  kubectl create -f prometheus-service.yaml
  698  kubectl get svc
  699  kubectl get pods
  700  kubectl get pods --namespace monitoring
  701  kubectl apply -k . -n monitoring
  702  kubectl get all -n monitoring
  703  cd
  704  ll
  705  mv Deploy-Prometheus-Grafana-on-Kubernetes Prometheus-Grafana
  706  ll
  707  kubectl get all -n monitoring
  708  kubectl describe pod pod/prometheus-6977fc7cfd-f882g
  709  kubectl describe pod/prometheus-6977fc7cfd-f882g
  710  kubectl get all -n monitoring
  711  kubectl delete -n monitoring
  712  kubectl delete -namespace  monitoring
  713  kls get ns all
  714  kubectl  get ns all
  715  kubectl  get ns 
  716  kubectl delete --namespace  monitoring
  717  kubectl delete namespace  monitoring
  718  ll
  719  free -h
  720  k3s
  721  cd /etc/
  722  ll
  723  ls
  724  ll
  725  cd
  726  ll
  727  cat pro-gra.yml
  728  helm repo add bitnami https://charts.bitnami.com/bitnami
  729   helm install my-release bitnami/mongodb
  730  kubectl get cluster
  731  kubectl get all
  732  ifup ens33
  733  nmtui
  734  ifup ens33
  735  nmdcli d s
  736  nmcli d s
  737  ip a
  738  nmtui
  739  ip a
  740  ping 8.8.8.8
  741  mount /dev/sr0 /mnt
  742  ll
  743  ll\
  744  ip a
  745  nmtui
  746  ip a
  747  ping 8.8.8.8
  748  netstat -nultp
  749  kubectl get svc
  750  free -h
  751  kubectl get svc
  752   helm install prometheus prometheus-community/kube-prometheus-stack
  753  ping 8.8.8.8
  754   helm install dhana prometheus-community/kube-prometheus-stack
  755  ll
  756  rm -rf prometheus-grafana.yml
  757  rm -rf pro-gra.yml
  758  helm show values prometheus-community/kube-prometheus-stack
  759  helm install my-release bitnami/grafana
  760  history
  761   export KUBECONFIG=/etc/rancher/k3s/k3s.yaml
  762  helm install my-release bitnami/grafana
  763  echo "User: admin"
  764  echo "Password: $(kubectl get secret my-release-grafana-admin --namespace default -o jsonpath="{.data.GF_SECURITY_ADMIN_PASSWORD}" | base64 --decode)"
  765  kubectl get svc
  766  firewall-cmd --list-all
  767  firewall-cmd --perm -add-port=3000/tcp
  768  firewall-cmd --perm --add-port=3000/tcp
  769  firewall-cmd --reload
  770  kubectl get svc
  771  kubectl get all
  772  kubectl port-forward svc/my-release-grafana 8080:3000
  773  kubectl port-forward pod/my-release-grafana-64488d7767-blqcv 8080:3000
  774  kubectl describe pod/my-release-grafana-64488d7767-blqcv
  775  kubectl delete pod/my-release-grafana-64488d7767-blqcv
  776  kubectl get all
  777  helm delete my-release
  778   helm install dhana prometheus-community/kube-prometheus-stack
  779   helm install prometheus  prometheus-community/kube-prometheus-stack
  780  helm install my-release bitnami/grafana
  781  get svc
  782  kubectl get svc
  783  kubectl get all
  784  kubectl logs pod/my-release-grafana-67d697b64c-tl8fr
  785  kubectl describe  pod/my-release-grafana-67d697b64c-tl8fr
  786  kubectl delete  pod/my-release-grafana-67d697b64c-tl8fr
  787  kubectl get all
  788  kubectl describe  pod/my-release-grafana-67d697b64c-w9chq
  789  kubectl get all
  790  helm delete my-release
  791   helm install dhana prometheus-community/kube-prometheus-stack
  792  ll
  793   helm install dhana prometheus-community/kube-prometheus-stack
  794  helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
  795  helm repo update
  796  kubectl delete crd alertmanagerconfigs.monitoring.coreos.com
  797  kubectl delete crd alertmanagers.monitoring.coreos.com
  798  kubectl delete crd podmonitors.monitoring.coreos.com
  799  kubectl delete crd probes.monitoring.coreos.com
  800  kubectl delete crd prometheuses.monitoring.coreos.com
  801  kubectl delete crd prometheusrules.monitoring.coreos.com
  802  kubectl delete crd servicemonitors.monitoring.coreos.com
  803  kubectl delete crd thanosrulers.monitoring.coreos.com
  804  helm show values kube-prometheus-stack
  805  helm repo list
  806  helm repo prometheus-community
  807  kubectl show values prometheus-community/kube-prometheus-stack
  808  helm  show values prometheus-community/kube-prometheus-stack
  809  helm  show values prometheus-community/kube-prometheus-stack >  prometheus-grafana.yml
  810  cat prometheus-grafana.yml
  811  vim  prometheus-grafana.yml
  812   helm install prometheus prometheus-community/kube-prometheus-stack
  813  helm install  prometheus prometheus-community/kube-prometheus-stack
  814  history
  815  export KUBECONFIG=/etc/rancher/k3s/k3s.yaml
  816  ! 879
  817  helm install  prometheus prometheus-community/kube-prometheus-stack
  818  export KUBECONFIG=/etc/rancher/k3s/k3s.yaml
  819  helm install  prometheus prometheus-community/kube-prometheus-stack
  820  helm install  prometheus1 prometheus-community/kube-prometheus-stack
  821  helm delete  prometheus1
  822  helm delete  prometheus
  823  kubectl delete crd alertmanagerconfigs.monitoring.coreos.com
  824  kubectl delete crd alertmanagers.monitoring.coreos.com
  825  kubectl delete crd podmonitors.monitoring.coreos.com
  826  kubectl delete crd probes.monitoring.coreos.com
  827  kubectl delete crd prometheuses.monitoring.coreos.com
  828  kubectl delete crd prometheusrules.monitoring.coreos.com
  829  kubectl delete crd servicemonitors.monitoring.coreos.com
  830  kubectl delete crd thanosrulers.monitoring.coreos.com
  831  helm install my-release bitnami/grafana
  832  history
  833  helm install my-release bitnami/grafana
  834  export KUBECONFIG=/etc/rancher/k3s/k3s.yaml
  835  helm install my-release bitnami/grafana
  836  kubectl get all
  837  kubectl describe pod/prometheus1-kube-prometheu-admission-create--1-rcs8h
  838  kubectl get all
  839  kubectl delete pod/prometheus1-kube-prometheu-admission-create--1-rcs8h
  840  kubectl delete replicaset.apps/my-release-grafana-599d8b86b4
  841  helm delete my-release
  842  kubectl get all
  843  kubectl delete service/kubernetes
  844  kubectl delete pod/prometheus1-kube-prometheu-admission-create--1-v4wlm
  845  kubectl delete job.batch/prometheus-kube-prometheus-admission-create
  846  kubectl delete job.batch/prometheus1-kube-prometheu-admission-create
  847  kubectl get all
  848  kubectl delete service/kubernetes
  849  kubectl get all
  850  kubectl get po
  851  kubectl get svc
  852  kubectl get no
  853  top
  854  kubectl get all
  855  helm delete prometheus1
  856  history
  857   export KUBECONFIG=/etc/rancher/k3s/k3s.yaml
  858  helm delete prometheus1
  859   export KUBECONFIG=/etc/rancher/k3s/k3s.yaml
  860  helm delete prometheus1
  861   export KUBECONFIG=/etc/rancher/k3s/k3s.yaml
  862  kubectl get all
  863  kubectl get po
  864  kubectl get svc
  865  kubectl get ns
  866  helm delete prometheus1
  867  /usr/local/bin/k3s-uninstall.sh
  868  ll
  869  free -h
  870  init 6
  871  kubectl get po
  872  kubectl get svc
  873  kubectl svc
  874  kubectl get svc
  875  kubectl get all
  876  kubectl port-forward pod/prometheus1-grafana-c77c67fd-9xc2f  80:3000
  877  ll
  878   helm search repo prometheus-community
  879  helm show values prometheus-community/kube-prometheus-stack
  880  helm show values prometheus-community/kube-prometheus-stack > kube-prometheus-stack.yml
  881  ll
  882  vim kube-prometheus-stack.yml
  883  cp kube-prometheus-stack.yml /home/dhana/
  884  vim kube-prometheus-stack.yml
  885  kubectl create -f kube-prometheus-stack.yml
  886  curl -sfL https://get.k3s.io | sh -
  887  kubectl create -f kube-prometheus-stack.yml
  888  kubectl create -f kube-prometheus-stack.yml  --validate=false
  889  init 6
  890  kubectl
  891  kubectl api-resources
  892  kubectl
  893  kubectl api-versions
  894  kubectl 
  895  mount /dev/sr0 /mnt
  896  yum install httpd
  897  kill -9 10212
  898  yum install httpd
  899  systemctl status httpd
  900  init 3
  901  kubectl get nodes
  902  kubectl get po
  903  kubectl get all
  904  init 5
  905  ll
  906  kubectl get po
  907  kubectl cerate -f  metricbeat-kubernetes.yaml
  908  kubectl create -f  metricbeat-kubernetes.yaml
  909  kubectl delete -f  metricbeat-kubernetes.yaml
  910  rm -rf metricbeat-kubernetes.yaml
  911  mv kube-metricbeat.yml /home/dhana/
  912  ll
  913  kubectl create -f  values-metricbeat.yaml
  914  kubectl create -f  values-metricbeat.yaml --validate=false
  915  helm repo all
  916  helm repo list
  917  history
]0;root@localhost:~[root@localhost ~]# helm install --name metricbeat elastic/metricbeat[C[C[C-elastic/metricbeatfelastic/metricbeat elastic/metricbeatvelastic/metricbeataelastic/metricbeatlues-metricbeat.yamlelastic/metricbeat elastic/metricbeat
Error: unknown flag: --name
]0;root@localhost:~[root@localhost ~]# helm install --name metricbeat -f values-metricbeat.yaml elastic/metricbeat[1P
Error: INSTALLATION FAILED: Kubernetes cluster unreachable: Get "http://localhost:8080/version": dial tcp [::1]:8080: connect: connection refused
]0;root@localhost:~[root@localhost ~]# export KUBECONFIG=/etc/rancher/k3s/k3s.yaml
]0;root@localhost:~[root@localhost ~]# export KUBECONFIG=/etc/rancher/k3s/k3s.yaml[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[Chelm install -name metricbeat -f values-metricbeat.yaml elastic/metricbeat
Error: INSTALLATION FAILED: cannot re-use a name that is still in use
]0;root@localhost:~[root@localhost ~]# helm install -name metricbeat -f values-metricbeat.yaml elastic/metricbeat[1@1
NAME: metricbeat1
LAST DEPLOYED: Sun Apr 10 13:49:13 2022
NAMESPACE: ame
STATUS: deployed
REVISION: 1
TEST SUITE: None
NOTES:
1. Watch all containers come up.
  $ kubectl get pods --namespace=ame -l app=metricbeat1-metricbeat -w
]0;root@localhost:~[root@localhost ~]#  kubectl get pods --namespace=ame -l app=metricbeat1-metricbeat -w
NAME                           READY   STATUS   RESTARTS      AGE
metricbeat1-metricbeat-qhvj9   0/1     Error    1 (10s ago)   21s
metricbeat1-metricbeat-qhvj9   0/1     CrashLoopBackOff   1 (5s ago)    21s
metricbeat1-metricbeat-qhvj9   0/1     Running            2 (17s ago)   33s
metricbeat1-metricbeat-qhvj9   0/1     Error              2 (21s ago)   37s
metricbeat1-metricbeat-qhvj9   0/1     CrashLoopBackOff   2 (5s ago)    41s
^C]0;root@localhost:~[root@localhost ~]# kubectl describe metricbeat1-metricbeat-qhvj9 -n ame
error: the server doesn't have a resource type "metricbeat1-metricbeat-qhvj9"
]0;root@localhost:~[root@localhost ~]# kubectl describe metricbeat1-metricbeat-qhvj9 -n ame[C[C[C[C[C[1@ [1@p[1@o[1@d[1@s[C[1@ 
Name:         metricbeat1-metricbeat-qhvj9
Namespace:    ame
Priority:     0
Node:         localhost.localdomain/192.168.1.30
Start Time:   Sun, 10 Apr 2022 13:49:17 +0530
Labels:       app=metricbeat1-metricbeat
              chart=metricbeat-7.17.1
              controller-revision-hash=cc6bd946c
              heritage=Helm
              pod-template-generation=1
              release=metricbeat1
Annotations:  configChecksum: 7cdb7f24e539c4cea341f4dffcc30e3784b4f1acd8fd5d903617f03c98ed5fc
Status:       Running
IP:           10.42.0.80
IPs:
  IP:           10.42.0.80
Controlled By:  DaemonSet/metricbeat1-metricbeat
Containers:
  metricbeat:
    Container ID:  containerd://0aa7eeba22d6e49a529e2d6273a6346a04c6ae1a84eda26d56747547525c274a
    Image:         docker.elastic.co/beats/metricbeat:7.3.0
    Image ID:      docker.elastic.co/beats/metricbeat@sha256:8821ae9b18ba923ca435f621de4b0003f05b5733b997739da550f4355852b868
    Port:          <none>
    Host Port:     <none>
    Args:
      -e
      -E
      http.enabled=true
      --system.hostfs=/hostfs
    State:          Waiting
      Reason:       CrashLoopBackOff
    Last State:     Terminated
      Reason:       Error
      Exit Code:    1
      Started:      Sun, 10 Apr 2022 13:50:25 +0530
      Finished:     Sun, 10 Apr 2022 13:50:29 +0530
    Ready:          False
    Restart Count:  3
    Limits:
      cpu:     1
      memory:  200Mi
    Requests:
      cpu:     100m
      memory:  100Mi
    Liveness:  exec [sh -c #!/usr/bin/env bash -e
curl --fail 127.0.0.1:5066
] delay=10s timeout=5s period=10s #success=1 #failure=3
    Readiness:  exec [sh -c #!/usr/bin/env bash -e
metricbeat test output
] delay=10s timeout=5s period=10s #success=1 #failure=3
    Environment:
      POD_NAMESPACE:  ame (v1:metadata.namespace)
      NODE_NAME:       (v1:spec.nodeName)
    Mounts:
      /hostfs/proc from proc (ro)
      /hostfs/sys/fs/cgroup from cgroup (ro)
      /usr/share/metricbeat/data from data (rw)
      /usr/share/metricbeat/kube-state-metrics-metricbeat.yml from metricbeat-config (ro,path="kube-state-metrics-metricbeat.yml")
      /usr/share/metricbeat/metricbeat.yml from metricbeat-config (ro,path="metricbeat.yml")
      /var/run/docker.sock from varrundockersock (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-w8zs8 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             False 
  ContainersReady   False 
  PodScheduled      True 
Volumes:
  metricbeat-config:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      metricbeat1-metricbeat-config
    Optional:  false
  data:
    Type:          HostPath (bare host directory volume)
    Path:          /var/lib/metricbeat1-metricbeat-ame-data
    HostPathType:  DirectoryOrCreate
  varrundockersock:
    Type:          HostPath (bare host directory volume)
    Path:          /var/run/docker.sock
    HostPathType:  
  proc:
    Type:          HostPath (bare host directory volume)
    Path:          /proc
    HostPathType:  
  cgroup:
    Type:          HostPath (bare host directory volume)
    Path:          /sys/fs/cgroup
    HostPathType:  
  kube-api-access-w8zs8:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type     Reason     Age                From               Message
  ----     ------     ----               ----               -------
  Normal   Scheduled  84s                default-scheduler  Successfully assigned ame/metricbeat1-metricbeat-qhvj9 to localhost.localdomain
  Normal   Pulled     16s (x4 over 80s)  kubelet            Container image "docker.elastic.co/beats/metricbeat:7.3.0" already present on machine
  Normal   Created    16s (x4 over 80s)  kubelet            Created container metricbeat
  Normal   Started    16s (x4 over 79s)  kubelet            Started container metricbeat
  Warning  BackOff    4s (x7 over 68s)   kubelet            Back-off restarting failed container
]0;root@localhost:~[root@localhost ~]# kubectl describe pods  metricbeat1-metricbeat-qhvj9 -n ame[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[6P[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[8P[1@l[1@o[1@g[1@s[C[1@ 
2022-04-10T08:21:15.856Z	INFO	instance/beat.go:606	Home path: [/usr/share/metricbeat] Config path: [/usr/share/metricbeat] Data path: [/usr/share/metricbeat/data] Logs path: [/usr/share/metricbeat/logs]
2022-04-10T08:21:15.857Z	INFO	instance/beat.go:614	Beat ID: 5adcd7d1-adc0-4e84-b4b0-4dd6e0e1a8de
2022-04-10T08:21:15.857Z	INFO	[seccomp]	seccomp/seccomp.go:124	Syscall filter successfully installed
2022-04-10T08:21:15.857Z	INFO	[beat]	instance/beat.go:902	Beat info	{"system_info": {"beat": {"path": {"config": "/usr/share/metricbeat", "data": "/usr/share/metricbeat/data", "home": "/usr/share/metricbeat", "logs": "/usr/share/metricbeat/logs"}, "type": "metricbeat", "uuid": "5adcd7d1-adc0-4e84-b4b0-4dd6e0e1a8de"}}}
2022-04-10T08:21:15.857Z	INFO	[beat]	instance/beat.go:911	Build info	{"system_info": {"build": {"commit": "6f0ec01a0e57fe7d4fd703b017fb5a2f6448d097", "libbeat": "7.3.0", "time": "2019-07-24T17:41:50.000Z", "version": "7.3.0"}}}
2022-04-10T08:21:15.857Z	INFO	[beat]	instance/beat.go:914	Go runtime info	{"system_info": {"go": {"os":"linux","arch":"amd64","max_procs":1,"version":"go1.12.4"}}}
2022-04-10T08:21:15.858Z	INFO	[beat]	instance/beat.go:918	Host info	{"system_info": {"host": {"architecture":"x86_64","boot_time":"2022-04-10T08:05:49Z","containerized":false,"name":"metricbeat1-metricbeat-qhvj9","ip":["127.0.0.1/8","::1/128","10.42.0.80/24","fe80::44bd:eaff:fe3b:9a22/64"],"kernel_version":"3.10.0-1160.el7.x86_64","mac":["46:bd:ea:3b:9a:22"],"os":{"family":"redhat","platform":"centos","name":"CentOS Linux","version":"7 (Core)","major":7,"minor":6,"patch":1810,"codename":"Core"},"timezone":"UTC","timezone_offset_sec":0}}}
2022-04-10T08:21:15.859Z	INFO	[beat]	instance/beat.go:947	Process info	{"system_info": {"process": {"capabilities": {"inheritable":["chown","dac_override","fowner","fsetid","kill","setgid","setuid","setpcap","net_bind_service","net_raw","sys_chroot","mknod","audit_write","setfcap"],"permitted":["chown","dac_override","fowner","fsetid","kill","setgid","setuid","setpcap","net_bind_service","net_raw","sys_chroot","mknod","audit_write","setfcap"],"effective":["chown","dac_override","fowner","fsetid","kill","setgid","setuid","setpcap","net_bind_service","net_raw","sys_chroot","mknod","audit_write","setfcap"],"bounding":["chown","dac_override","fowner","fsetid","kill","setgid","setuid","setpcap","net_bind_service","net_raw","sys_chroot","mknod","audit_write","setfcap"],"ambient":null}, "cwd": "/usr/share/metricbeat", "exe": "/usr/share/metricbeat/metricbeat", "name": "metricbeat", "pid": 1, "ppid": 0, "seccomp": {"mode":"filter","no_new_privs":true}, "start_time": "2022-04-10T08:21:15.600Z"}}}
2022-04-10T08:21:15.859Z	INFO	instance/beat.go:292	Setup Beat: metricbeat; Version: 7.3.0
2022-04-10T08:21:15.859Z	INFO	[index-management]	idxmgmt/std.go:178	Set output.elasticsearch.index to 'metricbeat-7.3.0' as ILM is enabled.
2022-04-10T08:21:15.859Z	INFO	elasticsearch/client.go:170	Elasticsearch url: http://192.168.1.50:9200/
2022-04-10T08:21:15.859Z	INFO	[publisher]	pipeline/module.go:97	Beat name: metricbeat1-metricbeat-qhvj9
2022-04-10T08:21:15.860Z	INFO	kubernetes/util.go:86	kubernetes: Using pod name metricbeat1-metricbeat-qhvj9 and namespace ame to discover kubernetes node
2022-04-10T08:21:16.872Z	ERROR	kubernetes/util.go:90	kubernetes: Querying for pod failed with error: performing request: Get https://10.43.0.1:443/api/v1/namespaces/ame/pods/metricbeat1-metricbeat-qhvj9: dial tcp 10.43.0.1:443: connect: no route to host
2022-04-10T08:21:16.872Z	INFO	kubernetes/watcher.go:182	kubernetes: Performing a resource sync for *v1.PodList
2022-04-10T08:21:17.881Z	ERROR	kubernetes/watcher.go:185	kubernetes: Performing a resource sync err performing request: Get https://10.43.0.1:443/api/v1/pods?fieldSelector=spec.nodeName%3Dlocalhost&resourceVersion=0: dial tcp 10.43.0.1:443: connect: no route to host for *v1.PodList
2022-04-10T08:21:17.882Z	INFO	kubernetes/util.go:86	kubernetes: Using pod name metricbeat1-metricbeat-qhvj9 and namespace ame to discover kubernetes node
2022-04-10T08:21:18.886Z	ERROR	kubernetes/util.go:90	kubernetes: Querying for pod failed with error: performing request: Get https://10.43.0.1:443/api/v1/namespaces/ame/pods/metricbeat1-metricbeat-qhvj9: dial tcp 10.43.0.1:443: connect: no route to host
2022-04-10T08:21:18.890Z	INFO	kubernetes/util.go:86	kubernetes: Using pod name metricbeat1-metricbeat-qhvj9 and namespace ame to discover kubernetes node
2022-04-10T08:21:19.900Z	ERROR	kubernetes/util.go:90	kubernetes: Querying for pod failed with error: performing request: Get https://10.43.0.1:443/api/v1/namespaces/ame/pods/metricbeat1-metricbeat-qhvj9: dial tcp 10.43.0.1:443: connect: no route to host
2022-04-10T08:21:19.911Z	INFO	filesystem/filesystem.go:57	Ignoring filesystem types: sysfs, rootfs, ramfs, bdev, proc, cgroup, cpuset, tmpfs, devtmpfs, debugfs, securityfs, sockfs, dax, bpf, pipefs, configfs, devpts, hugetlbfs, autofs, pstore, mqueue, selinuxfs, fuse, fusectl, rpc_pipefs, overlay
2022-04-10T08:21:19.911Z	INFO	[system.fsstat]	fsstat/fsstat.go:56	Ignoring filesystem types: %ssysfs, rootfs, ramfs, bdev, proc, cgroup, cpuset, tmpfs, devtmpfs, debugfs, securityfs, sockfs, dax, bpf, pipefs, configfs, devpts, hugetlbfs, autofs, pstore, mqueue, selinuxfs, fuse, fusectl, rpc_pipefs, overlay
2022-04-10T08:21:19.911Z	INFO	instance/beat.go:385	metricbeat stopped.
2022-04-10T08:21:19.911Z	ERROR	instance/beat.go:877	Exiting: 1 error: performing request: Get https://10.43.0.1:443/api/v1/pods?fieldSelector=spec.nodeName%3Dlocalhost&resourceVersion=0: dial tcp 10.43.0.1:443: connect: no route to host
Exiting: 1 error: performing request: Get https://10.43.0.1:443/api/v1/pods?fieldSelector=spec.nodeName%3Dlocalhost&resourceVersion=0: dial tcp 10.43.0.1:443: connect: no route to host
]0;root@localhost:~[root@localhost ~]# kubectl logs  metricbeat1-metricbeat-qhvj9 -n ame[1P[1P[1P[1P[1@e[1@d[1@i[1@t
error: the server doesn't have a resource type "metricbeat1-metricbeat-qhvj9"
]0;root@localhost:~[root@localhost ~]# kubectl edit  metricbeat1-metricbeat-qhvj9 -n ame[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[Clogs[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[9@describe pod[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[6P[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C kubectl get pods --namespace=ame -l app=metricbeat1-metricbeat -w
NAME                           READY   STATUS             RESTARTS      AGE
metricbeat1-metricbeat-qhvj9   0/1     CrashLoopBackOff   4 (69s ago)   3m12s
metricbeat1-metricbeat-qhvj9   0/1     Running            5 (87s ago)   3m30s
metricbeat1-metricbeat-qhvj9   0/1     Error              5 (91s ago)   3m34s
metricbeat1-metricbeat-qhvj9   0/1     CrashLoopBackOff   5 (8s ago)    3m41s
  ^C]0;root@localhost:~[root@localhost ~]#  kubectl get pods --namespace=ame -l app=metricbeat1-metricbeat -w[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[17Pkubectl edit  metricbeat1-metricbeat-qhvj9 -n ame[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[Clogs[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[9@describe pod[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[6P[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C kubectl get pods --namespace=ame -l app=metricbeat1-metricbeat -w[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[Chelm install -name metricbeat1 -f values-metricbeat.yaml elastic/metricbeat[1P[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[1@1[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[1@u[1@n
Error: unknown shorthand flag: 'f' in -f
]0;root@localhost:~[root@localhost ~]# helm uninstall -name metricbeat1 -f values-metricbeat.yaml elastic/metricbeat[2P[12P[C[7P[C[1P[10P[1P[1P[1P[1P[1P[1P[1P[1P[1P[1P[1P[1P[1P[1P[1P[1P[1P[1P[1P[1P[1P elastic/metricbeat[1Pelastic/metricbeat[1Plastic/metricbeat[1Pastic/metricbeat[1Pstic/metricbeat[1Ptic/metricbeat[1Pic/metricbeat[1Pc/metricbeat[1P/metricbeat[1Pmetricbeat[1Petricbeat[1Ptricbeat[1Pricbeat[1Picbeat[1Pcbeat[1Pbeat[1Peat[1Pat[1Pt[Kdelete metricbeat1
Error: uninstall: Release not loaded: metricbeat1: release: not found
]0;root@localhost:~[root@localhost ~]# helm delete metricbeat1[1P metricbeat1[1P metricbeat1[1P metricbeat1[1P metricbeat1[1P metricbeat1[1P metricbeat1u metricbeat1m metricbeat1[1P metricbeat1n metricbeat1i metricbeat1n metricbeat1s metricbeat1t metricbeat1a metricbeat1l metricbeat1l metricbeat1[C metricbeat1
Error: uninstall: Release not loaded: metricbeat1: release: not found
]0;root@localhost:~[root@localhost ~]# metricbeat1[K[K[K[K[K[K[K[K[K[K[Khelm uninstall  metricbeat1[4Pdelete[C[C[C[C[C[C[C[C[C[C[C[Cuninstall -name metricbeat1 -f values-metricbeat.yaml elastic/metricbeat[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[11P kubectl get pods --namespace=ame -l app=metricbeat1-metricbeat -w[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[17Pkubectl edit  metricbeat1-metricbeat-qhvj9 -n ame[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[Clogs[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[9@describe pod[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[6P[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C kubectl get pods --namespace=ame -l app=metricbeat1-metricbeat -w[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[Chelm install -name metricbeat1 -f values-metricbeat.yaml elastic/metricbeat[1P[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[31Pexport KUBECONFIG=/etc/rancher/k3s/k3s.yaml
]0;root@localhost:~[root@localhost ~]# export KUBECONFIG=/etc/rancher/k3s/k3s.yaml[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[16Phelm uninstall  metricbeat1
Error: uninstall: Release not loaded: metricbeat1: release: not found
]0;root@localhost:~[root@localhost ~]# helm uninstall  metricbeat1[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[Khistory
    1      #         values:
    2      #         - e2e-az1
    3      #         - e2e-az2
    4    securityContext:
    5      fsGroup: 65534
    6      runAsGroup: 65534
    7      runAsNonRoot: true
    8      runAsUser: 65534
    9    ## Prometheus-operator image
   10    ##
   11    image:
   12      repository: quay.io/coreos/prometheus-operator
   13      tag: v0.38.1
   14      sha: ""
   15      pullPolicy: IfNotPresent
   16    ## Configmap-reload image to use for reloading configmaps
   17    ##
   18    configmapReloadImage:
   19      repository: docker.io/jimmidyson/configmap-reload
   20      tag: v0.3.0
   21      sha: ""
   22    ## Prometheus-config-reloader image to use for config and rule reloading
   23    ##
   24    prometheusConfigReloaderImage:
   25      repository: quay.io/coreos/prometheus-config-reloader
   26      tag: v0.38.1
   27      sha: ""
   28    ## Set the prometheus config reloader side-car CPU limit
   29    ##
   30    configReloaderCpu: 100m
   31    ## Set the prometheus config reloader side-car memory limit
   32    ##
   33    configReloaderMemory: 25Mi
   34    ## Hyperkube image to use when cleaning up
   35    ##
   36    hyperkubeImage:
   37      repository: k8s.gcr.io/hyperkube
   38      tag: v1.16.12
   39      sha: ""
   40      pullPolicy: IfNotPresent
   41  ## Deploy a Prometheus instance
   42  ##
   43  prometheus:
   44    enabled: true
   45    ## Annotations for Prometheus
   46    ##
   47    annotations: {}
   48    ## Service account for Prometheuses to use.
   49    ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/
   50    ##
   51    serviceAccount:
   52      create: true
   53      name: ""
   54    ## Configuration for Prometheus service
   55    ##
   56    service:
   57      annotations: {}
   58      labels: {}
   59      clusterIP: ""
   60      ## Port for Prometheus Service to listen on
   61      ##
   62      port: 9090
   63      ## To be used with a proxy extraContainer port
   64      targetPort: 9090
   65      ## List of IP addresses at which the Prometheus server service is available
   66      ## Ref: https://kubernetes.io/docs/user-guide/services/#external-ips
   67      ##
   68      externalIPs: []
   69      ## Port to expose on each node
   70      ## Only used if service.type is 'NodePort'
   71      ##
   72      nodePort: 30090
   73      ## Loadbalancer IP
   74      ## Only use if service.type is "loadbalancer"
   75      loadBalancerIP: ""
   76      loadBalancerSourceRanges: []
   77      ## Service type
   78      ##
   79      type: ClusterIP
   80      sessionAffinity: ""
   81    ## Configuration for creating a separate Service for each statefulset Prometheus replica
   82    ##
   83    servicePerReplica:
   84      enabled: false
   85      annotations: {}
   86      ## Port for Prometheus Service per replica to listen on
   87      ##
   88      port: 9090
   89      ## To be used with a proxy extraContainer port
   90      targetPort: 9090
   91      ## Port to expose on each node
   92      ## Only used if servicePerReplica.type is 'NodePort'
   93      ##
   94      nodePort: 30091
   95      ## Loadbalancer source IP ranges
   96      ## Only used if servicePerReplica.type is "loadbalancer"
   97      loadBalancerSourceRanges: []
   98      ## Service type
   99      ##
  100      type: ClusterIP
  101    ## Configure pod disruption budgets for Prometheus
  102    ## ref: https://kubernetes.io/docs/tasks/run-application/configure-pdb/#specifying-a-poddisruptionbudget
  103    ## This configuration is immutable once created and will require the PDB to be deleted to be changed
  104    ## https://github.com/kubernetes/kubernetes/issues/45398
  105    ##
  106    podDisruptionBudget:
  107      enabled: false
  108      minAvailable: 1
  109      maxUnavailable: ""
  110  # Ingress exposes thanos sidecar outside the clsuter
  111    thanosIngress:
  112      enabled: false
  113      annotations: {}
  114      labels: {}
  115      servicePort: 10901
  116      ## Hosts must be provided if Ingress is enabled.
  117      ##
  118      hosts: []
  119        # - thanos-gateway.domain.com
  120      ## Paths to use for ingress rules
  121      ##
  122      paths: []
  123      # - /
  124      ## TLS configuration for Alertmanager Ingress
  125      ## Secret must be manually created in the namespace
  126      ##
  127      tls: []
  128      # - secretName: thanos-gateway-tls
  129      #   hosts:
  130      #   - thanos-gateway.domain.com
  131    ingress:
  132      enabled: false
  133      annotations: {}
  134      labels: {}
  135      ## Hostnames.
  136      ## Must be provided if Ingress is enabled.
  137      ##
  138      # hosts:
  139      #   - prometheus.domain.com
  140      hosts: []
  141      ## Paths to use for ingress rules - one path should match the prometheusSpec.routePrefix
  142      ##
  143      paths: []
  144      # - /
  145      ## TLS configuration for Prometheus Ingress
  146      ## Secret must be manually created in the namespace
  147      ##
  148      tls: []
  149        # - secretName: prometheus-general-tls
  150        #   hosts:
  151        #     - prometheus.example.com
  152    ## Configuration for creating an Ingress that will map to each Prometheus replica service
  153    ## prometheus.servicePerReplica must be enabled
  154    ##
  155    ingressPerReplica:
  156      enabled: false
  157      annotations: {}
  158      labels: {}
  159      ## Final form of the hostname for each per replica ingress is
  160      ## {{ ingressPerReplica.hostPrefix }}-{{ $replicaNumber }}.{{ ingressPerReplica.hostDomain }}
  161      ##
  162      ## Prefix for the per replica ingress that will have `-$replicaNumber`
  163      ## appended to the end
  164      hostPrefix: ""
  165      ## Domain that will be used for the per replica ingress
  166      hostDomain: ""
  167      ## Paths to use for ingress rules
  168      ##
  169      paths: []
  170      # - /
  171      ## Secret name containing the TLS certificate for Prometheus per replica ingress
  172      ## Secret must be manually created in the namespace
  173      tlsSecretName: ""
  174      ## Separated secret for each per replica Ingress. Can be used together with cert-manager
  175      ##
  176      tlsSecretPerReplica:
  177        enabled: false
  178        ## Final form of the secret for each per replica ingress is
  179        ## {{ tlsSecretPerReplica.prefix }}-{{ $replicaNumber }}
  180        ##
  181        prefix: "prometheus"
  182    ## Configure additional options for default pod security policy for Prometheus
  183    ## ref: https://kubernetes.io/docs/concepts/policy/pod-security-policy/
  184    podSecurityPolicy:
  185      allowedCapabilities: []
  186    serviceMonitor:
  187      ## Scrape interval. If not set, the Prometheus default scrape interval is used.
  188      ##
  189      interval: ""
  190      selfMonitor: true
  191      ## scheme: HTTP scheme to use for scraping. Can be used with `tlsConfig` for example if using istio mTLS.
  192      scheme: ""
  193      ## tlsConfig: TLS configuration to use when scraping the endpoint. For example if using istio mTLS.
  194      ## Of type: https://github.com/coreos/prometheus-operator/blob/master/Documentation/api.md#tlsconfig
  195      tlsConfig: {}
  196      bearerTokenFile:
  197      ## metric relabel configs to apply to samples before ingestion.
  198      ##
  199      metricRelabelings: []
  200      # - action: keep
  201      #   regex: 'kube_(daemonset|deployment|pod|namespace|node|statefulset).+'
  202      #   sourceLabels: [__name__]
  203      # relabel configs to apply to samples before ingestion.
  204      ##
  205      relabelings: []
  206      # - sourceLabels: [__meta_kubernetes_pod_node_name]
  207      #   separator: ;
  208      #   regex: ^(.*)$
  209      #   targetLabel: nodename
  210      #   replacement: $1
  211      #   action: replace
  212    ## Settings affecting prometheusSpec
  213    ## ref: https://github.com/coreos/prometheus-operator/blob/master/Documentation/api.md#prometheusspec
  214    ##
  215    prometheusSpec:
  216      ## If true, pass --storage.tsdb.max-block-duration=2h to prometheus. This is already done if using Thanos
  217      ##
  218      disableCompaction: false
  219      ## APIServerConfig
  220      ## ref: https://github.com/coreos/prometheus-operator/blob/master/Documentation/api.md#apiserverconfig
  221      ##
  222      apiserverConfig: {}
  223      ## Interval between consecutive scrapes.
  224      ##
  225      scrapeInterval: ""
  226      ## Interval between consecutive evaluations.
  227      ##
  228      evaluationInterval: ""
  229      ## ListenLocal makes the Prometheus server listen on loopback, so that it does not bind against the Pod IP.
  230      ##
  231      listenLocal: false
  232      ## EnableAdminAPI enables Prometheus the administrative HTTP API which includes functionality such as deleting time series.
  233      ## This is disabled by default.
  234      ## ref: https://prometheus.io/docs/prometheus/latest/querying/api/#tsdb-admin-apis
  235      ##
  236      enableAdminAPI: false
  237      ## Image of Prometheus.
  238      ##
  239      image:
  240        repository: quay.io/prometheus/prometheus
  241        tag: v2.18.2
  242        sha: ""
  243      ## Tolerations for use with node taints
  244      ## ref: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/
  245      ##
  246      tolerations: []
  247      #  - key: "key"
  248      #    operator: "Equal"
  249      #    value: "value"
  250      #    effect: "NoSchedule"
  251      ## Alertmanagers to which alerts will be sent
  252      ## ref: https://github.com/coreos/prometheus-operator/blob/master/Documentation/api.md#alertmanagerendpoints
  253      ##
  254      ## Default configuration will connect to the alertmanager deployed as part of this release
  255      ##
  256      alertingEndpoints: []
  257      # - name: ""
  258      #   namespace: ""
  259      #   port: http
  260      #   scheme: http
  261      #   pathPrefix: ""
  262      #   tlsConfig: {}
  263      #   bearerTokenFile: ""
  264      #   apiVersion: v2
  265      ## External labels to add to any time series or alerts when communicating with external systems
  266      ##
  267      externalLabels: {}
  268      ## Name of the external label used to denote replica name
  269      ##
  270      replicaExternalLabelName: ""
  271      ## If true, the Operator won't add the external label used to denote replica name
  272      ##
  273      replicaExternalLabelNameClear: false
  274      ## Name of the external label used to denote Prometheus instance name
  275      ##
  276      prometheusExternalLabelName: ""
  277      ## If true, the Operator won't add the external label used to denote Prometheus instance name
  278      ##
  279      prometheusExternalLabelNameClear: false
  280      ## External URL at which Prometheus will be reachable.
  281      ##
  282      externalUrl: ""
  283      ## Define which Nodes the Pods are scheduled on.
  284      ## ref: https://kubernetes.io/docs/user-guide/node-selection/
  285      ##
  286      nodeSelector: {}
  287      ## Secrets is a list of Secrets in the same namespace as the Prometheus object, which shall be mounted into the Prometheus Pods.
  288      ## The Secrets are mounted into /etc/prometheus/secrets/. Secrets changes after initial creation of a Prometheus object are not
  289      ## reflected in the running Pods. To change the secrets mounted into the Prometheus Pods, the object must be deleted and recreated
  290      ## with the new list of secrets.
  291      ##
  292      secrets: []
  293      ## ConfigMaps is a list of ConfigMaps in the same namespace as the Prometheus object, which shall be mounted into the Prometheus Pods.
  294      ## The ConfigMaps are mounted into /etc/prometheus/configmaps/.
  295      ##
  296      configMaps: []
  297      ## QuerySpec defines the query command line flags when starting Prometheus.
  298      ## ref: https://github.com/coreos/prometheus-operator/blob/master/Documentation/api.md#queryspec
  299      ##
  300      query: {}
  301      ## Namespaces to be selected for PrometheusRules discovery.
  302      ## If nil, select own namespace. Namespaces to be selected for ServiceMonitor discovery.
  303      ## See https://github.com/coreos/prometheus-operator/blob/master/Documentation/api.md#namespaceselector for usage
  304      ##
  305      ruleNamespaceSelector: {}
  306      ## If true, a nil or {} value for prometheus.prometheusSpec.ruleSelector will cause the
  307      ## prometheus resource to be created with selectors based on values in the helm deployment,
  308      ## which will also match the PrometheusRule resources created
  309      ##
  310      ruleSelectorNilUsesHelmValues: true
  311      ## PrometheusRules to be selected for target discovery.
  312      ## If {}, select all ServiceMonitors
  313      ##
  314      ruleSelector: {}
  315      ## Example which select all prometheusrules resources
  316      ## with label "prometheus" with values any of "example-rules" or "example-rules-2"
  317      # ruleSelector:
  318      #   matchExpressions:
  319      #     - key: prometheus
  320      #       operator: In
  321      #       values:
  322      #         - example-rules
  323      #         - example-rules-2
  324      #
  325      ## Example which select all prometheusrules resources with label "role" set to "example-rules"
  326      # ruleSelector:
  327      #   matchLabels:
  328      #     role: example-rules
  329      ## If true, a nil or {} value for prometheus.prometheusSpec.serviceMonitorSelector will cause the
  330      ## prometheus resource to be created with selectors based on values in the helm deployment,
  331      ## which will also match the servicemonitors created
  332      ##
  333      serviceMonitorSelectorNilUsesHelmValues: true
  334      ## ServiceMonitors to be selected for target discovery.
  335      ## If {}, select all ServiceMonitors
  336      ##
  337      serviceMonitorSelector: {}
  338      ## Example which selects ServiceMonitors with label "prometheus" set to "somelabel"
  339      # serviceMonitorSelector:
  340      #   matchLabels:
  341      #     prometheus: somelabel
  342      ## Namespaces to be selected for ServiceMonitor discovery.
  343      ## See https://github.com/coreos/prometheus-operator/blob/master/Documentation/api.md#namespaceselector for usage
  344      ##
  345      serviceMonitorNamespaceSelector: {}
  346      ## If true, a nil or {} value for prometheus.prometheusSpec.podMonitorSelector will cause the
  347      ## prometheus resource to be created with selectors based on values in the helm deployment,
  348      ## which will also match the podmonitors created
  349      ##
  350      podMonitorSelectorNilUsesHelmValues: true
  351      ## PodMonitors to be selected for target discovery.
  352      ## If {}, select all PodMonitors
  353      ##
  354      podMonitorSelector: {}
  355      ## Example which selects PodMonitors with label "prometheus" set to "somelabel"
  356      # podMonitorSelector:
  357      #   matchLabels:
  358      #     prometheus: somelabel
  359      ## Namespaces to be selected for PodMonitor discovery.
  360      ## See https://github.com/coreos/prometheus-operator/blob/master/Documentation/api.md#namespaceselector for usage
  361      ##
  362      podMonitorNamespaceSelector: {}
  363      ## How long to retain metrics
  364      ##
  365      retention: 10d
  366      ## Maximum size of metrics
  367      ##
  368      retentionSize: ""
  369      ## Enable compression of the write-ahead log using Snappy.
  370      ##
  371      walCompression: false
  372      ## If true, the Operator won't process any Prometheus configuration changes
  373      ##
  374      paused: false
  375      ## Number of Prometheus replicas desired
  376      ##
  377      replicas: 1
  378      ## Log level for Prometheus be configured in
  379      ##
  380      logLevel: info
  381      ## Log format for Prometheus be configured in
  382      ##
  383      logFormat: logfmt
  384      ## Prefix used to register routes, overriding externalUrl route.
  385      ## Useful for proxies that rewrite URLs.
  386      ##
  387      routePrefix: /
  388      ## Standard object’s metadata. More info: https://github.com/kubernetes/community/blob/master/contributors/devel/sig-architecture/api-conventions.md#metadata
  389      ## Metadata Labels and Annotations gets propagated to the prometheus pods.
  390      ##
  391      podMetadata: {}
  392      # labels:
  393      #   app: prometheus
  394      #   k8s-app: prometheus
  395      ## Pod anti-affinity can prevent the scheduler from placing Prometheus replicas on the same node.
  396      ## The default value "soft" means that the scheduler should *prefer* to not schedule two replica pods onto the same node but no guarantee is provided.
  397      ## The value "hard" means that the scheduler is *required* to not schedule two replica pods onto the same node.
  398      ## The value "" will disable pod anti-affinity so that no anti-affinity rules will be configured.
  399      podAntiAffinity: ""
  400      ## If anti-affinity is enabled sets the topologyKey to use for anti-affinity.
  401      ## This can be changed to, for example, failure-domain.beta.kubernetes.io/zone
  402      ##
  403      podAntiAffinityTopologyKey: kubernetes.io/hostname
  404      ## Assign custom affinity rules to the prometheus instance
  405      ## ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/
  406      ##
  407      affinity: {}
  408      # nodeAffinity:
  409      #   requiredDuringSchedulingIgnoredDuringExecution:
  410      #     nodeSelectorTerms:
  411      #     - matchExpressions:
  412      #       - key: kubernetes.io/e2e-az-name
  413      #         operator: In
  414      #         values:
  415      #         - e2e-az1
  416      #         - e2e-az2
  417      ## The remote_read spec configuration for Prometheus.
  418      ## ref: https://github.com/coreos/prometheus-operator/blob/master/Documentation/api.md#remotereadspec
  419      remoteRead: []
  420      # - url: http://remote1/read
  421      ## The remote_write spec configuration for Prometheus.
  422      ## ref: https://github.com/coreos/prometheus-operator/blob/master/Documentation/api.md#remotewritespec
  423      remoteWrite: []
  424      # - url: http://remote1/push
  425      ## Enable/Disable Grafana dashboards provisioning for prometheus remote write feature
  426      remoteWriteDashboards: false
  427      ## Resource limits & requests
  428      ##
  429      resources: {}
  430      # requests:
  431      #   memory: 400Mi
  432      ## Prometheus StorageSpec for persistent data
  433      ## ref: https://github.com/coreos/prometheus-operator/blob/master/Documentation/user-guides/storage.md
  434      ##
  435      storageSpec: {}
  436      #  volumeClaimTemplate:
  437      #    spec:
  438      #      storageClassName: gluster
  439      #      accessModes: ["ReadWriteOnce"]
  440      #      resources:
  441      #        requests:
  442      #          storage: 50Gi
  443      #    selector: {}
  444      # Additional volumes on the output StatefulSet definition.
  445      volumes: []
  446      # Additional VolumeMounts on the output StatefulSet definition.
  447      volumeMounts: []
  448      ## AdditionalScrapeConfigs allows specifying additional Prometheus scrape configurations. Scrape configurations
  449      ## are appended to the configurations generated by the Prometheus Operator. Job configurations must have the form
  450      ## as specified in the official Prometheus documentation:
  451      ## https://prometheus.io/docs/prometheus/latest/configuration/configuration/#scrape_config. As scrape configs are
  452      ## appended, the user is responsible to make sure it is valid. Note that using this feature may expose the possibility
  453      ## to break upgrades of Prometheus. It is advised to review Prometheus release notes to ensure that no incompatible
  454      ## scrape configs are going to break Prometheus after the upgrade.
  455      ##
  456      ## The scrape configuraiton example below will find master nodes, provided they have the name .*mst.*, relabel the
  457      ## port to 2379 and allow etcd scraping provided it is running on all Kubernetes master nodes
  458      ##
  459      additionalScrapeConfigs: []
  460      # - job_name: kube-etcd
  461      #   kubernetes_sd_configs:
  462      #     - role: node
  463      #   scheme: https
  464      #   tls_config:
  465      #     ca_file:   /etc/prometheus/secrets/etcd-client-cert/etcd-ca
  466      #     cert_file: /etc/prometheus/secrets/etcd-client-cert/etcd-client
  467      #     key_file:  /etc/prometheus/secrets/etcd-client-cert/etcd-client-key
  468      #   relabel_configs:
  469      #   - action: labelmap
  470      #     regex: __meta_kubernetes_node_label_(.+)
  471      #   - source_labels: [__address__]
  472      #     action: replace
  473      #     targetLabel: __address__
  474      #     regex: ([^:;]+):(\d+)
  475      #     replacement: ${1}:2379
  476      #   - source_labels: [__meta_kubernetes_node_name]
  477      #     action: keep
  478      #     regex: .*mst.*
  479      #   - source_labels: [__meta_kubernetes_node_name]
  480      #     action: replace
  481      #     targetLabel: node
  482      #     regex: (.*)
  483      #     replacement: ${1}
  484      #   metric_relabel_configs:
  485      #   - regex: (kubernetes_io_hostname|failure_domain_beta_kubernetes_io_region|beta_kubernetes_io_os|beta_kubernetes_io_arch|beta_kubernetes_io_instance_type|failure_domain_beta_kubernetes_io_zone)
  486      #     action: labeldrop
  487      ## If additional scrape configurations are already deployed in a single secret file you can use this section.
  488      ## Expected values are the secret name and key
  489      ## Cannot be used with additionalScrapeConfigs
  490      additionalScrapeConfigsSecret: {}
  491        # enabled: false
  492        # name:
  493        # key:
  494      ## additionalPrometheusSecretsAnnotations allows to add annotations to the kubernetes secret. This can be useful
  495      ## when deploying via spinnaker to disable versioning on the secret, strategy.spinnaker.io/versioned: 'false'
  496      additionalPrometheusSecretsAnnotations: {}
  497      ## AdditionalAlertManagerConfigs allows for manual configuration of alertmanager jobs in the form as specified
  498      ## in the official Prometheus documentation https://prometheus.io/docs/prometheus/latest/configuration/configuration/#<alertmanager_config>.
  499      ## AlertManager configurations specified are appended to the configurations generated by the Prometheus Operator.
  500      ## As AlertManager configs are appended, the user is responsible to make sure it is valid. Note that using this
  501      ## feature may expose the possibility to break upgrades of Prometheus. It is advised to review Prometheus release
  502      ## notes to ensure that no incompatible AlertManager configs are going to break Prometheus after the upgrade.
  503      ##
  504      additionalAlertManagerConfigs: []
  505      # - consul_sd_configs:
  506      #   - server: consul.dev.test:8500
  507      #     scheme: http
  508      #     datacenter: dev
  509      #     tag_separator: ','
  510      #     services:
  511      #       - metrics-prometheus-alertmanager
  512      ## AdditionalAlertRelabelConfigs allows specifying Prometheus alert relabel configurations. Alert relabel configurations specified are appended
  513      ## to the configurations generated by the Prometheus Operator. Alert relabel configurations specified must have the form as specified in the
  514      ## official Prometheus documentation: https://prometheus.io/docs/prometheus/latest/configuration/configuration/#alert_relabel_configs.
  515      ## As alert relabel configs are appended, the user is responsible to make sure it is valid. Note that using this feature may expose the
  516      ## possibility to break upgrades of Prometheus. It is advised to review Prometheus release notes to ensure that no incompatible alert relabel
  517      ## configs are going to break Prometheus after the upgrade.
  518      ##
  519      additionalAlertRelabelConfigs: []
  520      # - separator: ;
  521      #   regex: prometheus_replica
  522      #   replacement: $1
  523      #   action: labeldrop
  524      ## SecurityContext holds pod-level security attributes and common container settings.
  525      ## This defaults to non root user with uid 1000 and gid 2000.
  526      ## https://github.com/coreos/prometheus-operator/blob/master/Documentation/api.md
  527      ##
  528      securityContext:
  529        runAsGroup: 2000
  530        runAsNonRoot: true
  531        runAsUser: 1000
  532        fsGroup: 2000
  533      ## Priority class assigned to the Pods
  534      ##
  535      priorityClassName: ""
  536  ll
  537  mv value.txt  pro-gra.yml
  538  kubectl create -f pro-gra.yml 
  539  kubectl create -f pro-gra.yml --validate=false--validate=false
  540  kubectl create -f pro-gra.yml --validate=false
  541  vim pro-gra.yml 
  542  kubectl create -f pro-gra.yml --validate=false
  543  ll
  544  kubectl create -f pro-gra.yml --validate=false
  545  ll
  546  ll
  547  rm -rf mongodb.yaml
  548  cat mongo.yaml
  549  yum install git
  550  git clone https://github.com/DeekshithSN/kubernetes.git
  551  ll
  552  cd kubernetes/
  553  ll
  554  cd monitoring/
  555  ll
  556  /usr/local/bin/k3s-uninstall.sh
  557  /usr/local/bin/k3s-agent-uninstall.sh
  558  curl -sfL https://get.k3s.io | sh -
  559  k3s 
  560  k3s  kubectl
  561  helm install my-release bitnami/jenkins
  562  netstat -nultp
  563  export KUBECONFIG=/etc/rancher/k3s/k3s.yaml
  564  helm repo update
  565  kubectl get all
  566  helm install my-release bitnami/jenkins
  567  helm uninstall my-release bitnami/jenkins
  568  kubectl get po
  569  kubectl delete pod my-release-jenkins-68f6b4fbdc-7gfg7
  570  kubectl get po
  571  helm install prometheus stable/prometheus-operator
  572  helm install my-release bitnami/kube-prometheus
  573  kubectl get deploy -w --namespace default -l app.kubernetes.io/name=kube-prometheus-operator,app.kubernetes.io/instance=my-release
  574  kubectl get svc
  575  kubectl port-forward --namespace default svc/my-release-kube-prometheus-prometheus 9090:9090
  576  kubectl get svc
  577  helm install my-release bitnami/grafana
  578  helm install grafana bitnami/grafana
  579   echo "Browse to http://127.0.0.1:8080"
  580  kubectl port-forward svc/grafana 8080:3000
  581  elm delete my-release
  582  helm delete my-release
  583  helm delete grafana
  584  helm install my-release bitnami/grafana
  585  echo "Password: $(kubectl get secret my-release-grafana-admin --namespace default -o jsonpath="{.data.GF_SECURITY_ADMIN_PASSWORD}" | base64 --decode)"
  586   kubectl port-forward svc/my-release-grafana 8080:3000
  587   helm uninstall my-release bitnami/grafana
  588  kubectl get po
  589  init 0
  590  kubectl get svc
  591  kubectl get po
  592  kubectl get all
  593  kubectl get svc
  594  kubectl get  pod
  595  #kubectl port-forward prometheus-my-release-kube-prometheus-prometheus-0 90
  596  firewall-cmd --perm --add-port=9090/tcp
  597  firewall-cmd --reload
  598  kubectl port-forward prometheus-my-release-kube-prometheus-prometheus-0 9090
  599  kubectl get  pod
  600  kubectl port-forward prometheus-my-release-kube-prometheus-prometheus-0 9090
  601  netstat -nultp
  602  kubectl get svc
  603  kubectl get all
  604  kubectl port-forward prometheus-my-release-kube-prometheus-prometheus-0 9090
  605  kubectl get all
  606  kubectl get po
  607  kubect delete pod grafana-6cc877d4c7-krwwm
  608  kubectl delete pod grafana-6cc877d4c7-krwwm
  609  kubectl get po
  610  kubectl get svc
  611  kubectl get po
  612  kubectl get deployment
  613  helm uninstall grafana
  614  kubectl get svc
  615  helm uninstall my-release
  616  helm uninstall grafana
  617  kubectl get po
  618  kubectl get all
  619  helm delete grafana
  620  helm delete my-release
  621  kubectl get svc
  622  kubectl delete my-release
  623  kubectl delete grafana
  624  kubectl get pods --namespace=monitoring
  625  ll
  626   cd prome-grafana
  627  mkdir prome-grafana
  628  pwd
  629  cd prome-grafana/
  630  ll
  631  pwd
  632  cd
  633  cd kubernetes/
  634  ll
  635  cd monitoring/
  636  ll
  637  cp kubernetes-grafana  kubernetes-prometheus /root/prome-grafana/
  638  cp -rvf  kubernetes-grafana  kubernetes-prometheus /root/prome-grafana/
  639  ll
  640  cd 
  641  cd prome-grafana/
  642  ll
  643  kubectl create namespace monitoring
  644  #kubectl create -f clusterRole.yaml
  645  cd kubernetes-prometheus/
  646  kubectl create -f clusterRole.yaml
  647  kubectl create -f config-map.yaml
  648  kubectl create -f prometheus-deployment.yaml
  649  kubectl get deployments --namespace=monitoring
  650  kubectl get pods --namespace=monitoring
  651  kubectl port-forward prometheus-deployment-599bbd9457-gnpvp 8080:9090 -n monitoring
  652  kubectl port-forward prometheus-monitoring-3331088907-hm5n1 8080:9090 -n monitoringkubectl get pods --namespace=monitoring
  653  kubectl get pods --namespace=monitoring
  654  kubectl port-forward prometheus-deployment-599bbd9457-gnpvp 8080:9090 -n monitoring
  655  ll
  656  kubectl get svc
  657  kubectl get svc all
  658  kubectl get svc --all
  659  kubectl get svc
  660  kubectl port-forward prometheus-deployment-599bbd9457-gnpvp 8080:9090 -n monitoring
  661  kubectltl get pods
  662  kubectl get pods
  663  kubectl get pods --all-namespace
  664  kubectl get pods -all-namespace
  665  kubectl get pods --help
  666  kubectl get deployments --namespace=monitoring
  667  kubectl delete pod prometheus-deployment\
  668  kubectl delete pod prometheus-deployment
  669  kubectl get pods --namespace=monitoring
  670  kubectl delete pod prometheus-deployment-599bbd9457-gnpvp
  671  kubectl describe prometheus-deployment-599bbd9457-gnpvp
  672  kubectkubectl get pods --namespace=monitoring
  673  kubectl get pods --namespace=monitoring
  674  kubectk get namespace
  675  kubectl get namespace
  676  kubectl delete  namespace monitoring
  677  kubectl get pods --namespace=monitoring
  678  ll
  679  kubectl get pods
  680  $ helm install prometheus stable/prometheus-operator --namespace prometheus
  681   helm install prometheus stable/prometheus-operator --namespace prometheus
  682  $ kubectl create namespace prometheus
  683   kubectl create namespace prometheus
  684  $ helm install prometheus stable/prometheus-operator --namespace prometheus
  685   helm install prometheus stable/prometheus-operator --namespace prometheus
  686  ll
  687  git clone https://github.com/gurpreet0610/Deploy-Prometheus-Grafana-on-Kubernetes.git
  688  ll
  689  cd Deploy-Prometheus-Grafana-on-Kubernetes/
  690  ll
  691  kubectl create -f monitoring-namespace.yaml
  692  kubectl create -f prometheus-cluster-role.yaml
  693  kubectl create -f prometheus-config.yaml
  694  kubectl create -f prometheus-deployment.yaml
  695  kubectl create -f prometheus-pvc-nfs.yaml
  696  kubectl create -f prometheus-pv-nfs.yaml
  697  kubectl create -f prometheus-service.yaml
  698  kubectl get svc
  699  kubectl get pods
  700  kubectl get pods --namespace monitoring
  701  kubectl apply -k . -n monitoring
  702  kubectl get all -n monitoring
  703  cd
  704  ll
  705  mv Deploy-Prometheus-Grafana-on-Kubernetes Prometheus-Grafana
  706  ll
  707  kubectl get all -n monitoring
  708  kubectl describe pod pod/prometheus-6977fc7cfd-f882g
  709  kubectl describe pod/prometheus-6977fc7cfd-f882g
  710  kubectl get all -n monitoring
  711  kubectl delete -n monitoring
  712  kubectl delete -namespace  monitoring
  713  kls get ns all
  714  kubectl  get ns all
  715  kubectl  get ns 
  716  kubectl delete --namespace  monitoring
  717  kubectl delete namespace  monitoring
  718  ll
  719  free -h
  720  k3s
  721  cd /etc/
  722  ll
  723  ls
  724  ll
  725  cd
  726  ll
  727  cat pro-gra.yml
  728  helm repo add bitnami https://charts.bitnami.com/bitnami
  729   helm install my-release bitnami/mongodb
  730  kubectl get cluster
  731  kubectl get all
  732  ifup ens33
  733  nmtui
  734  ifup ens33
  735  nmdcli d s
  736  nmcli d s
  737  ip a
  738  nmtui
  739  ip a
  740  ping 8.8.8.8
  741  mount /dev/sr0 /mnt
  742  ll
  743  ll\
  744  ip a
  745  nmtui
  746  ip a
  747  ping 8.8.8.8
  748  netstat -nultp
  749  kubectl get svc
  750  free -h
  751  kubectl get svc
  752   helm install prometheus prometheus-community/kube-prometheus-stack
  753  ping 8.8.8.8
  754   helm install dhana prometheus-community/kube-prometheus-stack
  755  ll
  756  rm -rf prometheus-grafana.yml
  757  rm -rf pro-gra.yml
  758  helm show values prometheus-community/kube-prometheus-stack
  759  helm install my-release bitnami/grafana
  760  history
  761   export KUBECONFIG=/etc/rancher/k3s/k3s.yaml
  762  helm install my-release bitnami/grafana
  763  echo "User: admin"
  764  echo "Password: $(kubectl get secret my-release-grafana-admin --namespace default -o jsonpath="{.data.GF_SECURITY_ADMIN_PASSWORD}" | base64 --decode)"
  765  kubectl get svc
  766  firewall-cmd --list-all
  767  firewall-cmd --perm -add-port=3000/tcp
  768  firewall-cmd --perm --add-port=3000/tcp
  769  firewall-cmd --reload
  770  kubectl get svc
  771  kubectl get all
  772  kubectl port-forward svc/my-release-grafana 8080:3000
  773  kubectl port-forward pod/my-release-grafana-64488d7767-blqcv 8080:3000
  774  kubectl describe pod/my-release-grafana-64488d7767-blqcv
  775  kubectl delete pod/my-release-grafana-64488d7767-blqcv
  776  kubectl get all
  777  helm delete my-release
  778   helm install dhana prometheus-community/kube-prometheus-stack
  779   helm install prometheus  prometheus-community/kube-prometheus-stack
  780  helm install my-release bitnami/grafana
  781  get svc
  782  kubectl get svc
  783  kubectl get all
  784  kubectl logs pod/my-release-grafana-67d697b64c-tl8fr
  785  kubectl describe  pod/my-release-grafana-67d697b64c-tl8fr
  786  kubectl delete  pod/my-release-grafana-67d697b64c-tl8fr
  787  kubectl get all
  788  kubectl describe  pod/my-release-grafana-67d697b64c-w9chq
  789  kubectl get all
  790  helm delete my-release
  791   helm install dhana prometheus-community/kube-prometheus-stack
  792  ll
  793   helm install dhana prometheus-community/kube-prometheus-stack
  794  helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
  795  helm repo update
  796  kubectl delete crd alertmanagerconfigs.monitoring.coreos.com
  797  kubectl delete crd alertmanagers.monitoring.coreos.com
  798  kubectl delete crd podmonitors.monitoring.coreos.com
  799  kubectl delete crd probes.monitoring.coreos.com
  800  kubectl delete crd prometheuses.monitoring.coreos.com
  801  kubectl delete crd prometheusrules.monitoring.coreos.com
  802  kubectl delete crd servicemonitors.monitoring.coreos.com
  803  kubectl delete crd thanosrulers.monitoring.coreos.com
  804  helm show values kube-prometheus-stack
  805  helm repo list
  806  helm repo prometheus-community
  807  kubectl show values prometheus-community/kube-prometheus-stack
  808  helm  show values prometheus-community/kube-prometheus-stack
  809  helm  show values prometheus-community/kube-prometheus-stack >  prometheus-grafana.yml
  810  cat prometheus-grafana.yml
  811  vim  prometheus-grafana.yml
  812   helm install prometheus prometheus-community/kube-prometheus-stack
  813  helm install  prometheus prometheus-community/kube-prometheus-stack
  814  history
  815  export KUBECONFIG=/etc/rancher/k3s/k3s.yaml
  816  ! 879
  817  helm install  prometheus prometheus-community/kube-prometheus-stack
  818  export KUBECONFIG=/etc/rancher/k3s/k3s.yaml
  819  helm install  prometheus prometheus-community/kube-prometheus-stack
  820  helm install  prometheus1 prometheus-community/kube-prometheus-stack
  821  helm delete  prometheus1
  822  helm delete  prometheus
  823  kubectl delete crd alertmanagerconfigs.monitoring.coreos.com
  824  kubectl delete crd alertmanagers.monitoring.coreos.com
  825  kubectl delete crd podmonitors.monitoring.coreos.com
  826  kubectl delete crd probes.monitoring.coreos.com
  827  kubectl delete crd prometheuses.monitoring.coreos.com
  828  kubectl delete crd prometheusrules.monitoring.coreos.com
  829  kubectl delete crd servicemonitors.monitoring.coreos.com
  830  kubectl delete crd thanosrulers.monitoring.coreos.com
  831  helm install my-release bitnami/grafana
  832  history
  833  helm install my-release bitnami/grafana
  834  export KUBECONFIG=/etc/rancher/k3s/k3s.yaml
  835  helm install my-release bitnami/grafana
  836  kubectl get all
  837  kubectl describe pod/prometheus1-kube-prometheu-admission-create--1-rcs8h
  838  kubectl get all
  839  kubectl delete pod/prometheus1-kube-prometheu-admission-create--1-rcs8h
  840  kubectl delete replicaset.apps/my-release-grafana-599d8b86b4
  841  helm delete my-release
  842  kubectl get all
  843  kubectl delete service/kubernetes
  844  kubectl delete pod/prometheus1-kube-prometheu-admission-create--1-v4wlm
  845  kubectl delete job.batch/prometheus-kube-prometheus-admission-create
  846  kubectl delete job.batch/prometheus1-kube-prometheu-admission-create
  847  kubectl get all
  848  kubectl delete service/kubernetes
  849  kubectl get all
  850  kubectl get po
  851  kubectl get svc
  852  kubectl get no
  853  top
  854  kubectl get all
  855  helm delete prometheus1
  856  history
  857   export KUBECONFIG=/etc/rancher/k3s/k3s.yaml
  858  helm delete prometheus1
  859   export KUBECONFIG=/etc/rancher/k3s/k3s.yaml
  860  helm delete prometheus1
  861   export KUBECONFIG=/etc/rancher/k3s/k3s.yaml
  862  kubectl get all
  863  kubectl get po
  864  kubectl get svc
  865  kubectl get ns
  866  helm delete prometheus1
  867  /usr/local/bin/k3s-uninstall.sh
  868  ll
  869  free -h
  870  init 6
  871  kubectl get po
  872  kubectl get svc
  873  kubectl svc
  874  kubectl get svc
  875  kubectl get all
  876  kubectl port-forward pod/prometheus1-grafana-c77c67fd-9xc2f  80:3000
  877  ll
  878   helm search repo prometheus-community
  879  helm show values prometheus-community/kube-prometheus-stack
  880  helm show values prometheus-community/kube-prometheus-stack > kube-prometheus-stack.yml
  881  ll
  882  vim kube-prometheus-stack.yml
  883  cp kube-prometheus-stack.yml /home/dhana/
  884  vim kube-prometheus-stack.yml
  885  kubectl create -f kube-prometheus-stack.yml
  886  curl -sfL https://get.k3s.io | sh -
  887  kubectl create -f kube-prometheus-stack.yml
  888  kubectl create -f kube-prometheus-stack.yml  --validate=false
  889  init 6
  890  kubectl
  891  kubectl api-resources
  892  kubectl
  893  kubectl api-versions
  894  kubectl 
  895  mount /dev/sr0 /mnt
  896  yum install httpd
  897  kill -9 10212
  898  yum install httpd
  899  systemctl status httpd
  900  init 3
  901  kubectl get nodes
  902  kubectl get po
  903  kubectl get all
  904  init 5
  905  ll
  906  kubectl get po
  907  kubectl cerate -f  metricbeat-kubernetes.yaml
  908  kubectl create -f  metricbeat-kubernetes.yaml
  909  kubectl delete -f  metricbeat-kubernetes.yaml
  910  rm -rf metricbeat-kubernetes.yaml
  911  mv kube-metricbeat.yml /home/dhana/
  912  ll
  913  kubectl create -f  values-metricbeat.yaml
  914  kubectl create -f  values-metricbeat.yaml --validate=false
  915  helm repo all
  916  helm repo list
  917  history
  918  helm install --name metricbeat -f values-metricbeat.yaml elastic/metricbeat
  919  helm install -name metricbeat -f values-metricbeat.yaml elastic/metricbeat
  920  export KUBECONFIG=/etc/rancher/k3s/k3s.yaml
  921  helm install -name metricbeat -f values-metricbeat.yaml elastic/metricbeat
  922  helm install -name metricbeat1 -f values-metricbeat.yaml elastic/metricbeat
  923   kubectl get pods --namespace=ame -l app=metricbeat1-metricbeat -w
  924  kubectl describe metricbeat1-metricbeat-qhvj9 -n ame
  925  kubectl describe pods  metricbeat1-metricbeat-qhvj9 -n ame
  926  kubectl logs  metricbeat1-metricbeat-qhvj9 -n ame
  927  kubectl edit  metricbeat1-metricbeat-qhvj9 -n ame
  928   kubectl get pods --namespace=ame -l app=metricbeat1-metricbeat -w
  929  helm uninstall -name metricbeat1 -f values-metricbeat.yaml elastic/metricbeat
  930  helm delete metricbeat1
  931  helm uninstall  metricbeat1
  932  export KUBECONFIG=/etc/rancher/k3s/k3s.yaml
  933  helm uninstall  metricbeat1
  934  history
]0;root@localhost:~[root@localhost ~]# history[K[K[K[K[K[K[Kl[K[H[2J[root@localhost ~]# helm uninstall  metricbeat1[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[Cexport KUBECONFIG=/etc/rancher/k3s/k3s.yaml[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[16Phelm uninstall  metricbeat1
Error: uninstall: Release not loaded: metricbeat1: release: not found
]0;root@localhost:~[root@localhost ~]# helm uninstall  metricbeat1[K
Error: uninstall: Release not loaded: metricbeat: release: not found
]0;root@localhost:~[root@localhost ~]# helm uninstall  metricbeat1
Error: uninstall: Release not loaded: metricbeat1: release: not found
]0;root@localhost:~[root@localhost ~]# helm [K[K[K[K[Kll
total 156
-rw-------.  1 root root  2227 Mar 21 19:14 anaconda-ks.cfg
-rwx------.  1 root root 11156 Mar 23 22:44 [0m[01;32mget_helm.sh[0m
-rw-r--r--.  1 root root  2320 Mar 21 19:43 initial-setup-ks.cfg
-rw-r--r--.  1 root root 93490 Mar 22 23:05 kube-prometheus-stack.yml
drwxr-xr-x. 31 root root  4096 Mar 21 21:21 [01;34mkubernetes[0m
drwxr-xr-x.  9 root root  4096 Mar 21 14:30 [01;34mlocal-repo[0m
-rw-r--r--.  1 root root  7565 Apr  8 21:00 Metricbeat-kube-not.yml
-rw-r--r--.  1 root root   104 Mar 20 14:31 mongo-configmap.yaml
-rw-r--r--.  1 root root  1114 Mar 20 14:32 mongo-express.yaml
-rw-r--r--.  1 root root   158 Mar 20 14:32 mongo-secret.yaml
-rw-r--r--.  1 root root   862 Mar 20 14:31 mongo.yaml
drwxr-xr-x.  4 root root  4096 Mar 21 21:26 [01;34mprome-grafana[0m
drwxr-xr-x.  4 root root  4096 Mar 21 22:01 [01;34mPrometheus-Grafana[0m
-rw-r--r--.  1 root root  4077 Apr  8 22:23 values-metricbeat.yaml
]0;root@localhost:~[root@localhost ~]# mount .[K/dev/se[Kr0 /mnt/
mount: /dev/sr0 is write-protected, mounting read-only
]0;root@localhost:~[root@localhost ~]# yum install git
Loaded plugins: langpacks, product-id, search-disabled-repos, subscription-manager

This system is not registered with an entitlement server. You can use subscription-manager to register.

Package git-1.8.3.1-23.el7_8.x86_64 already installed and latest version
Nothing to do
]0;root@localhost:~[root@localhost ~]# git clone git@github.com:ahmetb/kubectx.git
Cloning into 'kubectx'...
The authenticity of host 'github.com (13.234.176.102)' can't be established.
ECDSA key fingerprint is SHA256:p2QAMXNIC1TJYWeIOttrVc98/R1BUFWu3/LiyKgUfQM.
ECDSA key fingerprint is MD5:7b:99:81:1e:4c:91:a5:0d:5a:2e:2e:80:13:3f:24:ca.
Are you sure you want to continue connecting (yes/no)? yes
Warning: Permanently added 'github.com,13.234.176.102' (ECDSA) to the list of known hosts.
Permission denied (publickey).
fatal: Could not read from remote repository.

Please make sure you have the correct access rights
and the repository exists.
]0;root@localhost:~[root@localhost ~]# git clone git@github.com:ahmetb/kubectx.git
Cloning into 'kubectx'...
Permission denied (publickey).
fatal: Could not read from remote repository.

Please make sure you have the correct access rights
and the repository exists.
]0;root@localhost:~[root@localhost ~]# ll
total 156
-rw-------.  1 root root  2227 Mar 21 19:14 anaconda-ks.cfg
-rwx------.  1 root root 11156 Mar 23 22:44 [0m[01;32mget_helm.sh[0m
-rw-r--r--.  1 root root  2320 Mar 21 19:43 initial-setup-ks.cfg
-rw-r--r--.  1 root root 93490 Mar 22 23:05 kube-prometheus-stack.yml
drwxr-xr-x. 31 root root  4096 Mar 21 21:21 [01;34mkubernetes[0m
drwxr-xr-x.  9 root root  4096 Mar 21 14:30 [01;34mlocal-repo[0m
-rw-r--r--.  1 root root  7565 Apr  8 21:00 Metricbeat-kube-not.yml
-rw-r--r--.  1 root root   104 Mar 20 14:31 mongo-configmap.yaml
-rw-r--r--.  1 root root  1114 Mar 20 14:32 mongo-express.yaml
-rw-r--r--.  1 root root   158 Mar 20 14:32 mongo-secret.yaml
-rw-r--r--.  1 root root   862 Mar 20 14:31 mongo.yaml
drwxr-xr-x.  4 root root  4096 Mar 21 21:26 [01;34mprome-grafana[0m
drwxr-xr-x.  4 root root  4096 Mar 21 22:01 [01;34mPrometheus-Grafana[0m
-rw-r--r--.  1 root root  4077 Apr  8 22:23 values-metricbeat.yaml
]0;root@localhost:~[root@localhost ~]# llgit clone git@github.com:ahmetb/kubectx.git[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[Khttps://github.com/ahmetb/kubectx.git
Cloning into 'kubectx'...
remote: Enumerating objects: 1460, done.[K
remote: Counting objects:   0% (1/175)[Kremote: Counting objects:   1% (2/175)[Kremote: Counting objects:   2% (4/175)[Kremote: Counting objects:   3% (6/175)[Kremote: Counting objects:   4% (7/175)[Kremote: Counting objects:   5% (9/175)[Kremote: Counting objects:   6% (11/175)[Kremote: Counting objects:   7% (13/175)[Kremote: Counting objects:   8% (14/175)[Kremote: Counting objects:   9% (16/175)[Kremote: Counting objects:  10% (18/175)[Kremote: Counting objects:  11% (20/175)[Kremote: Counting objects:  12% (21/175)[Kremote: Counting objects:  13% (23/175)[Kremote: Counting objects:  14% (25/175)[Kremote: Counting objects:  15% (27/175)[Kremote: Counting objects:  16% (28/175)[Kremote: Counting objects:  17% (30/175)[Kremote: Counting objects:  18% (32/175)[Kremote: Counting objects:  19% (34/175)[Kremote: Counting objects:  20% (35/175)[Kremote: Counting objects:  21% (37/175)[Kremote: Counting objects:  22% (39/175)[Kremote: Counting objects:  23% (41/175)[Kremote: Counting objects:  24% (42/175)[Kremote: Counting objects:  25% (44/175)[Kremote: Counting objects:  26% (46/175)[Kremote: Counting objects:  27% (48/175)[Kremote: Counting objects:  28% (49/175)[Kremote: Counting objects:  29% (51/175)[Kremote: Counting objects:  30% (53/175)[Kremote: Counting objects:  31% (55/175)[Kremote: Counting objects:  32% (56/175)[Kremote: Counting objects:  33% (58/175)[Kremote: Counting objects:  34% (60/175)[Kremote: Counting objects:  35% (62/175)[Kremote: Counting objects:  36% (63/175)[Kremote: Counting objects:  37% (65/175)[Kremote: Counting objects:  38% (67/175)[Kremote: Counting objects:  39% (69/175)[Kremote: Counting objects:  40% (70/175)[Kremote: Counting objects:  41% (72/175)[Kremote: Counting objects:  42% (74/175)[Kremote: Counting objects:  43% (76/175)[Kremote: Counting objects:  44% (77/175)[Kremote: Counting objects:  45% (79/175)[Kremote: Counting objects:  46% (81/175)[Kremote: Counting objects:  47% (83/175)[Kremote: Counting objects:  48% (84/175)[Kremote: Counting objects:  49% (86/175)[Kremote: Counting objects:  50% (88/175)[Kremote: Counting objects:  51% (90/175)[Kremote: Counting objects:  52% (91/175)[Kremote: Counting objects:  53% (93/175)[Kremote: Counting objects:  54% (95/175)[Kremote: Counting objects:  55% (97/175)[Kremote: Counting objects:  56% (98/175)[Kremote: Counting objects:  57% (100/175)[Kremote: Counting objects:  58% (102/175)[Kremote: Counting objects:  59% (104/175)[Kremote: Counting objects:  60% (105/175)[Kremote: Counting objects:  61% (107/175)[Kremote: Counting objects:  62% (109/175)[Kremote: Counting objects:  63% (111/175)[Kremote: Counting objects:  64% (112/175)[Kremote: Counting objects:  65% (114/175)[Kremote: Counting objects:  66% (116/175)[Kremote: Counting objects:  67% (118/175)[Kremote: Counting objects:  68% (119/175)[Kremote: Counting objects:  69% (121/175)[Kremote: Counting objects:  70% (123/175)[Kremote: Counting objects:  71% (125/175)[Kremote: Counting objects:  72% (126/175)[Kremote: Counting objects:  73% (128/175)[Kremote: Counting objects:  74% (130/175)[Kremote: Counting objects:  75% (132/175)[Kremote: Counting objects:  76% (133/175)[Kremote: Counting objects:  77% (135/175)[Kremote: Counting objects:  78% (137/175)[Kremote: Counting objects:  79% (139/175)[Kremote: Counting objects:  80% (140/175)[Kremote: Counting objects:  81% (142/175)[Kremote: Counting objects:  82% (144/175)[Kremote: Counting objects:  83% (146/175)[Kremote: Counting objects:  84% (147/175)[Kremote: Counting objects:  85% (149/175)[Kremote: Counting objects:  86% (151/175)[Kremote: Counting objects:  87% (153/175)[Kremote: Counting objects:  88% (154/175)[Kremote: Counting objects:  89% (156/175)[Kremote: Counting objects:  90% (158/175)[Kremote: Counting objects:  91% (160/175)[Kremote: Counting objects:  92% (161/175)[Kremote: Counting objects:  93% (163/175)[Kremote: Counting objects:  94% (165/175)[Kremote: Counting objects:  95% (167/175)[Kremote: Counting objects:  96% (168/175)[Kremote: Counting objects:  97% (170/175)[Kremote: Counting objects:  98% (172/175)[Kremote: Counting objects:  99% (174/175)[Kremote: Counting objects: 100% (175/175)[Kremote: Counting objects: 100% (175/175), done.[K
remote: Compressing objects:   0% (1/118)[Kremote: Compressing objects:   1% (2/118)[Kremote: Compressing objects:   2% (3/118)[Kremote: Compressing objects:   3% (4/118)[Kremote: Compressing objects:   4% (5/118)[Kremote: Compressing objects:   5% (6/118)[Kremote: Compressing objects:   6% (8/118)[Kremote: Compressing objects:   7% (9/118)[Kremote: Compressing objects:   8% (10/118)[Kremote: Compressing objects:   9% (11/118)[Kremote: Compressing objects:  10% (12/118)[Kremote: Compressing objects:  11% (13/118)[Kremote: Compressing objects:  12% (15/118)[Kremote: Compressing objects:  13% (16/118)[Kremote: Compressing objects:  14% (17/118)[Kremote: Compressing objects:  15% (18/118)[Kremote: Compressing objects:  16% (19/118)[Kremote: Compressing objects:  17% (21/118)[Kremote: Compressing objects:  18% (22/118)[Kremote: Compressing objects:  19% (23/118)[Kremote: Compressing objects:  20% (24/118)[Kremote: Compressing objects:  21% (25/118)[Kremote: Compressing objects:  22% (26/118)[Kremote: Compressing objects:  23% (28/118)[Kremote: Compressing objects:  24% (29/118)[Kremote: Compressing objects:  25% (30/118)[Kremote: Compressing objects:  26% (31/118)[Kremote: Compressing objects:  27% (32/118)[Kremote: Compressing objects:  28% (34/118)[Kremote: Compressing objects:  29% (35/118)[Kremote: Compressing objects:  30% (36/118)[Kremote: Compressing objects:  31% (37/118)[Kremote: Compressing objects:  32% (38/118)[Kremote: Compressing objects:  33% (39/118)[Kremote: Compressing objects:  34% (41/118)[Kremote: Compressing objects:  35% (42/118)[Kremote: Compressing objects:  36% (43/118)[Kremote: Compressing objects:  37% (44/118)[Kremote: Compressing objects:  38% (45/118)[Kremote: Compressing objects:  39% (47/118)[Kremote: Compressing objects:  40% (48/118)[Kremote: Compressing objects:  41% (49/118)[Kremote: Compressing objects:  42% (50/118)[Kremote: Compressing objects:  43% (51/118)[Kremote: Compressing objects:  44% (52/118)[Kremote: Compressing objects:  45% (54/118)[Kremote: Compressing objects:  46% (55/118)[Kremote: Compressing objects:  47% (56/118)[Kremote: Compressing objects:  48% (57/118)[Kremote: Compressing objects:  49% (58/118)[Kremote: Compressing objects:  50% (59/118)[Kremote: Compressing objects:  51% (61/118)[Kremote: Compressing objects:  52% (62/118)[Kremote: Compressing objects:  53% (63/118)[Kremote: Compressing objects:  54% (64/118)[Kremote: Compressing objects:  55% (65/118)[Kremote: Compressing objects:  56% (67/118)[Kremote: Compressing objects:  57% (68/118)[Kremote: Compressing objects:  58% (69/118)[Kremote: Compressing objects:  59% (70/118)[Kremote: Compressing objects:  60% (71/118)[Kremote: Compressing objects:  61% (72/118)[Kremote: Compressing objects:  62% (74/118)[Kremote: Compressing objects:  63% (75/118)[Kremote: Compressing objects:  64% (76/118)[Kremote: Compressing objects:  65% (77/118)[Kremote: Compressing objects:  66% (78/118)[Kremote: Compressing objects:  67% (80/118)[Kremote: Compressing objects:  68% (81/118)[Kremote: Compressing objects:  69% (82/118)[Kremote: Compressing objects:  70% (83/118)[Kremote: Compressing objects:  71% (84/118)[Kremote: Compressing objects:  72% (85/118)[Kremote: Compressing objects:  73% (87/118)[Kremote: Compressing objects:  74% (88/118)[Kremote: Compressing objects:  75% (89/118)[Kremote: Compressing objects:  76% (90/118)[Kremote: Compressing objects:  77% (91/118)[Kremote: Compressing objects:  78% (93/118)[Kremote: Compressing objects:  79% (94/118)[Kremote: Compressing objects:  80% (95/118)[Kremote: Compressing objects:  81% (96/118)[Kremote: Compressing objects:  82% (97/118)[Kremote: Compressing objects:  83% (98/118)[Kremote: Compressing objects:  84% (100/118)[Kremote: Compressing objects:  85% (101/118)[Kremote: Compressing objects:  86% (102/118)[Kremote: Compressing objects:  87% (103/118)[Kremote: Compressing objects:  88% (104/118)[Kremote: Compressing objects:  89% (106/118)[Kremote: Compressing objects:  90% (107/118)[Kremote: Compressing objects:  91% (108/118)[Kremote: Compressing objects:  92% (109/118)[Kremote: Compressing objects:  93% (110/118)[Kremote: Compressing objects:  94% (111/118)[Kremote: Compressing objects:  95% (113/118)[Kremote: Compressing objects:  96% (114/118)[Kremote: Compressing objects:  97% (115/118)[Kremote: Compressing objects:  98% (116/118)[Kremote: Compressing objects:  99% (117/118)[Kremote: Compressing objects: 100% (118/118)[Kremote: Compressing objects: 100% (118/118), done.[K
Receiving objects:   0% (1/1460)   Receiving objects:   1% (15/1460)   Receiving objects:   2% (30/1460)   Receiving objects:   3% (44/1460)   Receiving objects:   4% (59/1460)   Receiving objects:   5% (73/1460)   Receiving objects:   6% (88/1460)   Receiving objects:   7% (103/1460)   Receiving objects:   8% (117/1460)   Receiving objects:   9% (132/1460)   Receiving objects:  10% (146/1460)   Receiving objects:  11% (161/1460)   Receiving objects:  12% (176/1460)   Receiving objects:  13% (190/1460)   Receiving objects:  14% (205/1460)   Receiving objects:  15% (219/1460)   Receiving objects:  16% (234/1460)   Receiving objects:  17% (249/1460)   Receiving objects:  18% (263/1460)   Receiving objects:  19% (278/1460)   Receiving objects:  20% (292/1460)   Receiving objects:  21% (307/1460)   Receiving objects:  22% (322/1460)   Receiving objects:  23% (336/1460)   Receiving objects:  24% (351/1460)   Receiving objects:  25% (365/1460)   Receiving objects:  26% (380/1460)   Receiving objects:  27% (395/1460)   Receiving objects:  28% (409/1460)   Receiving objects:  29% (424/1460)   Receiving objects:  30% (438/1460)   Receiving objects:  31% (453/1460)   Receiving objects:  32% (468/1460)   Receiving objects:  33% (482/1460)   Receiving objects:  34% (497/1460)   Receiving objects:  35% (511/1460)   Receiving objects:  36% (526/1460)   Receiving objects:  37% (541/1460), 724.01 KiB | 1.40 MiB/s   Receiving objects:  38% (555/1460), 724.01 KiB | 1.40 MiB/s   Receiving objects:  39% (570/1460), 724.01 KiB | 1.40 MiB/s   Receiving objects:  40% (584/1460), 724.01 KiB | 1.40 MiB/s   Receiving objects:  41% (599/1460), 724.01 KiB | 1.40 MiB/s   Receiving objects:  42% (614/1460), 724.01 KiB | 1.40 MiB/s   Receiving objects:  43% (628/1460), 724.01 KiB | 1.40 MiB/s   Receiving objects:  44% (643/1460), 724.01 KiB | 1.40 MiB/s   Receiving objects:  45% (657/1460), 724.01 KiB | 1.40 MiB/s   Receiving objects:  46% (672/1460), 724.01 KiB | 1.40 MiB/s   Receiving objects:  47% (687/1460), 724.01 KiB | 1.40 MiB/s   Receiving objects:  48% (701/1460), 724.01 KiB | 1.40 MiB/s   Receiving objects:  49% (716/1460), 724.01 KiB | 1.40 MiB/s   Receiving objects:  50% (730/1460), 724.01 KiB | 1.40 MiB/s   Receiving objects:  51% (745/1460), 724.01 KiB | 1.40 MiB/s   Receiving objects:  52% (760/1460), 724.01 KiB | 1.40 MiB/s   Receiving objects:  53% (774/1460), 724.01 KiB | 1.40 MiB/s   Receiving objects:  54% (789/1460), 724.01 KiB | 1.40 MiB/s   Receiving objects:  55% (803/1460), 724.01 KiB | 1.40 MiB/s   Receiving objects:  56% (818/1460), 724.01 KiB | 1.40 MiB/s   Receiving objects:  57% (833/1460), 724.01 KiB | 1.40 MiB/s   Receiving objects:  58% (847/1460), 724.01 KiB | 1.40 MiB/s   Receiving objects:  59% (862/1460), 724.01 KiB | 1.40 MiB/s   Receiving objects:  60% (876/1460), 724.01 KiB | 1.40 MiB/s   Receiving objects:  61% (891/1460), 724.01 KiB | 1.40 MiB/s   Receiving objects:  62% (906/1460), 724.01 KiB | 1.40 MiB/s   Receiving objects:  63% (920/1460), 724.01 KiB | 1.40 MiB/s   Receiving objects:  64% (935/1460), 724.01 KiB | 1.40 MiB/s   Receiving objects:  65% (949/1460), 724.01 KiB | 1.40 MiB/s   Receiving objects:  66% (964/1460), 724.01 KiB | 1.40 MiB/s   Receiving objects:  67% (979/1460), 724.01 KiB | 1.40 MiB/s   Receiving objects:  68% (993/1460), 724.01 KiB | 1.40 MiB/s   Receiving objects:  69% (1008/1460), 724.01 KiB | 1.40 MiB/s   Receiving objects:  70% (1022/1460), 724.01 KiB | 1.40 MiB/s   Receiving objects:  71% (1037/1460), 724.01 KiB | 1.40 MiB/s   Receiving objects:  72% (1052/1460), 724.01 KiB | 1.40 MiB/s   Receiving objects:  73% (1066/1460), 724.01 KiB | 1.40 MiB/s   Receiving objects:  74% (1081/1460), 724.01 KiB | 1.40 MiB/s   Receiving objects:  75% (1095/1460), 724.01 KiB | 1.40 MiB/s   Receiving objects:  76% (1110/1460), 724.01 KiB | 1.40 MiB/s   Receiving objects:  77% (1125/1460), 724.01 KiB | 1.40 MiB/s   Receiving objects:  78% (1139/1460), 724.01 KiB | 1.40 MiB/s   Receiving objects:  79% (1154/1460), 724.01 KiB | 1.40 MiB/s   Receiving objects:  80% (1168/1460), 724.01 KiB | 1.40 MiB/s   Receiving objects:  81% (1183/1460), 724.01 KiB | 1.40 MiB/s   Receiving objects:  82% (1198/1460), 724.01 KiB | 1.40 MiB/s   Receiving objects:  83% (1212/1460), 724.01 KiB | 1.40 MiB/s   Receiving objects:  84% (1227/1460), 724.01 KiB | 1.40 MiB/s   Receiving objects:  85% (1241/1460), 724.01 KiB | 1.40 MiB/s   Receiving objects:  86% (1256/1460), 724.01 KiB | 1.40 MiB/s   Receiving objects:  87% (1271/1460), 724.01 KiB | 1.40 MiB/s   Receiving objects:  88% (1285/1460), 724.01 KiB | 1.40 MiB/s   Receiving objects:  89% (1300/1460), 724.01 KiB | 1.40 MiB/s   Receiving objects:  90% (1314/1460), 724.01 KiB | 1.40 MiB/s   Receiving objects:  91% (1329/1460), 724.01 KiB | 1.40 MiB/s   Receiving objects:  92% (1344/1460), 724.01 KiB | 1.40 MiB/s   Receiving objects:  93% (1358/1460), 724.01 KiB | 1.40 MiB/s   Receiving objects:  94% (1373/1460), 724.01 KiB | 1.40 MiB/s   Receiving objects:  95% (1387/1460), 724.01 KiB | 1.40 MiB/s   Receiving objects:  96% (1402/1460), 724.01 KiB | 1.40 MiB/s   remote: Total 1460 (delta 87), reused 97 (delta 51), pack-reused 1285[K
Receiving objects:  97% (1417/1460), 724.01 KiB | 1.40 MiB/s   Receiving objects:  98% (1431/1460), 724.01 KiB | 1.40 MiB/s   Receiving objects:  99% (1446/1460), 724.01 KiB | 1.40 MiB/s   Receiving objects: 100% (1460/1460), 724.01 KiB | 1.40 MiB/s   Receiving objects: 100% (1460/1460), 905.99 KiB | 1.40 MiB/s, done.
Resolving deltas:   0% (0/819)   Resolving deltas:   3% (27/819)   Resolving deltas:   4% (33/819)   Resolving deltas:   6% (51/819)   Resolving deltas:   7% (64/819)   Resolving deltas:   9% (79/819)   Resolving deltas:  10% (89/819)   Resolving deltas:  12% (99/819)   Resolving deltas:  13% (112/819)   Resolving deltas:  14% (115/819)   Resolving deltas:  15% (128/819)   Resolving deltas:  17% (141/819)   Resolving deltas:  20% (164/819)   Resolving deltas:  21% (174/819)   Resolving deltas:  22% (181/819)   Resolving deltas:  23% (190/819)   Resolving deltas:  24% (197/819)   Resolving deltas:  25% (206/819)   Resolving deltas:  26% (213/819)   Resolving deltas:  27% (223/819)   Resolving deltas:  28% (231/819)   Resolving deltas:  29% (242/819)   Resolving deltas:  30% (248/819)   Resolving deltas:  31% (254/819)   Resolving deltas:  34% (283/819)   Resolving deltas:  35% (289/819)   Resolving deltas:  36% (298/819)   Resolving deltas:  37% (305/819)   Resolving deltas:  38% (312/819)   Resolving deltas:  39% (322/819)   Resolving deltas:  50% (410/819)   Resolving deltas:  52% (428/819)   Resolving deltas:  53% (442/819)   Resolving deltas:  54% (445/819)   Resolving deltas:  55% (454/819)   Resolving deltas:  56% (459/819)   Resolving deltas:  60% (492/819)   Resolving deltas:  61% (501/819)   Resolving deltas:  62% (508/819)   Resolving deltas:  63% (516/819)   Resolving deltas:  64% (529/819)   Resolving deltas:  65% (539/819)   Resolving deltas:  66% (541/819)   Resolving deltas:  69% (567/819)   Resolving deltas:  73% (602/819)   Resolving deltas:  74% (607/819)   Resolving deltas:  75% (617/819)   Resolving deltas:  77% (636/819)   Resolving deltas:  78% (641/819)   Resolving deltas:  79% (648/819)   Resolving deltas:  81% (668/819)   Resolving deltas:  83% (685/819)   Resolving deltas:  84% (694/819)   Resolving deltas:  85% (703/819)   Resolving deltas:  86% (710/819)   Resolving deltas:  87% (718/819)   Resolving deltas:  88% (727/819)   Resolving deltas:  89% (729/819)   Resolving deltas:  92% (754/819)   Resolving deltas:  93% (768/819)   Resolving deltas:  94% (776/819)   Resolving deltas:  95% (780/819)   Resolving deltas:  97% (796/819)   Resolving deltas:  98% (806/819)   Resolving deltas:  99% (814/819)   Resolving deltas: 100% (819/819)   Resolving deltas: 100% (819/819), done.
]0;root@localhost:~[root@localhost ~]# ll
total 160
-rw-------.  1 root root  2227 Mar 21 19:14 anaconda-ks.cfg
-rwx------.  1 root root 11156 Mar 23 22:44 [0m[01;32mget_helm.sh[0m
-rw-r--r--.  1 root root  2320 Mar 21 19:43 initial-setup-ks.cfg
drwxr-xr-x. 10 root root  4096 Apr 10 13:59 [01;34mkubectx[0m
-rw-r--r--.  1 root root 93490 Mar 22 23:05 kube-prometheus-stack.yml
drwxr-xr-x. 31 root root  4096 Mar 21 21:21 [01;34mkubernetes[0m
drwxr-xr-x.  9 root root  4096 Mar 21 14:30 [01;34mlocal-repo[0m
-rw-r--r--.  1 root root  7565 Apr  8 21:00 Metricbeat-kube-not.yml
-rw-r--r--.  1 root root   104 Mar 20 14:31 mongo-configmap.yaml
-rw-r--r--.  1 root root  1114 Mar 20 14:32 mongo-express.yaml
-rw-r--r--.  1 root root   158 Mar 20 14:32 mongo-secret.yaml
-rw-r--r--.  1 root root   862 Mar 20 14:31 mongo.yaml
drwxr-xr-x.  4 root root  4096 Mar 21 21:26 [01;34mprome-grafana[0m
drwxr-xr-x.  4 root root  4096 Mar 21 22:01 [01;34mPrometheus-Grafana[0m
-rw-r--r--.  1 root root  4077 Apr  8 22:23 values-metricbeat.yaml
]0;root@localhost:~[root@localhost ~]# cd kubectx
]0;root@localhost:~/kubectx[root@localhost kubectx]# ll
total 112
drwxr-xr-x. 4 root root  4096 Apr 10 13:59 [0m[01;34mcmd[0m
drwxr-xr-x. 2 root root  4096 Apr 10 13:59 [01;34mcompletion[0m
-rw-r--r--. 1 root root   968 Apr 10 13:59 CONTRIBUTING.md
-rw-r--r--. 1 root root  2209 Apr 10 13:59 go.mod
-rw-r--r--. 1 root root 43623 Apr 10 13:59 go.sum
drwxr-xr-x. 2 root root  4096 Apr 10 13:59 [01;34mimg[0m
drwxr-xr-x. 7 root root  4096 Apr 10 13:59 [01;34minternal[0m
-rwxr-xr-x. 1 root root  6108 Apr 10 13:59 [01;32mkubectx[0m
-rwxr-xr-x. 1 root root  5555 Apr 10 13:59 [01;32mkubens[0m
-rw-r--r--. 1 root root 11357 Apr 10 13:59 LICENSE
-rw-r--r--. 1 root root  9390 Apr 10 13:59 README.md
drwxr-xr-x. 3 root root  4096 Apr 10 13:59 [01;34mtest[0m
]0;root@localhost:~/kubectx[root@localhost kubectx]# kubens
bash: kubens: command not found...
]0;root@localhost:~/kubectx[root@localhost kubectx]# cp  kubectx kubens /usr/local/bin/
]0;root@localhost:~/kubectx[root@localhost kubectx]# kubens
[40m[33mdefault(B[m
kube-system
kube-public
kube-node-lease
ame
]0;root@localhost:~/kubectx[root@localhost kubectx]# ^C
]0;root@localhost:~/kubectx[root@localhost kubectx]# kubens ame
Context "default" modified.
Active namespace is "ame".
]0;root@localhost:~/kubectx[root@localhost kubectx]# ll
total 112
drwxr-xr-x. 4 root root  4096 Apr 10 13:59 [0m[01;34mcmd[0m
drwxr-xr-x. 2 root root  4096 Apr 10 13:59 [01;34mcompletion[0m
-rw-r--r--. 1 root root   968 Apr 10 13:59 CONTRIBUTING.md
-rw-r--r--. 1 root root  2209 Apr 10 13:59 go.mod
-rw-r--r--. 1 root root 43623 Apr 10 13:59 go.sum
drwxr-xr-x. 2 root root  4096 Apr 10 13:59 [01;34mimg[0m
drwxr-xr-x. 7 root root  4096 Apr 10 13:59 [01;34minternal[0m
-rwxr-xr-x. 1 root root  6108 Apr 10 13:59 [01;32mkubectx[0m
-rwxr-xr-x. 1 root root  5555 Apr 10 13:59 [01;32mkubens[0m
-rw-r--r--. 1 root root 11357 Apr 10 13:59 LICENSE
-rw-r--r--. 1 root root  9390 Apr 10 13:59 README.md
drwxr-xr-x. 3 root root  4096 Apr 10 13:59 [01;34mtest[0m
]0;root@localhost:~/kubectx[root@localhost kubectx]# kube [Kctl get po
NAME                                              READY   STATUS             RESTARTS        AGE
metricbeat-metricbeat-metrics-9d49d77df-p8rwg     0/1     Running            2 (26m ago)     39h
metricbeat-kube-state-metrics-5f95966df4-7r46f    1/1     Running            15 (38h ago)    39h
metricbeat-metricbeat-4lvgk                       0/1     Running            12 (38h ago)    39h
metricbeat1-metricbeat-metrics-df5f5dccb-9bm7h    0/1     Running            0               14m
metricbeat1-kube-state-metrics-6bbc8cf68c-d6ssl   0/1     CrashLoopBackOff   7 (3m44s ago)   14m
metricbeat1-metricbeat-qhvj9                      0/1     CrashLoopBackOff   7 (3m18s ago)   14m
]0;root@localhost:~/kubectx[root@localhost kubectx]# kubectl get po[K[K[K[K[K[Ktop po
NAME                                             CPU(cores)   MEMORY(bytes)   
metricbeat-kube-state-metrics-5f95966df4-7r46f   1m           25Mi            
metricbeat-metricbeat-4lvgk                      32m          121Mi           
metricbeat-metricbeat-metrics-9d49d77df-p8rwg    8m           63Mi            
metricbeat1-metricbeat-metrics-df5f5dccb-9bm7h   8m           24Mi            
]0;root@localhost:~/kubectx[root@localhost kubectx]# kubectl delete metricbeat-kube-state-metrics-5f95966df4-7r46f
error: the server doesn't have a resource type "metricbeat-kube-state-metrics-5f95966df4-7r46f"
]0;root@localhost:~/kubectx[root@localhost kubectx]# kubectl delete metricbeat-kube-state-metrics-5f95966df4-7r46f[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[Kkubectl top poget[C[C[Cll[Kkubens ame[Kcp  kubectx kubens /usr/local/bin/[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[Ckubens[K[4Pllcd kubectxll[Kgit clone https://github.com/ahmetb/kubectx.git[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[Cll[Kgit clone git@github.com:ahmetb/kubectx.git[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[28Pyum install gitmount /dev/sr0 /mnt/ll[Khelm uninstall  metricbeat1[K1[Khelm uninstall  metricbeat1export KUBECONFIG=/etc/rancher/k3s/k3s.yaml[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[16Phelm uninstall  metricbeat1[4Pdelete[C[C[C[C[C[C[C[C[C[C[C[Cuninstall -name metricbeat1 -f values-metricbeat.yaml elastic/metricbeat[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[11P kubectl get pods --namespace=ame -l app=metricbeat1-metricbeat -w[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[17Pkubectl edit  metricbeat1-metricbeat-qhvj9 -n ame[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[Clogs[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[9@describe pod[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C -n ame[K[1@m[1@e[1@t[1@r[1@i[1@c[1@b[1@e[1@a[1@t[1@-[1@m[1@e[1@t[1@r[1@i[1@c[1@b[1@e[1@a[1@t[1@-[1@m[1@e[1@t[1@r[1@i[1@c[1@s[1@-[1@9[1@d[1@4[1@9[1@d[1@7[1@7[1@d[1@f[1@-[1@p[1@8[1@r[1@w[1@g[C[1@ 
Name:         metricbeat-metricbeat-metrics-9d49d77df-p8rwg
Namespace:    ame
Priority:     0
Node:         localhost.localdomain/192.168.1.30
Start Time:   Fri, 08 Apr 2022 22:26:39 +0530
Labels:       app=metricbeat-metricbeat-metrics
              chart=metricbeat-7.17.1
              pod-template-hash=9d49d77df
              release=metricbeat
Annotations:  configChecksum: df3920aa95c9238387a8c8029e3067dcb137e79166bac065526b4a4324555c8
Status:       Running
IP:           10.42.0.77
IPs:
  IP:           10.42.0.77
Controlled By:  ReplicaSet/metricbeat-metricbeat-metrics-9d49d77df
Containers:
  metricbeat:
    Container ID:  containerd://14eac63ea2cb3034c4f9f51e9f87315c318993aa8251de846438a7fdbc2dcbe5
    Image:         docker.elastic.co/beats/metricbeat:7.3.0
    Image ID:      docker.elastic.co/beats/metricbeat@sha256:8821ae9b18ba923ca435f621de4b0003f05b5733b997739da550f4355852b868
    Port:          <none>
    Host Port:     <none>
    Args:
      -c
      /usr/share/metricbeat/kube-state-metrics-metricbeat.yml
      -e
      -E
      http.enabled=true
    State:          Running
      Started:      Sun, 10 Apr 2022 13:37:39 +0530
    Last State:     Terminated
      Reason:       Unknown
      Exit Code:    255
      Started:      Fri, 08 Apr 2022 22:40:37 +0530
      Finished:     Sun, 10 Apr 2022 13:37:15 +0530
    Ready:          False
    Restart Count:  2
    Limits:
      cpu:     1
      memory:  200Mi
    Requests:
      cpu:     100m
      memory:  100Mi
    Liveness:  exec [sh -c #!/usr/bin/env bash -e
curl --fail 127.0.0.1:5066
] delay=10s timeout=5s period=10s #success=1 #failure=3
    Readiness:  exec [sh -c #!/usr/bin/env bash -e
metricbeat test output
] delay=10s timeout=5s period=10s #success=1 #failure=3
    Environment:
      POD_NAMESPACE:             ame (v1:metadata.namespace)
      KUBE_STATE_METRICS_HOSTS:  $(METRICBEAT_KUBE_STATE_METRICS_SERVICE_HOST):$(METRICBEAT_KUBE_STATE_METRICS_SERVICE_PORT_HTTP)
    Mounts:
      /usr/share/metricbeat/kube-state-metrics-metricbeat.yml from metricbeat-config (ro,path="kube-state-metrics-metricbeat.yml")
      /usr/share/metricbeat/metricbeat.yml from metricbeat-config (ro,path="metricbeat.yml")
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-rf975 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             False 
  ContainersReady   False 
  PodScheduled      True 
Volumes:
  metricbeat-config:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      metricbeat-metricbeat-config
    Optional:  false
  kube-api-access-rf975:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason       Age                  From               Message
  ----     ------       ----                 ----               -------
  Normal   Scheduled    39h                  default-scheduler  Successfully assigned ame/metricbeat-metricbeat-metrics-9d49d77df-p8rwg to localhost.localdomain
  Normal   Pulling      39h                  kubelet            Pulling image "docker.elastic.co/beats/metricbeat:7.3.0"
  Normal   Pulled       39h                  kubelet            Successfully pulled image "docker.elastic.co/beats/metricbeat:7.3.0" in 11m53.87628007s
  Warning  FailedMount  39h                  kubelet            MountVolume.SetUp failed for volume "metricbeat-config" : failed to sync configmap cache: timed out waiting for the condition
  Normal   Killing      39h                  kubelet            Container metricbeat failed liveness probe, will be restarted
  Warning  Unhealthy    39h                  kubelet            Readiness probe errored: rpc error: code = NotFound desc = failed to exec in container: failed to load task: no running task found: task 5137b453913f625fd756b28c1adbe2cb91076f59a232eb32671cb8f8a0dac681 not found: not found
  Normal   Pulled       39h                  kubelet            Container image "docker.elastic.co/beats/metricbeat:7.3.0" already present on machine
  Normal   Created      39h (x2 over 39h)    kubelet            Created container metricbeat
  Normal   Started      39h (x2 over 39h)    kubelet            Started container metricbeat
  Warning  Unhealthy    39h (x4 over 39h)    kubelet            Liveness probe failed:
  Warning  Unhealthy    38h (x172 over 39h)  kubelet            Readiness probe failed: elasticsearch: http://192.168.1.50:9200/...
  parse url... OK
  connection...
    parse host... OK
    dns lookup... OK
    addresses: 192.168.1.50
    dial up... ERROR dial tcp 192.168.1.50:9200: connect: no route to host
  Warning  Unhealthy       38h (x8 over 39h)      kubelet  Readiness probe failed:
  Normal   SandboxChanged  28m                    kubelet  Pod sandbox changed, it will be killed and re-created.
  Normal   Pulled          28m                    kubelet  Container image "docker.elastic.co/beats/metricbeat:7.3.0" already present on machine
  Normal   Created         28m                    kubelet  Created container metricbeat
  Normal   Started         28m                    kubelet  Started container metricbeat
  Warning  Unhealthy       3m18s (x167 over 27m)  kubelet  Readiness probe failed: elasticsearch: http://192.168.1.50:9200/...
  parse url... OK
  connection...
    parse host... OK
    dns lookup... OK
    addresses: 192.168.1.50
    dial up... ERROR dial tcp 192.168.1.50:9200: connect: no route to host
]0;root@localhost:~/kubectx[root@localhost kubectx]# kubectl describe pods  metricbeat-metricbeat-metrics-9d49d77df-p8rwg  -n ame[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[Kkubectl top po[K[K[K[K[K[K[K[K[K[K[K[K[K[Kkubectl get po
NAME                                              READY   STATUS             RESTARTS       AGE
metricbeat-metricbeat-metrics-9d49d77df-p8rwg     0/1     Running            2 (29m ago)    39h
metricbeat-kube-state-metrics-5f95966df4-7r46f    1/1     Running            15 (38h ago)   39h
metricbeat-metricbeat-4lvgk                       0/1     Running            12 (38h ago)   39h
metricbeat1-metricbeat-metrics-df5f5dccb-9bm7h    0/1     Running            0              17m
metricbeat1-kube-state-metrics-6bbc8cf68c-d6ssl   0/1     CrashLoopBackOff   8 (86s ago)    17m
metricbeat1-metricbeat-qhvj9                      0/1     CrashLoopBackOff   8 (57s ago)    17m
]0;root@localhost:~/kubectx[root@localhost kubectx]# helm install [K[K[K[K[K[K[K[Kdelete metricbeat
release "metricbeat" uninstalled
]0;root@localhost:~/kubectx[root@localhost kubectx]# helm delete metricbeat[Kmetricbeat1
release "metricbeat1" uninstalled
]0;root@localhost:~/kubectx[root@localhost kubectx]# cd 
]0;root@localhost:~[root@localhost ~]# kubectx
[40m[33mdefault(B[m
]0;root@localhost:~[root@localhost ~]# kubectx[K[K[Kns
default
kube-system
kube-public
kube-node-lease
[40m[33mame(B[m
]0;root@localhost:~[root@localhost ~]#  kubens default
Context "default" modified.
Active namespace is "default".
]0;root@localhost:~[root@localhost ~]# kubens default[K
[40m[33mdefault(B[m
kube-system
kube-public
kube-node-lease
ame
]0;root@localhost:~[root@localhost ~]# kubens ame
Context "default" modified.
Active namespace is "ame".
]0;root@localhost:~[root@localhost ~]# kubectl create ns dhana -o yaml
apiVersion: v1
kind: Namespace
metadata:
  creationTimestamp: "2022-04-10T08:47:55Z"
  labels:
    kubernetes.io/metadata.name: dhana
  name: dhana
  resourceVersion: "19629"
  uid: e61217c1-0106-40d4-a633-f8564acaacab
spec:
  finalizers:
  - kubernetes
status:
  phase: Active
]0;root@localhost:~[root@localhost ~]# kubectl create ns dhana -o yaml[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[Kkubectl create ns dhana -o yaml[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[Cns ame[K[K
default
kube-system
kube-public
kube-node-lease
[40m[33mame(B[m
dhana
]0;root@localhost:~[root@localhost ~]# kubensctl create ns dhana -o yaml[C[1P[1P[1P[1P[1P[1P[1@d[1@e[1@l[1@e[1@t[1@e[C[1@ 
error: unexpected -o output mode: yaml. We only support '-o name'
]0;root@localhost:~[root@localhost ~]# kubectl delete  ns dhana -o yaml[K[K
namespace "dhana" deleted
]0;root@localhost:~[root@localhost ~]# kubectl delete  ns dhana [K[K[Kget po -n kube-system
NAME                                      READY   STATUS      RESTARTS       AGE
helm-install-traefik-crd--1-fgrt5         0/1     Completed   0              18d
metrics-server-ff9dbcb6c-j797w            1/1     Running     84 (39h ago)   18d
local-path-provisioner-84bb864455-m7442   1/1     Running     13 (48m ago)   18d
svclb-traefik-74xqk                       2/2     Running     12 (48m ago)   17d
traefik-56c4b88c4b-6lv59                  1/1     Running     31 (48m ago)   17d
coredns-96cc4f57d-qlxzq                   1/1     Running     34 (48m ago)   18d
]0;root@localhost:~[root@localhost ~]# kubectl get po -n kube-system[Kkube-public
No resources found in kube-public namespace.
]0;root@localhost:~[root@localhost ~]# kubectl get po -n kube-public[Kkube-node-lease
No resources found in kube-node-lease namespace.
]0;root@localhost:~[root@localhost ~]# kubectl get po -n kube-node-lease[K[K[Kapi-resources
error: the server doesn't have a resource type "api-resources"
]0;root@localhost:~[root@localhost ~]# kubectl get api-resources[1Papi-resources[1Papi-resources[1Papi-resources[1Papi-resources
NAME                              SHORTNAMES   APIVERSION                             NAMESPACED   KIND
bindings                                       v1                                     true         Binding
componentstatuses                 cs           v1                                     false        ComponentStatus
configmaps                        cm           v1                                     true         ConfigMap
endpoints                         ep           v1                                     true         Endpoints
events                            ev           v1                                     true         Event
limitranges                       limits       v1                                     true         LimitRange
namespaces                        ns           v1                                     false        Namespace
nodes                             no           v1                                     false        Node
persistentvolumeclaims            pvc          v1                                     true         PersistentVolumeClaim
persistentvolumes                 pv           v1                                     false        PersistentVolume
pods                              po           v1                                     true         Pod
podtemplates                                   v1                                     true         PodTemplate
replicationcontrollers            rc           v1                                     true         ReplicationController
resourcequotas                    quota        v1                                     true         ResourceQuota
secrets                                        v1                                     true         Secret
serviceaccounts                   sa           v1                                     true         ServiceAccount
services                          svc          v1                                     true         Service
mutatingwebhookconfigurations                  admissionregistration.k8s.io/v1        false        MutatingWebhookConfiguration
validatingwebhookconfigurations                admissionregistration.k8s.io/v1        false        ValidatingWebhookConfiguration
customresourcedefinitions         crd,crds     apiextensions.k8s.io/v1                false        CustomResourceDefinition
apiservices                                    apiregistration.k8s.io/v1              false        APIService
controllerrevisions                            apps/v1                                true         ControllerRevision
daemonsets                        ds           apps/v1                                true         DaemonSet
deployments                       deploy       apps/v1                                true         Deployment
replicasets                       rs           apps/v1                                true         ReplicaSet
statefulsets                      sts          apps/v1                                true         StatefulSet
tokenreviews                                   authentication.k8s.io/v1               false        TokenReview
localsubjectaccessreviews                      authorization.k8s.io/v1                true         LocalSubjectAccessReview
selfsubjectaccessreviews                       authorization.k8s.io/v1                false        SelfSubjectAccessReview
selfsubjectrulesreviews                        authorization.k8s.io/v1                false        SelfSubjectRulesReview
subjectaccessreviews                           authorization.k8s.io/v1                false        SubjectAccessReview
horizontalpodautoscalers          hpa          autoscaling/v1                         true         HorizontalPodAutoscaler
cronjobs                          cj           batch/v1                               true         CronJob
jobs                                           batch/v1                               true         Job
certificatesigningrequests        csr          certificates.k8s.io/v1                 false        CertificateSigningRequest
leases                                         coordination.k8s.io/v1                 true         Lease
endpointslices                                 discovery.k8s.io/v1                    true         EndpointSlice
events                            ev           events.k8s.io/v1                       true         Event
flowschemas                                    flowcontrol.apiserver.k8s.io/v1beta1   false        FlowSchema
prioritylevelconfigurations                    flowcontrol.apiserver.k8s.io/v1beta1   false        PriorityLevelConfiguration
helmchartconfigs                               helm.cattle.io/v1                      true         HelmChartConfig
helmcharts                                     helm.cattle.io/v1                      true         HelmChart
addons                                         k3s.cattle.io/v1                       true         Addon
nodes                                          metrics.k8s.io/v1beta1                 false        NodeMetrics
pods                                           metrics.k8s.io/v1beta1                 true         PodMetrics
alertmanagerconfigs                            monitoring.coreos.com/v1alpha1         true         AlertmanagerConfig
alertmanagers                                  monitoring.coreos.com/v1               true         Alertmanager
podmonitors                                    monitoring.coreos.com/v1               true         PodMonitor
probes                                         monitoring.coreos.com/v1               true         Probe
prometheuses                                   monitoring.coreos.com/v1               true         Prometheus
prometheusrules                                monitoring.coreos.com/v1               true         PrometheusRule
servicemonitors                                monitoring.coreos.com/v1               true         ServiceMonitor
thanosrulers                                   monitoring.coreos.com/v1               true         ThanosRuler
ingressclasses                                 networking.k8s.io/v1                   false        IngressClass
ingresses                         ing          networking.k8s.io/v1                   true         Ingress
networkpolicies                   netpol       networking.k8s.io/v1                   true         NetworkPolicy
runtimeclasses                                 node.k8s.io/v1                         false        RuntimeClass
poddisruptionbudgets              pdb          policy/v1                              true         PodDisruptionBudget
podsecuritypolicies               psp          policy/v1beta1                         false        PodSecurityPolicy
clusterrolebindings                            rbac.authorization.k8s.io/v1           false        ClusterRoleBinding
clusterroles                                   rbac.authorization.k8s.io/v1           false        ClusterRole
rolebindings                                   rbac.authorization.k8s.io/v1           true         RoleBinding
roles                                          rbac.authorization.k8s.io/v1           true         Role
priorityclasses                   pc           scheduling.k8s.io/v1                   false        PriorityClass
csidrivers                                     storage.k8s.io/v1                      false        CSIDriver
csinodes                                       storage.k8s.io/v1                      false        CSINode
csistoragecapacities                           storage.k8s.io/v1beta1                 true         CSIStorageCapacity
storageclasses                    sc           storage.k8s.io/v1                      false        StorageClass
volumeattachments                              storage.k8s.io/v1                      false        VolumeAttachment
ingressroutes                                  traefik.containo.us/v1alpha1           true         IngressRoute
ingressroutetcps                               traefik.containo.us/v1alpha1           true         IngressRouteTCP
ingressrouteudps                               traefik.containo.us/v1alpha1           true         IngressRouteUDP
middlewares                                    traefik.containo.us/v1alpha1           true         Middleware
middlewaretcps                                 traefik.containo.us/v1alpha1           true         MiddlewareTCP
serverstransports                              traefik.containo.us/v1alpha1           true         ServersTransport
tlsoptions                                     traefik.containo.us/v1alpha1           true         TLSOption
tlsstores                                      traefik.containo.us/v1alpha1           true         TLSStore
traefikservices                                traefik.containo.us/v1alpha1           true         TraefikService
]0;root@localhost:~[root@localhost ~]# kubectl api-resources[K[K[K[K[K[K[K[K[Kversion
Error: unknown command "api-version" for "kubectl"

Did you mean this?
	api-versions

Run 'kubectl --help' for usage.
unknown command "api-version" for "kubectl"

Did you mean this?
	api-versions

]0;root@localhost:~[root@localhost ~]# kubectl api-version[Kapi-versions
admissionregistration.k8s.io/v1
apiextensions.k8s.io/v1
apiregistration.k8s.io/v1
apps/v1
authentication.k8s.io/v1
authorization.k8s.io/v1
autoscaling/v1
autoscaling/v2beta1
autoscaling/v2beta2
batch/v1
batch/v1beta1
certificates.k8s.io/v1
coordination.k8s.io/v1
discovery.k8s.io/v1
discovery.k8s.io/v1beta1
events.k8s.io/v1
events.k8s.io/v1beta1
flowcontrol.apiserver.k8s.io/v1beta1
helm.cattle.io/v1
k3s.cattle.io/v1
metrics.k8s.io/v1beta1
monitoring.coreos.com/v1
monitoring.coreos.com/v1alpha1
networking.k8s.io/v1
node.k8s.io/v1
node.k8s.io/v1beta1
policy/v1
policy/v1beta1
rbac.authorization.k8s.io/v1
scheduling.k8s.io/v1
storage.k8s.io/v1
storage.k8s.io/v1beta1
traefik.containo.us/v1alpha1
v1
]0;root@localhost:~[root@localhost ~]# api-versions[K[K[K[K[K[K[K[K[K[K[K[Kkubectl get servicename
error: the server doesn't have a resource type "servicename"
]0;root@localhost:~[root@localhost ~]# kubectl get servicename[Ksvc
No resources found in ame namespace.
]0;root@localhost:~[root@localhost ~]# kubens deaf[K[Kfault
Context "default" modified.
Active namespace is "default".
]0;root@localhost:~[root@localhost ~]# kubectl run ghost --image=ngnix 
pod/ghost created
]0;root@localhost:~[root@localhost ~]# kubectl run ghost --image=ngnix [C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[18Pns defaultctl run ghost --image=ngnix [K[K[Kget po
NAME    READY   STATUS         RESTARTS   AGE
ghost   0/1     ErrImagePull   0          11s
]0;root@localhost:~[root@localhost ~]# kubectl describel ghost
Error: unknown command "describel" for "kubectl"

Did you mean this?
	describe

Run 'kubectl --help' for usage.
unknown command "describel" for "kubectl"

Did you mean this?
	describe

]0;root@localhost:~[root@localhost ~]# kubectl describel ghost[1P ghost
error: the server doesn't have a resource type "ghost"
]0;root@localhost:~[root@localhost ~]# kubectl describe ghost[C ghostp ghosto ghostd ghost[C ghost
Name:         ghost
Namespace:    default
Priority:     0
Node:         localhost.localdomain/192.168.1.30
Start Time:   Sun, 10 Apr 2022 14:37:57 +0530
Labels:       run=ghost
Annotations:  <none>
Status:       Pending
IP:           10.42.0.82
IPs:
  IP:  10.42.0.82
Containers:
  ghost:
    Container ID:   
    Image:          ngnix
    Image ID:       
    Port:           <none>
    Host Port:      <none>
    State:          Waiting
      Reason:       ErrImagePull
    Ready:          False
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-t25cq (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             False 
  ContainersReady   False 
  PodScheduled      True 
Volumes:
  kube-api-access-t25cq:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason     Age                From               Message
  ----     ------     ----               ----               -------
  Normal   Scheduled  50s                default-scheduler  Successfully assigned default/ghost to localhost.localdomain
  Normal   Pulling    30s (x2 over 50s)  kubelet            Pulling image "ngnix"
  Warning  Failed     25s (x2 over 46s)  kubelet            Failed to pull image "ngnix": rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/ngnix:latest": failed to resolve reference "docker.io/library/ngnix:latest": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed
  Warning  Failed     25s (x2 over 46s)  kubelet            Error: ErrImagePull
  Normal   BackOff    14s (x2 over 45s)  kubelet            Back-off pulling image "ngnix"
  Warning  Failed     14s (x2 over 45s)  kubelet            Error: ImagePullBackOff
]0;root@localhost:~[root@localhost ~]# kubectl describe pod  ghost[5Pghostl ghost[9Pget porun ghost --image=ngnix [K:l[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[Kkubcet[K[K[K[Kbectl dek[Klete [1Pns default ctl get svcns default ctl delete[Cghost
error: the server doesn't have a resource type "ghost"
]0;root@localhost:~[root@localhost ~]# kubectl delete ghost[C ghostp ghosto ghostd ghost[C ghost
pod "ghost" deleted
]0;root@localhost:~[root@localhost ~]# kubectl delete pod  ghost[5Pghost[7@scribe pod [C[C[C[C[C[C[5Pghostl ghost[9Pget po[Kno
NAME                    STATUS   ROLES                  AGE   VERSION
localhost.localdomain   Ready    control-plane,master   18d   v1.22.7+k3s1
]0;root@localhost:~[root@localhost ~]# kubectl expalin 
Error: unknown command "expalin" for "kubectl"

Did you mean this?
	explain

Run 'kubectl --help' for usage.
unknown command "expalin" for "kubectl"

Did you mean this?
	explain



]0;root@localhost:/usr/local/bin[root@localhost bin]# kubectl expalin  pod[1Ppod[1P pod[1P pod[1P pod[1P pod[1P pod[1P pod[1P pode podx podp podl poda podi podn pod[C pod
KIND:     Pod
VERSION:  v1

DESCRIPTION:
     Pod is a collection of containers that can run on a host. This resource is
     created by clients and scheduled onto hosts.

FIELDS:
   apiVersion	<string>
     APIVersion defines the versioned schema of this representation of an
     object. Servers should convert recognized schemas to the latest internal
     value, and may reject unrecognized values. More info:
     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources

   kind	<string>
     Kind is a string value representing the REST resource this object
     represents. Servers may infer this from the endpoint the client submits
     requests to. Cannot be updated. In CamelCase. More info:
     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds

   metadata	<Object>
     Standard object's metadata. More info:
     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata

   spec	<Object>
     Specification of the desired behavior of the pod. More info:
     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#spec-and-status

   status	<Object>
     Most recently observed status of the pod. This data may not be up to date.
     Populated by the system. Read-only. More info:
     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#spec-and-status

]0;root@localhost:/usr/local/bin[root@localhost bin]# kubectl explain  pod.sec[K[K[Kspec
KIND:     Pod
VERSION:  v1

RESOURCE: spec <Object>

DESCRIPTION:
     Specification of the desired behavior of the pod. More info:
     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#spec-and-status

     PodSpec is a description of a pod.

FIELDS:
   activeDeadlineSeconds	<integer>
     Optional duration in seconds the pod may be active on the node relative to
     StartTime before the system will actively try to mark it failed and kill
     associated containers. Value must be a positive integer.

   affinity	<Object>
     If specified, the pod's scheduling constraints

   automountServiceAccountToken	<boolean>
     AutomountServiceAccountToken indicates whether a service account token
     should be automatically mounted.

   containers	<[]Object> -required-
     List of containers belonging to the pod. Containers cannot currently be
     added or removed. There must be at least one container in a Pod. Cannot be
     updated.

   dnsConfig	<Object>
     Specifies the DNS parameters of a pod. Parameters specified here will be
     merged to the generated DNS configuration based on DNSPolicy.

   dnsPolicy	<string>
     Set DNS policy for the pod. Defaults to "ClusterFirst". Valid values are
     'ClusterFirstWithHostNet', 'ClusterFirst', 'Default' or 'None'. DNS
     parameters given in DNSConfig will be merged with the policy selected with
     DNSPolicy. To have DNS options set along with hostNetwork, you have to
     specify DNS policy explicitly to 'ClusterFirstWithHostNet'.

   enableServiceLinks	<boolean>
     EnableServiceLinks indicates whether information about services should be
     injected into pod's environment variables, matching the syntax of Docker
     links. Optional: Defaults to true.

   ephemeralContainers	<[]Object>
     List of ephemeral containers run in this pod. Ephemeral containers may be
     run in an existing pod to perform user-initiated actions such as debugging.
     This list cannot be specified when creating a pod, and it cannot be
     modified by updating the pod spec. In order to add an ephemeral container
     to an existing pod, use the pod's ephemeralcontainers subresource. This
     field is alpha-level and is only honored by servers that enable the
     EphemeralContainers feature.

   hostAliases	<[]Object>
     HostAliases is an optional list of hosts and IPs that will be injected into
     the pod's hosts file if specified. This is only valid for non-hostNetwork
     pods.

   hostIPC	<boolean>
     Use the host's ipc namespace. Optional: Default to false.

   hostNetwork	<boolean>
     Host networking requested for this pod. Use the host's network namespace.
     If this option is set, the ports that will be used must be specified.
     Default to false.

   hostPID	<boolean>
     Use the host's pid namespace. Optional: Default to false.

   hostname	<string>
     Specifies the hostname of the Pod If not specified, the pod's hostname will
     be set to a system-defined value.

   imagePullSecrets	<[]Object>
     ImagePullSecrets is an optional list of references to secrets in the same
     namespace to use for pulling any of the images used by this PodSpec. If
     specified, these secrets will be passed to individual puller
     implementations for them to use. For example, in the case of docker, only
     DockerConfig type secrets are honored. More info:
     https://kubernetes.io/docs/concepts/containers/images#specifying-imagepullsecrets-on-a-pod

   initContainers	<[]Object>
     List of initialization containers belonging to the pod. Init containers are
     executed in order prior to containers being started. If any init container
     fails, the pod is considered to have failed and is handled according to its
     restartPolicy. The name for an init container or normal container must be
     unique among all containers. Init containers may not have Lifecycle
     actions, Readiness probes, Liveness probes, or Startup probes. The
     resourceRequirements of an init container are taken into account during
     scheduling by finding the highest request/limit for each resource type, and
     then using the max of of that value or the sum of the normal containers.
     Limits are applied to init containers in a similar fashion. Init containers
     cannot currently be added or removed. Cannot be updated. More info:
     https://kubernetes.io/docs/concepts/workloads/pods/init-containers/

   nodeName	<string>
     NodeName is a request to schedule this pod onto a specific node. If it is
     non-empty, the scheduler simply schedules this pod onto that node, assuming
     that it fits resource requirements.

   nodeSelector	<map[string]string>
     NodeSelector is a selector which must be true for the pod to fit on a node.
     Selector which must match a node's labels for the pod to be scheduled on
     that node. More info:
     https://kubernetes.io/docs/concepts/configuration/assign-pod-node/

   overhead	<map[string]string>
     Overhead represents the resource overhead associated with running a pod for
     a given RuntimeClass. This field will be autopopulated at admission time by
     the RuntimeClass admission controller. If the RuntimeClass admission
     controller is enabled, overhead must not be set in Pod create requests. The
     RuntimeClass admission controller will reject Pod create requests which
     have the overhead already set. If RuntimeClass is configured and selected
     in the PodSpec, Overhead will be set to the value defined in the
     corresponding RuntimeClass, otherwise it will remain unset and treated as
     zero. More info:
     https://git.k8s.io/enhancements/keps/sig-node/688-pod-overhead/README.md
     This field is beta-level as of Kubernetes v1.18, and is only honored by
     servers that enable the PodOverhead feature.

   preemptionPolicy	<string>
     PreemptionPolicy is the Policy for preempting pods with lower priority. One
     of Never, PreemptLowerPriority. Defaults to PreemptLowerPriority if unset.
     This field is beta-level, gated by the NonPreemptingPriority feature-gate.

   priority	<integer>
     The priority value. Various system components use this field to find the
     priority of the pod. When Priority Admission Controller is enabled, it
     prevents users from setting this field. The admission controller populates
     this field from PriorityClassName. The higher the value, the higher the
     priority.

   priorityClassName	<string>
     If specified, indicates the pod's priority. "system-node-critical" and
     "system-cluster-critical" are two special keywords which indicate the
     highest priorities with the former being the highest priority. Any other
     name must be defined by creating a PriorityClass object with that name. If
     not specified, the pod priority will be default or zero if there is no
     default.

   readinessGates	<[]Object>
     If specified, all readiness gates will be evaluated for pod readiness. A
     pod is ready when all its containers are ready AND all conditions specified
     in the readiness gates have status equal to "True" More info:
     https://git.k8s.io/enhancements/keps/sig-network/580-pod-readiness-gates

   restartPolicy	<string>
     Restart policy for all containers within the pod. One of Always, OnFailure,
     Never. Default to Always. More info:
     https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/#restart-policy

   runtimeClassName	<string>
     RuntimeClassName refers to a RuntimeClass object in the node.k8s.io group,
     which should be used to run this pod. If no RuntimeClass resource matches
     the named class, the pod will not be run. If unset or empty, the "legacy"
     RuntimeClass will be used, which is an implicit class with an empty
     definition that uses the default runtime handler. More info:
     https://git.k8s.io/enhancements/keps/sig-node/585-runtime-class This is a
     beta feature as of Kubernetes v1.14.

   schedulerName	<string>
     If specified, the pod will be dispatched by specified scheduler. If not
     specified, the pod will be dispatched by default scheduler.

   securityContext	<Object>
     SecurityContext holds pod-level security attributes and common container
     settings. Optional: Defaults to empty. See type description for default
     values of each field.

   serviceAccount	<string>
     DeprecatedServiceAccount is a depreciated alias for ServiceAccountName.
     Deprecated: Use serviceAccountName instead.

   serviceAccountName	<string>
     ServiceAccountName is the name of the ServiceAccount to use to run this
     pod. More info:
     https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/

   setHostnameAsFQDN	<boolean>
     If true the pod's hostname will be configured as the pod's FQDN, rather
     than the leaf name (the default). In Linux containers, this means setting
     the FQDN in the hostname field of the kernel (the nodename field of struct
     utsname). In Windows containers, this means setting the registry value of
     hostname for the registry key
     HKEY_LOCAL_MACHINE\SYSTEM\CurrentControlSet\Services\Tcpip\Parameters to
     FQDN. If a pod does not have FQDN, this has no effect. Default to false.

   shareProcessNamespace	<boolean>
     Share a single process namespace between all of the containers in a pod.
     When this is set containers will be able to view and signal processes from
     other containers in the same pod, and the first process in each container
     will not be assigned PID 1. HostPID and ShareProcessNamespace cannot both
     be set. Optional: Default to false.

   subdomain	<string>
     If specified, the fully qualified Pod hostname will be
     "<hostname>.<subdomain>.<pod namespace>.svc.<cluster domain>". If not
     specified, the pod will not have a domainname at all.

   terminationGracePeriodSeconds	<integer>
     Optional duration in seconds the pod needs to terminate gracefully. May be
     decreased in delete request. Value must be non-negative integer. The value
     zero indicates stop immediately via the kill signal (no opportunity to shut
     down). If this value is nil, the default grace period will be used instead.
     The grace period is the duration in seconds after the processes running in
     the pod are sent a termination signal and the time when the processes are
     forcibly halted with a kill signal. Set this value longer than the expected
     cleanup time for your process. Defaults to 30 seconds.

   tolerations	<[]Object>
     If specified, the pod's tolerations.

   topologySpreadConstraints	<[]Object>
     TopologySpreadConstraints describes how a group of pods ought to spread
     across topology domains. Scheduler will schedule pods in a way which abides
     by the constraints. All topologySpreadConstraints are ANDed.

   volumes	<[]Object>
     List of volumes that can be mounted by containers belonging to the pod.
     More info: https://kubernetes.io/docs/concepts/storage/volumes

]0;root@localhost:/usr/local/bin[root@localhost bin]# 
]0;root@localhost:/usr/local/bin[root@localhost bin]# kubectl get deplot[Ky
No resources found in default namespace.
]0;root@localhost:/usr/local/bin[root@localhost bin]# kubectl get deploy[1P deploy[1P deploy[1P deployc deployr deploye deploya deployt deploye deploy[C deploy[C[C[C[C[C[C[C deploy nginx [6P[1@-[1@-[1@i[1@m[1@a[1@g[1@e[C[1P[1@+[1@ [1P[1P[1@=[1@ [1P[C[C[C[C[C[C -n [K[K[K
error: exactly one NAME is required, got 0
See 'kubectl create deployment -h' for help and examples
]0;root@localhost:/usr/local/bin[root@localhost bin]# kubectl create  deploy --image=nginx  [C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[Kkubectl create deployment -h' i[K[K[K image[K[K[K[K[K--image=nginx
Create a deployment with the specified name.

Aliases:
deployment, deploy

Examples:
  # Create a deployment named my-dep that runs the busybox image
  kubectl create deployment my-dep --image=busybox
  
  # Create a deployment with a command
  kubectl create deployment my-dep --image=busybox -- date
  
  # Create a deployment named my-dep that runs the nginx image with 3 replicas
  kubectl create deployment my-dep --image=nginx --replicas=3
  
  # Create a deployment named my-dep that runs the busybox image and expose port 5701
  kubectl create deployment my-dep --image=busybox --port=5701

Options:
      --allow-missing-template-keys=true: If true, ignore any errors in templates when a field or map key is missing in
the template. Only applies to golang and jsonpath output formats.
      --dry-run='none': Must be "none", "server", or "client". If client strategy, only print the object that would be
sent, without sending it. If server strategy, submit server-side request without persisting the resource.
      --field-manager='kubectl-create': Name of the manager used to track field ownership.
      --image=[]: Image names to run.
  -o, --output='': Output format. One of:
json|yaml|name|go-template|go-template-file|template|templatefile|jsonpath|jsonpath-as-json|jsonpath-file.
      --port=-1: The port that this container exposes.
  -r, --replicas=1: Number of replicas to create. Default is 1.
      --save-config=false: If true, the configuration of current object will be saved in its annotation. Otherwise, the
annotation will be unchanged. This flag is useful when you want to perform kubectl apply on this object in the future.
      --show-managed-fields=false: If true, keep the managedFields when printing objects in JSON or YAML format.
      --template='': Template string or path to template file to use when -o=go-template, -o=go-template-file. The
template format is golang templates [http://golang.org/pkg/text/template/#pkg-overview].
      --validate=true: If true, use a schema to validate the input before sending it

Usage:
  kubectl create deployment NAME --image=image -- [COMMAND] [args...] [options]

Use "kubectl options" for a list of global command-line options (applies to all commands).
]0;root@localhost:/usr/local/bin[root@localhost bin]# kubectl create deployment -h --image=nginx= --image=nginx[1P --image=nginx[1P --image=nginx[1P --image=nginxd --image=nginxh --image=nginxa --image=nginxn --image=nginxa --image=nginx[C --image=nginx
deployment.apps/dhana created
]0;root@localhost:/usr/local/bin[root@localhost bin]# kubectl create deployment dhana  --image=nginx[4P-h[C[C[C[C[C[C[C[C[C[C[C[C[C[C[4P deploy --image=nginx  [20Pget deploy
NAME    READY   UP-TO-DATE   AVAILABLE   AGE
dhana   0/1     1            0           7s
]0;root@localhost:/usr/local/bin[root@localhost bin]# kubectl get deploy[Kpo
NAME                     READY   STATUS              RESTARTS   AGE
dhana-666f9cc654-bfdvx   0/1     ContainerCreating   0          13s
]0;root@localhost:/usr/local/bin[root@localhost bin]# d[Kkubectk[Kl get podeploy dhana-666f9cc654-bfdvx -o yaml > deploy-nginx.yml
Error from server (NotFound): deployments.apps "dhana-666f9cc654-bfdvx" not found
]0;root@localhost:/usr/local/bin[root@localhost bin]# kubectl get deploy dhana-666f9cc654-bfdvx -o yaml > deploy-nginx.yml[C[1P[1P[1P[1P[1P[1P[1P[1P[1P[1P[1P[1P[1P[1P[1P[1P[1P
]0;root@localhost:/usr/local/bin[root@localhost bin]# ll
total 174504
lrwxrwxrwx. 1 1001 1001       45 Mar 18 03:22 [0m[01;36mcorepack[0m -> [01;32m../lib/node_modules/corepack/dist/corepack.js[0m
lrwxrwxrwx. 1 root root        3 Mar 22 23:09 [01;36mcrictl[0m -> [01;32mk3s[0m
-rw-r--r--. 1 root root     1565 Apr 10 15:28 deploy-nginx.yml
-rwxr-xr-x. 1 root root 45072384 Mar 21 19:25 [01;32mhelm[0m
-rwxr-xr-x. 1 root root 53272576 Mar 22 23:07 [01;32mk3s[0m
-rwxr-xr-x. 1 root root     1943 Mar 22 23:09 [01;32mk3s-killall.sh[0m
-rwxr-xr-x. 1 root root     1397 Mar 22 23:09 [01;32mk3s-uninstall.sh[0m
lrwxrwxrwx. 1 root root        3 Mar 22 23:09 [01;36mkubectl[0m -> [01;32mk3s[0m
-rwxr-xr-x. 1 root root     6108 Apr 10 14:00 [01;32mkubectx[0m
-rwxr-xr-x. 1 root root     5555 Apr 10 14:00 [01;32mkubens[0m
-rwxr-xr-x. 1 1001 1001 80316256 Mar 18 03:22 [01;32mnode[0m
]0;root@localhost:/usr/local/bin[root@localhost bin]# mc [K[Kv deploy-nginx.yml /r[K
bin/        dev/        home/       lib64/      media/      opt/        root/       sbin/       sys/        usr/        
boot/       etc/        lib/        lost+found/ mnt/        proc/       run/        srv/        tmp/        var/        
[root@localhost bin]# mv deploy-nginx.yml /root/
]0;root@localhost:/usr/local/bin[root@localhost bin]# l[Kcd 
]0;root@localhost:~[root@localhost ~]# ll
total 164
-rw-------.  1 root root  2227 Mar 21 19:14 anaconda-ks.cfg
-rw-r--r--.  1 root root  1565 Apr 10 15:28 deploy-nginx.yml
-rwx------.  1 root root 11156 Mar 23 22:44 [0m[01;32mget_helm.sh[0m
-rw-r--r--.  1 root root  2320 Mar 21 19:43 initial-setup-ks.cfg
drwxr-xr-x. 10 root root  4096 Apr 10 13:59 [01;34mkubectx[0m
-rw-r--r--.  1 root root 93490 Mar 22 23:05 kube-prometheus-stack.yml
drwxr-xr-x. 31 root root  4096 Mar 21 21:21 [01;34mkubernetes[0m
drwxr-xr-x.  9 root root  4096 Mar 21 14:30 [01;34mlocal-repo[0m
-rw-r--r--.  1 root root  7565 Apr  8 21:00 Metricbeat-kube-not.yml
-rw-r--r--.  1 root root   104 Mar 20 14:31 mongo-configmap.yaml
-rw-r--r--.  1 root root  1114 Mar 20 14:32 mongo-express.yaml
-rw-r--r--.  1 root root   158 Mar 20 14:32 mongo-secret.yaml
-rw-r--r--.  1 root root   862 Mar 20 14:31 mongo.yaml
drwxr-xr-x.  4 root root  4096 Mar 21 21:26 [01;34mprome-grafana[0m
drwxr-xr-x.  4 root root  4096 Mar 21 22:01 [01;34mPrometheus-Grafana[0m
-rw-r--r--.  1 root root  4077 Apr  8 22:23 values-metricbeat.yaml
]0;root@localhost:~[root@localhost ~]# where is deploy-nginx.yml
bash: where: command not found...
]0;root@localhost:~[root@localhost ~]# where is deploy-nginx.yml[1P
deploy-nginx:]0;root@localhost:~[root@localhost ~]# 
]0;root@localhost:~[root@localhost ~]# vim deploy-nginxdeploy-nginx
[?1049h[?1h=[2;1H▽[6n[2;1H  [1;1H]11;?[1;39r[?12;25h[?12l[?25h[27m[m[H[2J[?25l[39;1H"deploy-nginxdeploy-nginx" [New File][>c[2;1H[1m[34m~                                                                                                                                     [3;1H~                                                                                                                                     [4;1H~                                                                                                                                     [5;1H~                                                                                                                                     [6;1H~                                                                                                                                     [7;1H~                                                                                                                                     [8;1H~                                                                                                                                     [9;1H~                                                                                                                                     [10;1H~                                                                                                                                     [11;1H~                                                                                                                                     [12;1H~                                                                                                                                     [13;1H~                                                                                                                                     [14;1H~                                                                                                                                     [15;1H~                                                                                                                                     [16;1H~                                                                                                                                     [17;1H~                                                                                                                                     [18;1H~                                                                                                                                     [19;1H~                                                                                                                                     [20;1H~                                                                                                                                     [21;1H~                                                                                                                                     [22;1H~                                                                                                                                     [23;1H~                                                                                                                                     [24;1H~                                                                                                                                     [25;1H~                                                                                                                                     [26;1H~                                                                                                                                     [27;1H~                                                                                                                                     [28;1H~                                                                                                                                     [29;1H~                                                                                                                                     [30;1H~                                                                                                                                     [31;1H~                                                                                                                                     [32;1H~                                                                                                                                     [33;1H~                                                                                                                                     [34;1H~                                                                                                                                     [35;1H~                                                                                                                                     [36;1H~                                                                                                                                     [37;1H~                                                                                                                                     [38;1H~                                                                                                                                     [m[39;117H0,0-1[9CAll[1;1H[?12l[?25h[?25l[39;1H[K[39;1H:[?12l[?25hq[?25l[?12l[?25h![?25l[?12l[?25h[?25l[39;1H[K[39;1H[?1l>[?12l[?25h[?1049l]0;root@localhost:~[root@localhost ~]# cd /root
]0;root@localhost:~[root@localhost ~]# ll
total 164
-rw-------.  1 root root  2227 Mar 21 19:14 anaconda-ks.cfg
-rw-r--r--.  1 root root  1565 Apr 10 15:28 deploy-nginx.yml
-rwx------.  1 root root 11156 Mar 23 22:44 [0m[01;32mget_helm.sh[0m
-rw-r--r--.  1 root root  2320 Mar 21 19:43 initial-setup-ks.cfg
drwxr-xr-x. 10 root root  4096 Apr 10 13:59 [01;34mkubectx[0m
-rw-r--r--.  1 root root 93490 Mar 22 23:05 kube-prometheus-stack.yml
drwxr-xr-x. 31 root root  4096 Mar 21 21:21 [01;34mkubernetes[0m
drwxr-xr-x.  9 root root  4096 Mar 21 14:30 [01;34mlocal-repo[0m
-rw-r--r--.  1 root root  7565 Apr  8 21:00 Metricbeat-kube-not.yml
-rw-r--r--.  1 root root   104 Mar 20 14:31 mongo-configmap.yaml
-rw-r--r--.  1 root root  1114 Mar 20 14:32 mongo-express.yaml
-rw-r--r--.  1 root root   158 Mar 20 14:32 mongo-secret.yaml
-rw-r--r--.  1 root root   862 Mar 20 14:31 mongo.yaml
drwxr-xr-x.  4 root root  4096 Mar 21 21:26 [01;34mprome-grafana[0m
drwxr-xr-x.  4 root root  4096 Mar 21 22:01 [01;34mPrometheus-Grafana[0m
-rw-r--r--.  1 root root  4077 Apr  8 22:23 values-metricbeat.yaml
]0;root@localhost:~[root@localhost ~]# cat deploy-nginx.yml
apiVersion: apps/v1
kind: Deployment
metadata:
  annotations:
    deployment.kubernetes.io/revision: "1"
  creationTimestamp: "2022-04-10T09:57:03Z"
  generation: 1
  labels:
    app: dhana
  name: dhana
  namespace: default
  resourceVersion: "20697"
  uid: a8f03198-3545-471b-8a4d-3027b165d82d
spec:
  progressDeadlineSeconds: 600
  replicas: 1
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      app: dhana
  strategy:
    rollingUpdate:
      maxSurge: 25%
      maxUnavailable: 25%
    type: RollingUpdate
  template:
    metadata:
      creationTimestamp: null
      labels:
        app: dhana
    spec:
      containers:
      - image: nginx
        imagePullPolicy: Always
        name: nginx
        resources: {}
        terminationMessagePath: /dev/termination-log
        terminationMessagePolicy: File
      dnsPolicy: ClusterFirst
      restartPolicy: Always
      schedulerName: default-scheduler
      securityContext: {}
      terminationGracePeriodSeconds: 30
status:
  availableReplicas: 1
  conditions:
  - lastTransitionTime: "2022-04-10T09:57:51Z"
    lastUpdateTime: "2022-04-10T09:57:51Z"
    message: Deployment has minimum availability.
    reason: MinimumReplicasAvailable
    status: "True"
    type: Available
  - lastTransitionTime: "2022-04-10T09:57:03Z"
    lastUpdateTime: "2022-04-10T09:57:51Z"
    message: ReplicaSet "dhana-666f9cc654" has successfully progressed.
    reason: NewReplicaSetAvailable
    status: "True"
    type: Progressing
  observedGeneration: 1
  readyReplicas: 1
  replicas: 1
  updatedReplicas: 1
]0;root@localhost:~[root@localhost ~]# cat deploy-nginx.yml[K^C
]0;root@localhost:~[root@localhost ~]# 
]0;root@localhost:~[root@localhost ~]# vim deploy-nginx.yml
[?1049h[?1h=[2;1H▽[6n[2;1H  [1;1H]11;?[1;39r[?12;25h[?12l[?25h[27m[m[H[2J[?25l[39;1H"deploy-nginx.yml" 62L, 1565C[>c[1;1H[36mapiVersion[m[35m:[m apps/v1
[36mkind[m[35m:[m Deployment
[36mmetadata[m[35m:[m
  [36mannotations[m[35m:[m
    [36mdeployment.kubernetes.io/revision[m[35m:[m [31m"1"[m
  [36mcreationTimestamp[m[35m:[m [31m"2022-04-10T09:57:03Z"[m
  [36mgeneration[m[35m:[m [31m1[m
  [36mlabels[m[35m:[m
    [36mapp[m[35m:[m dhana
  [36mname[m[35m:[m dhana
  [36mnamespace[m[35m:[m default
  [36mresourceVersion[m[35m:[m [31m"20697"[m
  [36muid[m[35m:[m a8f03198-3545-471b-8a4d-3027b165d82d
[36mspec[m[35m:[m
  [36mprogressDeadlineSeconds[m[35m:[m [31m600[m
  [36mreplicas[m[35m:[m [31m1[m
  [36mrevisionHistoryLimit[m[35m:[m [31m10[m
  [36mselector[m[35m:[m
    [36mmatchLabels[m[35m:[m[20;7H[36mapp[m[35m:[m dhana
  [36mstrategy[m[35m:[m
    [36mrollingUpdate[m[35m:[m[23;7H[36mmaxSurge[m[35m:[m 25%[24;7H[36mmaxUnavailable[m[35m:[m 25%
    [36mtype[m[35m:[m RollingUpdate
  [36mtemplate[m[35m:[m
    [36mmetadata[m[35m:[m[28;7H[36mcreationTimestamp[m[35m:[m [31mnull[m[29;7H[36mlabels[m[35m:[m[30;9H[36mapp[m[35m:[m dhana
    [36mspec[m[35m:[m[32;7H[36mcontainers[m[35m:[m[33;7H[33m- [m[36mimage[m[35m:[m nginx[34;9H[36mimagePullPolicy[m[35m:[m Always[35;9H[36mname[m[35m:[m nginx[36;9H[36mresources[m[35m:[m [35m{}[m[37;9H[36mterminationMessagePath[m[35m:[m /dev/termination-log[38;9H[36mterminationMessagePolicy[m[35m:[m File[39;117H1,1[11CTop[1;1H[?12l[?25h[?25l[39;117H2[2;1H[?12l[?25h[?25l[39;117H3[3;1H[?12l[?25h[?25l[39;117H4[4;1H[?12l[?25h[?25l[39;117H5[5;1H[?12l[?25h[?25l[39;117H6[6;1H[?12l[?25h[?25l[39;117H7[7;1H[?12l[?25h[?25l[39;117H8[8;1H[?12l[?25h[?25l[39;117H9[9;1H[?12l[?25h[?25l[39;117H10,1[10;1H[?12l[?25h[?25l[39;118H1[11;1H[?12l[?25h[?25l[39;118H2[12;1H[?12l[?25h[?25l[39;118H3[13;1H[?12l[?25h[?25l[39;118H4[14;1H[?12l[?25h[?25l[39;118H5[15;1H[?12l[?25h[?25l[39;118H6[16;1H[?12l[?25h[?25l[39;118H7[17;1H[?12l[?25h[?25l[39;118H8[18;1H[?12l[?25h[?25l[39;118H9[19;1H[?12l[?25h[?25l[39;117H20[20;1H[?12l[?25h[?25l[39;118H1[21;1H[?12l[?25h[?25l[39;118H2[22;1H[?12l[?25h[?25l[39;118H3[23;1H[?12l[?25h[?25l[39;118H4[24;1H[?12l[?25h[?25l[39;118H5[25;1H[?12l[?25h[?25l[39;118H6[26;1H[?12l[?25h[?25l[39;118H7[27;1H[?12l[?25h[?25l[39;118H8[28;1H[?12l[?25h[?25l[39;118H9[29;1H[?12l[?25h[?25l[39;117H30[30;1H[?12l[?25h[?25l[39;118H1[31;1H[?12l[?25h[?25l[39;118H2[32;1H[?12l[?25h[?25l[39;118H3[33;1H[?12l[?25h[?25l[39;118H4[34;1H[?12l[?25h[?25l[39;118H5[35;1H[?12l[?25h[?25l[39;118H4[34;1H[?12l[?25h[?25l[39;118H3[33;1H[?12l[?25h[?25l[39;118H2[32;1H[?12l[?25h[?25l[39;118H1[31;1H[?12l[?25h[?25l[39;118H0[30;1H[?12l[?25h[?25l[39;118H1[31;1H[?12l[?25h[?25l[39;118H2[32;1H[?12l[?25h[?25l[39;118H3[33;1H[?12l[?25h[?25l[39;118H4[34;1H[?12l[?25h[?25l[39;118H5[35;1H[?12l[?25h[?25l[39;118H6[36;1H[?12l[?25h[?25l[39;118H7[37;1H[?12l[?25h[?25l[39;118H8[38;1H[?12l[?25h[?25l[1;38r[38;1H
[1;39r[38;7H[36mdnsPolicy[m[35m:[m ClusterFirst[39;1H[K[39;117H39,1[11C4%[38;1H[?12l[?25h[?25l[1;38r[38;1H
[1;39r[38;7H[36mrestartPolicy[m[35m:[m Always[39;117H[K[39;117H40,1[11C8%[38;1H[?12l[?25h[?25l[1;38r[38;1H
[1;39r[38;7H[36mschedulerName[m[35m:[m default-scheduler[39;117H[K[39;117H41,1[10C12%[38;1H[?12l[?25h[?25l[1;38r[38;1H
[1;39r[38;7H[36msecurityContext[m[35m:[m [35m{}[m[39;117H[K[39;117H42,1[10C16%[38;1H[?12l[?25h[?25l[1;38r[38;1H
[1;39r[38;7H[36mterminationGracePeriodSeconds[m[35m:[m [31m30[m[39;117H[K[39;117H43,1[10C20%[38;1H[?12l[?25h[?25l[1;38r[38;1H
[1;39r[38;1H[36mstatus[m[35m:[m[39;117H[K[39;117H44,1[10C25%[38;1H[?12l[?25h[?25l[1;38r[38;1H
[1;39r[38;3H[36mavailableReplicas[m[35m:[m [31m1[m[39;117H[K[39;117H45,1[10C29%[38;1H[?12l[?25h[?25l[1;38r[38;1H
[1;39r[38;3H[36mconditions[m[35m:[m[39;117H[K[39;117H46,1[10C33%[38;1H[?12l[?25h[?25l[1;38r[38;1H
[1;39r[38;3H[33m- [m[36mlastTransitionTime[m[35m:[m [31m"2022-04-10T09:57:51Z"[m[39;117H[K[39;117H47,1[10C37%[38;1H[?12l[?25h[?25l[1;38r[38;1H
[1;39r[38;5H[36mlastUpdateTime[m[35m:[m [31m"2022-04-10T09:57:51Z"[m[39;117H[K[39;117H48,1[10C41%[38;1H[?12l[?25h[?25l[1;38r[38;1H
[1;39r[38;5H[36mmessage[m[35m:[m Deployment has minimum availability.[39;117H[K[39;117H49,1[10C45%[38;1H[?12l[?25h[?25l[1;38r[38;1H
[1;39r[38;5H[36mreason[m[35m:[m MinimumReplicasAvailable[39;117H[K[39;117H50,1[10C50%[38;1H[?12l[?25h[?25l[1;38r[38;1H
[1;39r[38;5H[36mstatus[m[35m:[m [31m"True"[m[39;117H[K[39;117H51,1[10C54%[38;1H[?12l[?25h[?25l[1;38r[38;1H
[1;39r[38;5H[36mtype[m[35m:[m Available[39;117H[K[39;117H52,1[10C58%[38;1H[?12l[?25h[?25l[1;38r[38;1H
[1;39r[38;3H[33m- [m[36mlastTransitionTime[m[35m:[m [31m"2022-04-10T09:57:03Z"[m[39;117H[K[39;117H53,1[10C62%[38;1H[?12l[?25h[?25l[1;38r[38;1H
[1;39r[38;5H[36mlastUpdateTime[m[35m:[m [31m"2022-04-10T09:57:51Z"[m[39;117H[K[39;117H54,1[10C66%[38;1H[?12l[?25h[?25l[1;38r[38;1H
[1;39r[38;5H[36mmessage[m[35m:[m ReplicaSet [31m"dhana-666f9cc654"[m has successfully progressed.[39;117H[K[39;117H55,1[10C70%[38;1H[?12l[?25h[?25l[1;38r[38;1H
[1;39r[38;5H[36mreason[m[35m:[m NewReplicaSetAvailable[39;117H[K[39;117H56,1[10C75%[38;1H[?12l[?25h[?25l[1;38r[38;1H
[1;39r[38;5H[36mstatus[m[35m:[m [31m"True"[m[39;117H[K[39;117H57,1[10C79%[38;1H[?12l[?25h[?25l[1;38r[38;1H
[1;39r[38;5H[36mtype[m[35m:[m Progressing[39;117H[K[39;117H58,1[10C83%[38;1H[?12l[?25h[?25l[1;38r[38;1H
[1;39r[38;3H[36mobservedGeneration[m[35m:[m [31m1[m[39;117H[K[39;117H59,1[10C87%[38;1H[?12l[?25h[?25l[1;38r[38;1H
[1;39r[38;3H[36mreadyReplicas[m[35m:[m [31m1[m[39;117H[K[39;117H60,1[10C91%[38;1H[?12l[?25h[?25l[1;38r[38;1H
[1;39r[38;3H[36mreplicas[m[35m:[m [31m1[m[39;117H[K[39;117H61,1[10C95%[38;1H[?12l[?25h[?25l[1;38r[38;1H
[1;39r[38;3H[36mupdatedReplicas[m[35m:[m [31m1[m[39;117H[K[39;117H62,1[10CBot[38;1H[?12l[?25h[?25l[39;118H1[37;1H[?12l[?25h[?25l[39;118H0[36;1H[?12l[?25h[?25l[39;117H59[35;1H[?12l[?25h[?25l[39;118H8[34;1H[?12l[?25h[?25l[39;118H9[35;1H[?12l[?25h[?25l[39;117H[K[39;1H:[?12l[?25hq[?25l[?12l[?25h![?25l[?12l[?25h[?25l[39;1H[K[39;1H[?1l>[?12l[?25h[?1049l]0;root@localhost:~[root@localhost ~]# history
   30    configReloaderCpu: 100m
   31    ## Set the prometheus config reloader side-car memory limit
   32    ##
   33    configReloaderMemory: 25Mi
   34    ## Hyperkube image to use when cleaning up
   35    ##
   36    hyperkubeImage:
   37      repository: k8s.gcr.io/hyperkube
   38      tag: v1.16.12
   39      sha: ""
   40      pullPolicy: IfNotPresent
   41  ## Deploy a Prometheus instance
   42  ##
   43  prometheus:
   44    enabled: true
   45    ## Annotations for Prometheus
   46    ##
   47    annotations: {}
   48    ## Service account for Prometheuses to use.
   49    ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/
   50    ##
   51    serviceAccount:
   52      create: true
   53      name: ""
   54    ## Configuration for Prometheus service
   55    ##
   56    service:
   57      annotations: {}
   58      labels: {}
   59      clusterIP: ""
   60      ## Port for Prometheus Service to listen on
   61      ##
   62      port: 9090
   63      ## To be used with a proxy extraContainer port
   64      targetPort: 9090
   65      ## List of IP addresses at which the Prometheus server service is available
   66      ## Ref: https://kubernetes.io/docs/user-guide/services/#external-ips
   67      ##
   68      externalIPs: []
   69      ## Port to expose on each node
   70      ## Only used if service.type is 'NodePort'
   71      ##
   72      nodePort: 30090
   73      ## Loadbalancer IP
   74      ## Only use if service.type is "loadbalancer"
   75      loadBalancerIP: ""
   76      loadBalancerSourceRanges: []
   77      ## Service type
   78      ##
   79      type: ClusterIP
   80      sessionAffinity: ""
   81    ## Configuration for creating a separate Service for each statefulset Prometheus replica
   82    ##
   83    servicePerReplica:
   84      enabled: false
   85      annotations: {}
   86      ## Port for Prometheus Service per replica to listen on
   87      ##
   88      port: 9090
   89      ## To be used with a proxy extraContainer port
   90      targetPort: 9090
   91      ## Port to expose on each node
   92      ## Only used if servicePerReplica.type is 'NodePort'
   93      ##
   94      nodePort: 30091
   95      ## Loadbalancer source IP ranges
   96      ## Only used if servicePerReplica.type is "loadbalancer"
   97      loadBalancerSourceRanges: []
   98      ## Service type
   99      ##
  100      type: ClusterIP
  101    ## Configure pod disruption budgets for Prometheus
  102    ## ref: https://kubernetes.io/docs/tasks/run-application/configure-pdb/#specifying-a-poddisruptionbudget
  103    ## This configuration is immutable once created and will require the PDB to be deleted to be changed
  104    ## https://github.com/kubernetes/kubernetes/issues/45398
  105    ##
  106    podDisruptionBudget:
  107      enabled: false
  108      minAvailable: 1
  109      maxUnavailable: ""
  110  # Ingress exposes thanos sidecar outside the clsuter
  111    thanosIngress:
  112      enabled: false
  113      annotations: {}
  114      labels: {}
  115      servicePort: 10901
  116      ## Hosts must be provided if Ingress is enabled.
  117      ##
  118      hosts: []
  119        # - thanos-gateway.domain.com
  120      ## Paths to use for ingress rules
  121      ##
  122      paths: []
  123      # - /
  124      ## TLS configuration for Alertmanager Ingress
  125      ## Secret must be manually created in the namespace
  126      ##
  127      tls: []
  128      # - secretName: thanos-gateway-tls
  129      #   hosts:
  130      #   - thanos-gateway.domain.com
  131    ingress:
  132      enabled: false
  133      annotations: {}
  134      labels: {}
  135      ## Hostnames.
  136      ## Must be provided if Ingress is enabled.
  137      ##
  138      # hosts:
  139      #   - prometheus.domain.com
  140      hosts: []
  141      ## Paths to use for ingress rules - one path should match the prometheusSpec.routePrefix
  142      ##
  143      paths: []
  144      # - /
  145      ## TLS configuration for Prometheus Ingress
  146      ## Secret must be manually created in the namespace
  147      ##
  148      tls: []
  149        # - secretName: prometheus-general-tls
  150        #   hosts:
  151        #     - prometheus.example.com
  152    ## Configuration for creating an Ingress that will map to each Prometheus replica service
  153    ## prometheus.servicePerReplica must be enabled
  154    ##
  155    ingressPerReplica:
  156      enabled: false
  157      annotations: {}
  158      labels: {}
  159      ## Final form of the hostname for each per replica ingress is
  160      ## {{ ingressPerReplica.hostPrefix }}-{{ $replicaNumber }}.{{ ingressPerReplica.hostDomain }}
  161      ##
  162      ## Prefix for the per replica ingress that will have `-$replicaNumber`
  163      ## appended to the end
  164      hostPrefix: ""
  165      ## Domain that will be used for the per replica ingress
  166      hostDomain: ""
  167      ## Paths to use for ingress rules
  168      ##
  169      paths: []
  170      # - /
  171      ## Secret name containing the TLS certificate for Prometheus per replica ingress
  172      ## Secret must be manually created in the namespace
  173      tlsSecretName: ""
  174      ## Separated secret for each per replica Ingress. Can be used together with cert-manager
  175      ##
  176      tlsSecretPerReplica:
  177        enabled: false
  178        ## Final form of the secret for each per replica ingress is
  179        ## {{ tlsSecretPerReplica.prefix }}-{{ $replicaNumber }}
  180        ##
  181        prefix: "prometheus"
  182    ## Configure additional options for default pod security policy for Prometheus
  183    ## ref: https://kubernetes.io/docs/concepts/policy/pod-security-policy/
  184    podSecurityPolicy:
  185      allowedCapabilities: []
  186    serviceMonitor:
  187      ## Scrape interval. If not set, the Prometheus default scrape interval is used.
  188      ##
  189      interval: ""
  190      selfMonitor: true
  191      ## scheme: HTTP scheme to use for scraping. Can be used with `tlsConfig` for example if using istio mTLS.
  192      scheme: ""
  193      ## tlsConfig: TLS configuration to use when scraping the endpoint. For example if using istio mTLS.
  194      ## Of type: https://github.com/coreos/prometheus-operator/blob/master/Documentation/api.md#tlsconfig
  195      tlsConfig: {}
  196      bearerTokenFile:
  197      ## metric relabel configs to apply to samples before ingestion.
  198      ##
  199      metricRelabelings: []
  200      # - action: keep
  201      #   regex: 'kube_(daemonset|deployment|pod|namespace|node|statefulset).+'
  202      #   sourceLabels: [__name__]
  203      # relabel configs to apply to samples before ingestion.
  204      ##
  205      relabelings: []
  206      # - sourceLabels: [__meta_kubernetes_pod_node_name]
  207      #   separator: ;
  208      #   regex: ^(.*)$
  209      #   targetLabel: nodename
  210      #   replacement: $1
  211      #   action: replace
  212    ## Settings affecting prometheusSpec
  213    ## ref: https://github.com/coreos/prometheus-operator/blob/master/Documentation/api.md#prometheusspec
  214    ##
  215    prometheusSpec:
  216      ## If true, pass --storage.tsdb.max-block-duration=2h to prometheus. This is already done if using Thanos
  217      ##
  218      disableCompaction: false
  219      ## APIServerConfig
  220      ## ref: https://github.com/coreos/prometheus-operator/blob/master/Documentation/api.md#apiserverconfig
  221      ##
  222      apiserverConfig: {}
  223      ## Interval between consecutive scrapes.
  224      ##
  225      scrapeInterval: ""
  226      ## Interval between consecutive evaluations.
  227      ##
  228      evaluationInterval: ""
  229      ## ListenLocal makes the Prometheus server listen on loopback, so that it does not bind against the Pod IP.
  230      ##
  231      listenLocal: false
  232      ## EnableAdminAPI enables Prometheus the administrative HTTP API which includes functionality such as deleting time series.
  233      ## This is disabled by default.
  234      ## ref: https://prometheus.io/docs/prometheus/latest/querying/api/#tsdb-admin-apis
  235      ##
  236      enableAdminAPI: false
  237      ## Image of Prometheus.
  238      ##
  239      image:
  240        repository: quay.io/prometheus/prometheus
  241        tag: v2.18.2
  242        sha: ""
  243      ## Tolerations for use with node taints
  244      ## ref: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/
  245      ##
  246      tolerations: []
  247      #  - key: "key"
  248      #    operator: "Equal"
  249      #    value: "value"
  250      #    effect: "NoSchedule"
  251      ## Alertmanagers to which alerts will be sent
  252      ## ref: https://github.com/coreos/prometheus-operator/blob/master/Documentation/api.md#alertmanagerendpoints
  253      ##
  254      ## Default configuration will connect to the alertmanager deployed as part of this release
  255      ##
  256      alertingEndpoints: []
  257      # - name: ""
  258      #   namespace: ""
  259      #   port: http
  260      #   scheme: http
  261      #   pathPrefix: ""
  262      #   tlsConfig: {}
  263      #   bearerTokenFile: ""
  264      #   apiVersion: v2
  265      ## External labels to add to any time series or alerts when communicating with external systems
  266      ##
  267      externalLabels: {}
  268      ## Name of the external label used to denote replica name
  269      ##
  270      replicaExternalLabelName: ""
  271      ## If true, the Operator won't add the external label used to denote replica name
  272      ##
  273      replicaExternalLabelNameClear: false
  274      ## Name of the external label used to denote Prometheus instance name
  275      ##
  276      prometheusExternalLabelName: ""
  277      ## If true, the Operator won't add the external label used to denote Prometheus instance name
  278      ##
  279      prometheusExternalLabelNameClear: false
  280      ## External URL at which Prometheus will be reachable.
  281      ##
  282      externalUrl: ""
  283      ## Define which Nodes the Pods are scheduled on.
  284      ## ref: https://kubernetes.io/docs/user-guide/node-selection/
  285      ##
  286      nodeSelector: {}
  287      ## Secrets is a list of Secrets in the same namespace as the Prometheus object, which shall be mounted into the Prometheus Pods.
  288      ## The Secrets are mounted into /etc/prometheus/secrets/. Secrets changes after initial creation of a Prometheus object are not
  289      ## reflected in the running Pods. To change the secrets mounted into the Prometheus Pods, the object must be deleted and recreated
  290      ## with the new list of secrets.
  291      ##
  292      secrets: []
  293      ## ConfigMaps is a list of ConfigMaps in the same namespace as the Prometheus object, which shall be mounted into the Prometheus Pods.
  294      ## The ConfigMaps are mounted into /etc/prometheus/configmaps/.
  295      ##
  296      configMaps: []
  297      ## QuerySpec defines the query command line flags when starting Prometheus.
  298      ## ref: https://github.com/coreos/prometheus-operator/blob/master/Documentation/api.md#queryspec
  299      ##
  300      query: {}
  301      ## Namespaces to be selected for PrometheusRules discovery.
  302      ## If nil, select own namespace. Namespaces to be selected for ServiceMonitor discovery.
  303      ## See https://github.com/coreos/prometheus-operator/blob/master/Documentation/api.md#namespaceselector for usage
  304      ##
  305      ruleNamespaceSelector: {}
  306      ## If true, a nil or {} value for prometheus.prometheusSpec.ruleSelector will cause the
  307      ## prometheus resource to be created with selectors based on values in the helm deployment,
  308      ## which will also match the PrometheusRule resources created
  309      ##
  310      ruleSelectorNilUsesHelmValues: true
  311      ## PrometheusRules to be selected for target discovery.
  312      ## If {}, select all ServiceMonitors
  313      ##
  314      ruleSelector: {}
  315      ## Example which select all prometheusrules resources
  316      ## with label "prometheus" with values any of "example-rules" or "example-rules-2"
  317      # ruleSelector:
  318      #   matchExpressions:
  319      #     - key: prometheus
  320      #       operator: In
  321      #       values:
  322      #         - example-rules
  323      #         - example-rules-2
  324      #
  325      ## Example which select all prometheusrules resources with label "role" set to "example-rules"
  326      # ruleSelector:
  327      #   matchLabels:
  328      #     role: example-rules
  329      ## If true, a nil or {} value for prometheus.prometheusSpec.serviceMonitorSelector will cause the
  330      ## prometheus resource to be created with selectors based on values in the helm deployment,
  331      ## which will also match the servicemonitors created
  332      ##
  333      serviceMonitorSelectorNilUsesHelmValues: true
  334      ## ServiceMonitors to be selected for target discovery.
  335      ## If {}, select all ServiceMonitors
  336      ##
  337      serviceMonitorSelector: {}
  338      ## Example which selects ServiceMonitors with label "prometheus" set to "somelabel"
  339      # serviceMonitorSelector:
  340      #   matchLabels:
  341      #     prometheus: somelabel
  342      ## Namespaces to be selected for ServiceMonitor discovery.
  343      ## See https://github.com/coreos/prometheus-operator/blob/master/Documentation/api.md#namespaceselector for usage
  344      ##
  345      serviceMonitorNamespaceSelector: {}
  346      ## If true, a nil or {} value for prometheus.prometheusSpec.podMonitorSelector will cause the
  347      ## prometheus resource to be created with selectors based on values in the helm deployment,
  348      ## which will also match the podmonitors created
  349      ##
  350      podMonitorSelectorNilUsesHelmValues: true
  351      ## PodMonitors to be selected for target discovery.
  352      ## If {}, select all PodMonitors
  353      ##
  354      podMonitorSelector: {}
  355      ## Example which selects PodMonitors with label "prometheus" set to "somelabel"
  356      # podMonitorSelector:
  357      #   matchLabels:
  358      #     prometheus: somelabel
  359      ## Namespaces to be selected for PodMonitor discovery.
  360      ## See https://github.com/coreos/prometheus-operator/blob/master/Documentation/api.md#namespaceselector for usage
  361      ##
  362      podMonitorNamespaceSelector: {}
  363      ## How long to retain metrics
  364      ##
  365      retention: 10d
  366      ## Maximum size of metrics
  367      ##
  368      retentionSize: ""
  369      ## Enable compression of the write-ahead log using Snappy.
  370      ##
  371      walCompression: false
  372      ## If true, the Operator won't process any Prometheus configuration changes
  373      ##
  374      paused: false
  375      ## Number of Prometheus replicas desired
  376      ##
  377      replicas: 1
  378      ## Log level for Prometheus be configured in
  379      ##
  380      logLevel: info
  381      ## Log format for Prometheus be configured in
  382      ##
  383      logFormat: logfmt
  384      ## Prefix used to register routes, overriding externalUrl route.
  385      ## Useful for proxies that rewrite URLs.
  386      ##
  387      routePrefix: /
  388      ## Standard object’s metadata. More info: https://github.com/kubernetes/community/blob/master/contributors/devel/sig-architecture/api-conventions.md#metadata
  389      ## Metadata Labels and Annotations gets propagated to the prometheus pods.
  390      ##
  391      podMetadata: {}
  392      # labels:
  393      #   app: prometheus
  394      #   k8s-app: prometheus
  395      ## Pod anti-affinity can prevent the scheduler from placing Prometheus replicas on the same node.
  396      ## The default value "soft" means that the scheduler should *prefer* to not schedule two replica pods onto the same node but no guarantee is provided.
  397      ## The value "hard" means that the scheduler is *required* to not schedule two replica pods onto the same node.
  398      ## The value "" will disable pod anti-affinity so that no anti-affinity rules will be configured.
  399      podAntiAffinity: ""
  400      ## If anti-affinity is enabled sets the topologyKey to use for anti-affinity.
  401      ## This can be changed to, for example, failure-domain.beta.kubernetes.io/zone
  402      ##
  403      podAntiAffinityTopologyKey: kubernetes.io/hostname
  404      ## Assign custom affinity rules to the prometheus instance
  405      ## ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/
  406      ##
  407      affinity: {}
  408      # nodeAffinity:
  409      #   requiredDuringSchedulingIgnoredDuringExecution:
  410      #     nodeSelectorTerms:
  411      #     - matchExpressions:
  412      #       - key: kubernetes.io/e2e-az-name
  413      #         operator: In
  414      #         values:
  415      #         - e2e-az1
  416      #         - e2e-az2
  417      ## The remote_read spec configuration for Prometheus.
  418      ## ref: https://github.com/coreos/prometheus-operator/blob/master/Documentation/api.md#remotereadspec
  419      remoteRead: []
  420      # - url: http://remote1/read
  421      ## The remote_write spec configuration for Prometheus.
  422      ## ref: https://github.com/coreos/prometheus-operator/blob/master/Documentation/api.md#remotewritespec
  423      remoteWrite: []
  424      # - url: http://remote1/push
  425      ## Enable/Disable Grafana dashboards provisioning for prometheus remote write feature
  426      remoteWriteDashboards: false
  427      ## Resource limits & requests
  428      ##
  429      resources: {}
  430      # requests:
  431      #   memory: 400Mi
  432      ## Prometheus StorageSpec for persistent data
  433      ## ref: https://github.com/coreos/prometheus-operator/blob/master/Documentation/user-guides/storage.md
  434      ##
  435      storageSpec: {}
  436      #  volumeClaimTemplate:
  437      #    spec:
  438      #      storageClassName: gluster
  439      #      accessModes: ["ReadWriteOnce"]
  440      #      resources:
  441      #        requests:
  442      #          storage: 50Gi
  443      #    selector: {}
  444      # Additional volumes on the output StatefulSet definition.
  445      volumes: []
  446      # Additional VolumeMounts on the output StatefulSet definition.
  447      volumeMounts: []
  448      ## AdditionalScrapeConfigs allows specifying additional Prometheus scrape configurations. Scrape configurations
  449      ## are appended to the configurations generated by the Prometheus Operator. Job configurations must have the form
  450      ## as specified in the official Prometheus documentation:
  451      ## https://prometheus.io/docs/prometheus/latest/configuration/configuration/#scrape_config. As scrape configs are
  452      ## appended, the user is responsible to make sure it is valid. Note that using this feature may expose the possibility
  453      ## to break upgrades of Prometheus. It is advised to review Prometheus release notes to ensure that no incompatible
  454      ## scrape configs are going to break Prometheus after the upgrade.
  455      ##
  456      ## The scrape configuraiton example below will find master nodes, provided they have the name .*mst.*, relabel the
  457      ## port to 2379 and allow etcd scraping provided it is running on all Kubernetes master nodes
  458      ##
  459      additionalScrapeConfigs: []
  460      # - job_name: kube-etcd
  461      #   kubernetes_sd_configs:
  462      #     - role: node
  463      #   scheme: https
  464      #   tls_config:
  465      #     ca_file:   /etc/prometheus/secrets/etcd-client-cert/etcd-ca
  466      #     cert_file: /etc/prometheus/secrets/etcd-client-cert/etcd-client
  467      #     key_file:  /etc/prometheus/secrets/etcd-client-cert/etcd-client-key
  468      #   relabel_configs:
  469      #   - action: labelmap
  470      #     regex: __meta_kubernetes_node_label_(.+)
  471      #   - source_labels: [__address__]
  472      #     action: replace
  473      #     targetLabel: __address__
  474      #     regex: ([^:;]+):(\d+)
  475      #     replacement: ${1}:2379
  476      #   - source_labels: [__meta_kubernetes_node_name]
  477      #     action: keep
  478      #     regex: .*mst.*
  479      #   - source_labels: [__meta_kubernetes_node_name]
  480      #     action: replace
  481      #     targetLabel: node
  482      #     regex: (.*)
  483      #     replacement: ${1}
  484      #   metric_relabel_configs:
  485      #   - regex: (kubernetes_io_hostname|failure_domain_beta_kubernetes_io_region|beta_kubernetes_io_os|beta_kubernetes_io_arch|beta_kubernetes_io_instance_type|failure_domain_beta_kubernetes_io_zone)
  486      #     action: labeldrop
  487      ## If additional scrape configurations are already deployed in a single secret file you can use this section.
  488      ## Expected values are the secret name and key
  489      ## Cannot be used with additionalScrapeConfigs
  490      additionalScrapeConfigsSecret: {}
  491        # enabled: false
  492        # name:
  493        # key:
  494      ## additionalPrometheusSecretsAnnotations allows to add annotations to the kubernetes secret. This can be useful
  495      ## when deploying via spinnaker to disable versioning on the secret, strategy.spinnaker.io/versioned: 'false'
  496      additionalPrometheusSecretsAnnotations: {}
  497      ## AdditionalAlertManagerConfigs allows for manual configuration of alertmanager jobs in the form as specified
  498      ## in the official Prometheus documentation https://prometheus.io/docs/prometheus/latest/configuration/configuration/#<alertmanager_config>.
  499      ## AlertManager configurations specified are appended to the configurations generated by the Prometheus Operator.
  500      ## As AlertManager configs are appended, the user is responsible to make sure it is valid. Note that using this
  501      ## feature may expose the possibility to break upgrades of Prometheus. It is advised to review Prometheus release
  502      ## notes to ensure that no incompatible AlertManager configs are going to break Prometheus after the upgrade.
  503      ##
  504      additionalAlertManagerConfigs: []
  505      # - consul_sd_configs:
  506      #   - server: consul.dev.test:8500
  507      #     scheme: http
  508      #     datacenter: dev
  509      #     tag_separator: ','
  510      #     services:
  511      #       - metrics-prometheus-alertmanager
  512      ## AdditionalAlertRelabelConfigs allows specifying Prometheus alert relabel configurations. Alert relabel configurations specified are appended
  513      ## to the configurations generated by the Prometheus Operator. Alert relabel configurations specified must have the form as specified in the
  514      ## official Prometheus documentation: https://prometheus.io/docs/prometheus/latest/configuration/configuration/#alert_relabel_configs.
  515      ## As alert relabel configs are appended, the user is responsible to make sure it is valid. Note that using this feature may expose the
  516      ## possibility to break upgrades of Prometheus. It is advised to review Prometheus release notes to ensure that no incompatible alert relabel
  517      ## configs are going to break Prometheus after the upgrade.
  518      ##
  519      additionalAlertRelabelConfigs: []
  520      # - separator: ;
  521      #   regex: prometheus_replica
  522      #   replacement: $1
  523      #   action: labeldrop
  524      ## SecurityContext holds pod-level security attributes and common container settings.
  525      ## This defaults to non root user with uid 1000 and gid 2000.
  526      ## https://github.com/coreos/prometheus-operator/blob/master/Documentation/api.md
  527      ##
  528      securityContext:
  529        runAsGroup: 2000
  530        runAsNonRoot: true
  531        runAsUser: 1000
  532        fsGroup: 2000
  533      ## Priority class assigned to the Pods
  534      ##
  535      priorityClassName: ""
  536  ll
  537  mv value.txt  pro-gra.yml
  538  kubectl create -f pro-gra.yml 
  539  kubectl create -f pro-gra.yml --validate=false--validate=false
  540  kubectl create -f pro-gra.yml --validate=false
  541  vim pro-gra.yml 
  542  kubectl create -f pro-gra.yml --validate=false
  543  ll
  544  kubectl create -f pro-gra.yml --validate=false
  545  ll
  546  ll
  547  rm -rf mongodb.yaml
  548  cat mongo.yaml
  549  yum install git
  550  git clone https://github.com/DeekshithSN/kubernetes.git
  551  ll
  552  cd kubernetes/
  553  ll
  554  cd monitoring/
  555  ll
  556  /usr/local/bin/k3s-uninstall.sh
  557  /usr/local/bin/k3s-agent-uninstall.sh
  558  curl -sfL https://get.k3s.io | sh -
  559  k3s 
  560  k3s  kubectl
  561  helm install my-release bitnami/jenkins
  562  netstat -nultp
  563  export KUBECONFIG=/etc/rancher/k3s/k3s.yaml
  564  helm repo update
  565  kubectl get all
  566  helm install my-release bitnami/jenkins
  567  helm uninstall my-release bitnami/jenkins
  568  kubectl get po
  569  kubectl delete pod my-release-jenkins-68f6b4fbdc-7gfg7
  570  kubectl get po
  571  helm install prometheus stable/prometheus-operator
  572  helm install my-release bitnami/kube-prometheus
  573  kubectl get deploy -w --namespace default -l app.kubernetes.io/name=kube-prometheus-operator,app.kubernetes.io/instance=my-release
  574  kubectl get svc
  575  kubectl port-forward --namespace default svc/my-release-kube-prometheus-prometheus 9090:9090
  576  kubectl get svc
  577  helm install my-release bitnami/grafana
  578  helm install grafana bitnami/grafana
  579   echo "Browse to http://127.0.0.1:8080"
  580  kubectl port-forward svc/grafana 8080:3000
  581  elm delete my-release
  582  helm delete my-release
  583  helm delete grafana
  584  helm install my-release bitnami/grafana
  585  echo "Password: $(kubectl get secret my-release-grafana-admin --namespace default -o jsonpath="{.data.GF_SECURITY_ADMIN_PASSWORD}" | base64 --decode)"
  586   kubectl port-forward svc/my-release-grafana 8080:3000
  587   helm uninstall my-release bitnami/grafana
  588  kubectl get po
  589  init 0
  590  kubectl get svc
  591  kubectl get po
  592  kubectl get all
  593  kubectl get svc
  594  kubectl get  pod
  595  #kubectl port-forward prometheus-my-release-kube-prometheus-prometheus-0 90
  596  firewall-cmd --perm --add-port=9090/tcp
  597  firewall-cmd --reload
  598  kubectl port-forward prometheus-my-release-kube-prometheus-prometheus-0 9090
  599  kubectl get  pod
  600  kubectl port-forward prometheus-my-release-kube-prometheus-prometheus-0 9090
  601  netstat -nultp
  602  kubectl get svc
  603  kubectl get all
  604  kubectl port-forward prometheus-my-release-kube-prometheus-prometheus-0 9090
  605  kubectl get all
  606  kubectl get po
  607  kubect delete pod grafana-6cc877d4c7-krwwm
  608  kubectl delete pod grafana-6cc877d4c7-krwwm
  609  kubectl get po
  610  kubectl get svc
  611  kubectl get po
  612  kubectl get deployment
  613  helm uninstall grafana
  614  kubectl get svc
  615  helm uninstall my-release
  616  helm uninstall grafana
  617  kubectl get po
  618  kubectl get all
  619  helm delete grafana
  620  helm delete my-release
  621  kubectl get svc
  622  kubectl delete my-release
  623  kubectl delete grafana
  624  kubectl get pods --namespace=monitoring
  625  ll
  626   cd prome-grafana
  627  mkdir prome-grafana
  628  pwd
  629  cd prome-grafana/
  630  ll
  631  pwd
  632  cd
  633  cd kubernetes/
  634  ll
  635  cd monitoring/
  636  ll
  637  cp kubernetes-grafana  kubernetes-prometheus /root/prome-grafana/
  638  cp -rvf  kubernetes-grafana  kubernetes-prometheus /root/prome-grafana/
  639  ll
  640  cd 
  641  cd prome-grafana/
  642  ll
  643  kubectl create namespace monitoring
  644  #kubectl create -f clusterRole.yaml
  645  cd kubernetes-prometheus/
  646  kubectl create -f clusterRole.yaml
  647  kubectl create -f config-map.yaml
  648  kubectl create -f prometheus-deployment.yaml
  649  kubectl get deployments --namespace=monitoring
  650  kubectl get pods --namespace=monitoring
  651  kubectl port-forward prometheus-deployment-599bbd9457-gnpvp 8080:9090 -n monitoring
  652  kubectl port-forward prometheus-monitoring-3331088907-hm5n1 8080:9090 -n monitoringkubectl get pods --namespace=monitoring
  653  kubectl get pods --namespace=monitoring
  654  kubectl port-forward prometheus-deployment-599bbd9457-gnpvp 8080:9090 -n monitoring
  655  ll
  656  kubectl get svc
  657  kubectl get svc all
  658  kubectl get svc --all
  659  kubectl get svc
  660  kubectl port-forward prometheus-deployment-599bbd9457-gnpvp 8080:9090 -n monitoring
  661  kubectltl get pods
  662  kubectl get pods
  663  kubectl get pods --all-namespace
  664  kubectl get pods -all-namespace
  665  kubectl get pods --help
  666  kubectl get deployments --namespace=monitoring
  667  kubectl delete pod prometheus-deployment\
  668  kubectl delete pod prometheus-deployment
  669  kubectl get pods --namespace=monitoring
  670  kubectl delete pod prometheus-deployment-599bbd9457-gnpvp
  671  kubectl describe prometheus-deployment-599bbd9457-gnpvp
  672  kubectkubectl get pods --namespace=monitoring
  673  kubectl get pods --namespace=monitoring
  674  kubectk get namespace
  675  kubectl get namespace
  676  kubectl delete  namespace monitoring
  677  kubectl get pods --namespace=monitoring
  678  ll
  679  kubectl get pods
  680  $ helm install prometheus stable/prometheus-operator --namespace prometheus
  681   helm install prometheus stable/prometheus-operator --namespace prometheus
  682  $ kubectl create namespace prometheus
  683   kubectl create namespace prometheus
  684  $ helm install prometheus stable/prometheus-operator --namespace prometheus
  685   helm install prometheus stable/prometheus-operator --namespace prometheus
  686  ll
  687  git clone https://github.com/gurpreet0610/Deploy-Prometheus-Grafana-on-Kubernetes.git
  688  ll
  689  cd Deploy-Prometheus-Grafana-on-Kubernetes/
  690  ll
  691  kubectl create -f monitoring-namespace.yaml
  692  kubectl create -f prometheus-cluster-role.yaml
  693  kubectl create -f prometheus-config.yaml
  694  kubectl create -f prometheus-deployment.yaml
  695  kubectl create -f prometheus-pvc-nfs.yaml
  696  kubectl create -f prometheus-pv-nfs.yaml
  697  kubectl create -f prometheus-service.yaml
  698  kubectl get svc
  699  kubectl get pods
  700  kubectl get pods --namespace monitoring
  701  kubectl apply -k . -n monitoring
  702  kubectl get all -n monitoring
  703  cd
  704  ll
  705  mv Deploy-Prometheus-Grafana-on-Kubernetes Prometheus-Grafana
  706  ll
  707  kubectl get all -n monitoring
  708  kubectl describe pod pod/prometheus-6977fc7cfd-f882g
  709  kubectl describe pod/prometheus-6977fc7cfd-f882g
  710  kubectl get all -n monitoring
  711  kubectl delete -n monitoring
  712  kubectl delete -namespace  monitoring
  713  kls get ns all
  714  kubectl  get ns all
  715  kubectl  get ns 
  716  kubectl delete --namespace  monitoring
  717  kubectl delete namespace  monitoring
  718  ll
  719  free -h
  720  k3s
  721  cd /etc/
  722  ll
  723  ls
  724  ll
  725  cd
  726  ll
  727  cat pro-gra.yml
  728  helm repo add bitnami https://charts.bitnami.com/bitnami
  729   helm install my-release bitnami/mongodb
  730  kubectl get cluster
  731  kubectl get all
  732  ifup ens33
  733  nmtui
  734  ifup ens33
  735  nmdcli d s
  736  nmcli d s
  737  ip a
  738  nmtui
  739  ip a
  740  ping 8.8.8.8
  741  mount /dev/sr0 /mnt
  742  ll
  743  ll\
  744  ip a
  745  nmtui
  746  ip a
  747  ping 8.8.8.8
  748  netstat -nultp
  749  kubectl get svc
  750  free -h
  751  kubectl get svc
  752   helm install prometheus prometheus-community/kube-prometheus-stack
  753  ping 8.8.8.8
  754   helm install dhana prometheus-community/kube-prometheus-stack
  755  ll
  756  rm -rf prometheus-grafana.yml
  757  rm -rf pro-gra.yml
  758  helm show values prometheus-community/kube-prometheus-stack
  759  helm install my-release bitnami/grafana
  760  history
  761   export KUBECONFIG=/etc/rancher/k3s/k3s.yaml
  762  helm install my-release bitnami/grafana
  763  echo "User: admin"
  764  echo "Password: $(kubectl get secret my-release-grafana-admin --namespace default -o jsonpath="{.data.GF_SECURITY_ADMIN_PASSWORD}" | base64 --decode)"
  765  kubectl get svc
  766  firewall-cmd --list-all
  767  firewall-cmd --perm -add-port=3000/tcp
  768  firewall-cmd --perm --add-port=3000/tcp
  769  firewall-cmd --reload
  770  kubectl get svc
  771  kubectl get all
  772  kubectl port-forward svc/my-release-grafana 8080:3000
  773  kubectl port-forward pod/my-release-grafana-64488d7767-blqcv 8080:3000
  774  kubectl describe pod/my-release-grafana-64488d7767-blqcv
  775  kubectl delete pod/my-release-grafana-64488d7767-blqcv
  776  kubectl get all
  777  helm delete my-release
  778   helm install dhana prometheus-community/kube-prometheus-stack
  779   helm install prometheus  prometheus-community/kube-prometheus-stack
  780  helm install my-release bitnami/grafana
  781  get svc
  782  kubectl get svc
  783  kubectl get all
  784  kubectl logs pod/my-release-grafana-67d697b64c-tl8fr
  785  kubectl describe  pod/my-release-grafana-67d697b64c-tl8fr
  786  kubectl delete  pod/my-release-grafana-67d697b64c-tl8fr
  787  kubectl get all
  788  kubectl describe  pod/my-release-grafana-67d697b64c-w9chq
  789  kubectl get all
  790  helm delete my-release
  791   helm install dhana prometheus-community/kube-prometheus-stack
  792  ll
  793   helm install dhana prometheus-community/kube-prometheus-stack
  794  helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
  795  helm repo update
  796  kubectl delete crd alertmanagerconfigs.monitoring.coreos.com
  797  kubectl delete crd alertmanagers.monitoring.coreos.com
  798  kubectl delete crd podmonitors.monitoring.coreos.com
  799  kubectl delete crd probes.monitoring.coreos.com
  800  kubectl delete crd prometheuses.monitoring.coreos.com
  801  kubectl delete crd prometheusrules.monitoring.coreos.com
  802  kubectl delete crd servicemonitors.monitoring.coreos.com
  803  kubectl delete crd thanosrulers.monitoring.coreos.com
  804  helm show values kube-prometheus-stack
  805  helm repo list
  806  helm repo prometheus-community
  807  kubectl show values prometheus-community/kube-prometheus-stack
  808  helm  show values prometheus-community/kube-prometheus-stack
  809  helm  show values prometheus-community/kube-prometheus-stack >  prometheus-grafana.yml
  810  cat prometheus-grafana.yml
  811  vim  prometheus-grafana.yml
  812   helm install prometheus prometheus-community/kube-prometheus-stack
  813  helm install  prometheus prometheus-community/kube-prometheus-stack
  814  history
  815  export KUBECONFIG=/etc/rancher/k3s/k3s.yaml
  816  ! 879
  817  helm install  prometheus prometheus-community/kube-prometheus-stack
  818  export KUBECONFIG=/etc/rancher/k3s/k3s.yaml
  819  helm install  prometheus prometheus-community/kube-prometheus-stack
  820  helm install  prometheus1 prometheus-community/kube-prometheus-stack
  821  helm delete  prometheus1
  822  helm delete  prometheus
  823  kubectl delete crd alertmanagerconfigs.monitoring.coreos.com
  824  kubectl delete crd alertmanagers.monitoring.coreos.com
  825  kubectl delete crd podmonitors.monitoring.coreos.com
  826  kubectl delete crd probes.monitoring.coreos.com
  827  kubectl delete crd prometheuses.monitoring.coreos.com
  828  kubectl delete crd prometheusrules.monitoring.coreos.com
  829  kubectl delete crd servicemonitors.monitoring.coreos.com
  830  kubectl delete crd thanosrulers.monitoring.coreos.com
  831  helm install my-release bitnami/grafana
  832  history
  833  helm install my-release bitnami/grafana
  834  export KUBECONFIG=/etc/rancher/k3s/k3s.yaml
  835  helm install my-release bitnami/grafana
  836  kubectl get all
  837  kubectl describe pod/prometheus1-kube-prometheu-admission-create--1-rcs8h
  838  kubectl get all
  839  kubectl delete pod/prometheus1-kube-prometheu-admission-create--1-rcs8h
  840  kubectl delete replicaset.apps/my-release-grafana-599d8b86b4
  841  helm delete my-release
  842  kubectl get all
  843  kubectl delete service/kubernetes
  844  kubectl delete pod/prometheus1-kube-prometheu-admission-create--1-v4wlm
  845  kubectl delete job.batch/prometheus-kube-prometheus-admission-create
  846  kubectl delete job.batch/prometheus1-kube-prometheu-admission-create
  847  kubectl get all
  848  kubectl delete service/kubernetes
  849  kubectl get all
  850  kubectl get po
  851  kubectl get svc
  852  kubectl get no
  853  top
  854  kubectl get all
  855  helm delete prometheus1
  856  history
  857   export KUBECONFIG=/etc/rancher/k3s/k3s.yaml
  858  helm delete prometheus1
  859   export KUBECONFIG=/etc/rancher/k3s/k3s.yaml
  860  helm delete prometheus1
  861   export KUBECONFIG=/etc/rancher/k3s/k3s.yaml
  862  kubectl get all
  863  kubectl get po
  864  kubectl get svc
  865  kubectl get ns
  866  helm delete prometheus1
  867  /usr/local/bin/k3s-uninstall.sh
  868  ll
  869  free -h
  870  init 6
  871  kubectl get po
  872  kubectl get svc
  873  kubectl svc
  874  kubectl get svc
  875  kubectl get all
  876  kubectl port-forward pod/prometheus1-grafana-c77c67fd-9xc2f  80:3000
  877  ll
  878   helm search repo prometheus-community
  879  helm show values prometheus-community/kube-prometheus-stack
  880  helm show values prometheus-community/kube-prometheus-stack > kube-prometheus-stack.yml
  881  ll
  882  vim kube-prometheus-stack.yml
  883  cp kube-prometheus-stack.yml /home/dhana/
  884  vim kube-prometheus-stack.yml
  885  kubectl create -f kube-prometheus-stack.yml
  886  curl -sfL https://get.k3s.io | sh -
  887  kubectl create -f kube-prometheus-stack.yml
  888  kubectl create -f kube-prometheus-stack.yml  --validate=false
  889  init 6
  890  kubectl
  891  kubectl api-resources
  892  kubectl
  893  kubectl api-versions
  894  kubectl 
  895  mount /dev/sr0 /mnt
  896  yum install httpd
  897  kill -9 10212
  898  yum install httpd
  899  systemctl status httpd
  900  init 3
  901  kubectl get nodes
  902  kubectl get po
  903  kubectl get all
  904  init 5
  905  ll
  906  kubectl get po
  907  kubectl cerate -f  metricbeat-kubernetes.yaml
  908  kubectl create -f  metricbeat-kubernetes.yaml
  909  kubectl delete -f  metricbeat-kubernetes.yaml
  910  rm -rf metricbeat-kubernetes.yaml
  911  mv kube-metricbeat.yml /home/dhana/
  912  ll
  913  kubectl create -f  values-metricbeat.yaml
  914  kubectl create -f  values-metricbeat.yaml --validate=false
  915  helm repo all
  916  helm repo list
  917  history
  918  helm install --name metricbeat -f values-metricbeat.yaml elastic/metricbeat
  919  helm install -name metricbeat -f values-metricbeat.yaml elastic/metricbeat
  920  export KUBECONFIG=/etc/rancher/k3s/k3s.yaml
  921  helm install -name metricbeat -f values-metricbeat.yaml elastic/metricbeat
  922  helm install -name metricbeat1 -f values-metricbeat.yaml elastic/metricbeat
  923   kubectl get pods --namespace=ame -l app=metricbeat1-metricbeat -w
  924  kubectl describe metricbeat1-metricbeat-qhvj9 -n ame
  925  kubectl describe pods  metricbeat1-metricbeat-qhvj9 -n ame
  926  kubectl logs  metricbeat1-metricbeat-qhvj9 -n ame
  927  kubectl edit  metricbeat1-metricbeat-qhvj9 -n ame
  928   kubectl get pods --namespace=ame -l app=metricbeat1-metricbeat -w
  929  helm uninstall -name metricbeat1 -f values-metricbeat.yaml elastic/metricbeat
  930  helm delete metricbeat1
  931  helm uninstall  metricbeat1
  932  export KUBECONFIG=/etc/rancher/k3s/k3s.yaml
  933  helm uninstall  metricbeat1
  934* 
  935  helm uninstall  metricbeat1
  936  helm uninstall  metricbeat
  937  helm uninstall  metricbeat1
  938  ll
  939  mount /dev/sr0 /mnt/
  940  yum install git
  941  git clone git@github.com:ahmetb/kubectx.git
  942  ll
  943  git clone https://github.com/ahmetb/kubectx.git
  944  ll
  945  cd kubectx
  946  ll
  947  kubens
  948  cp  kubectx kubens /usr/local/bin/
  949  kubens
  950  kubens ame
  951  ll
  952  kubectl get po
  953* 
  954* 
  955  kubectl describe pods  metricbeat-metricbeat-metrics-9d49d77df-p8rwg  -n ame
  956  kubectl get po
  957  helm delete metricbeat
  958  helm delete metricbeat1
  959  cd 
  960  kubectx
  961  kubens
  962  kubens default
  963  kubens
  964  kubens ame
  965  kubectl create ns dhana -o yaml
  966  kubens
  967  kubectl delete  ns dhana -o yaml
  968  kubectl delete  ns dhana 
  969  kubectl get po -n kube-system
  970  kubectl get po -n kube-public
  971  kubectl get po -n kube-node-lease
  972  kubectl get api-resources
  973  kubectl api-resources
  974  kubectl api-version
  975  kubectl api-versions
  976  kubectl get servicename
  977  kubectl get svc
  978* kubens default 
  979  kubectl run ghost --image=ngnix 
  980  kubectl get po
  981  kubectl describel ghost
  982  kubectl describe ghost
  983  kubectl describe pod  ghost
  984  kubectl delete ghost
  985  kubectl delete pod  ghost
  986  kubectl get no
  987  kubectl expalin 
  988  kubectl  explain
  989  curl -sL install-node.now.sh/lts | bash
  990  :CocInstall coc-yaml
  991  CocInstall coc-yaml
  992  :CocConfig
  993  curl -sL uninstall-node.now.sh/lts | bash
  994  ll
  995  cd /usr/local/
  996  ll
  997  cd bin/
  998  ll
  999  rm -rf npm npx
 1000  ll
 1001  vim .vimrc
 1002  vim firts.yml
 1003  yum install  yamllint
 1004  vim firts.yml
 1005  vim ~/.config/yamllint/config 
 1006  sudo vim ~/.config/yamllint/config 
 1007  kubectl expalin  pod
 1008  kubectl explain  pod
 1009  kubectl explain  pod.spec
 1010  kubectl get deploy
 1011  kubectl create  deploy --image=nginx  
 1012  kubectl create deployment -h --image=nginx
 1013  kubectl create deployment dhana  --image=nginx
 1014  kubectl get deploy
 1015  kubectl get po
 1016  kubectl get deploy dhana-666f9cc654-bfdvx -o yaml > deploy-nginx.yml
 1017  kubectl get deploy dhana -o yaml > deploy-nginx.yml
 1018  ll
 1019  mv deploy-nginx.yml /root/
 1020  cd 
 1021  ll
 1022  where is deploy-nginx.yml
 1023  whereis deploy-nginx.yml
 1024  vim deploy-nginxdeploy-nginx
 1025  cd /root
 1026  ll
 1027  cat deploy-nginx.yml
 1028  vim deploy-nginx.yml
 1029  history
]0;root@localhost:~[root@localhost ~]# kubectl cluster-info dump
{
    "kind": "NodeList",
    "apiVersion": "v1",
    "metadata": {
        "resourceVersion": "20940"
    },
    "items": [
        {
            "metadata": {
                "name": "localhost.localdomain",
                "uid": "6008b14d-ba1d-4802-b09b-c1cbba32cbbd",
                "resourceVersion": "20889",
                "creationTimestamp": "2022-03-22T17:40:06Z",
                "labels": {
                    "beta.kubernetes.io/arch": "amd64",
                    "beta.kubernetes.io/instance-type": "k3s",
                    "beta.kubernetes.io/os": "linux",
                    "kubernetes.io/arch": "amd64",
                    "kubernetes.io/hostname": "localhost.localdomain",
                    "kubernetes.io/os": "linux",
                    "node-role.kubernetes.io/control-plane": "true",
                    "node-role.kubernetes.io/master": "true",
                    "node.kubernetes.io/instance-type": "k3s"
                },
                "annotations": {
                    "flannel.alpha.coreos.com/backend-data": "{\"VNI\":1,\"VtepMAC\":\"8e:92:77:f3:df:fc\"}",
                    "flannel.alpha.coreos.com/backend-type": "vxlan",
                    "flannel.alpha.coreos.com/kube-subnet-manager": "true",
                    "flannel.alpha.coreos.com/public-ip": "192.168.1.30",
                    "k3s.io/hostname": "localhost.localdomain",
                    "k3s.io/internal-ip": "192.168.1.30",
                    "k3s.io/node-args": "[\"server\"]",
                    "k3s.io/node-config-hash": "5QFVFOUYVVEYZTAKZ47ZLFVH7TT264YJQLFANKIZQLCB7OIFGNYA====",
                    "k3s.io/node-env": "{\"K3S_DATA_DIR\":\"/var/lib/rancher/k3s/data/31ff0fd447a47323a7c863dbb0a3cd452e12b45f1ec67dc55efa575503c2c3ac\"}",
                    "node.alpha.kubernetes.io/ttl": "0",
                    "volumes.kubernetes.io/controller-managed-attach-detach": "true"
                },
                "finalizers": [
                    "wrangler.cattle.io/node"
                ]
            },
            "spec": {
                "podCIDR": "10.42.0.0/24",
                "podCIDRs": [
                    "10.42.0.0/24"
                ],
                "providerID": "k3s://localhost.localdomain"
            },
            "status": {
                "capacity": {
                    "cpu": "1",
                    "ephemeral-storage": "28175284Ki",
                    "hugepages-1Gi": "0",
                    "hugepages-2Mi": "0",
                    "memory": "2895196Ki",
                    "pods": "110"
                },
                "allocatable": {
                    "cpu": "1",
                    "ephemeral-storage": "27408916254",
                    "hugepages-1Gi": "0",
                    "hugepages-2Mi": "0",
                    "memory": "2895196Ki",
                    "pods": "110"
                },
                "conditions": [
                    {
                        "type": "MemoryPressure",
                        "status": "False",
                        "lastHeartbeatTime": "2022-04-10T10:13:13Z",
                        "lastTransitionTime": "2022-04-08T17:10:03Z",
                        "reason": "KubeletHasSufficientMemory",
                        "message": "kubelet has sufficient memory available"
                    },
                    {
                        "type": "DiskPressure",
                        "status": "False",
                        "lastHeartbeatTime": "2022-04-10T10:13:13Z",
                        "lastTransitionTime": "2022-04-08T17:10:03Z",
                        "reason": "KubeletHasNoDiskPressure",
                        "message": "kubelet has no disk pressure"
                    },
                    {
                        "type": "PIDPressure",
                        "status": "False",
                        "lastHeartbeatTime": "2022-04-10T10:13:13Z",
                        "lastTransitionTime": "2022-04-08T17:10:03Z",
                        "reason": "KubeletHasSufficientPID",
                        "message": "kubelet has sufficient PID available"
                    },
                    {
                        "type": "Ready",
                        "status": "True",
                        "lastHeartbeatTime": "2022-04-10T10:13:13Z",
                        "lastTransitionTime": "2022-04-10T08:07:30Z",
                        "reason": "KubeletReady",
                        "message": "kubelet is posting ready status"
                    }
                ],
                "addresses": [
                    {
                        "type": "InternalIP",
                        "address": "192.168.1.30"
                    },
                    {
                        "type": "Hostname",
                        "address": "localhost.localdomain"
                    }
                ],
                "daemonEndpoints": {
                    "kubeletEndpoint": {
                        "Port": 10250
                    }
                },
                "nodeInfo": {
                    "machineID": "6f4068d0e3b14502b442b30a0de63ab6",
                    "systemUUID": "E63B4D56-C778-C023-3A3A-528EE4FF200D",
                    "bootID": "3f0bfeaf-b38a-4ef0-b849-153e1b8ba537",
                    "kernelVersion": "3.10.0-1160.el7.x86_64",
                    "osImage": "Red Hat Enterprise Linux Server 7.9 (Maipo)",
                    "containerRuntimeVersion": "containerd://1.5.9-k3s1",
                    "kubeletVersion": "v1.22.7+k3s1",
                    "kubeProxyVersion": "v1.22.7+k3s1",
                    "operatingSystem": "linux",
                    "architecture": "amd64"
                },
                "images": [
                    {
                        "names": [
                            "docker.elastic.co/beats/metricbeat@sha256:8821ae9b18ba923ca435f621de4b0003f05b5733b997739da550f4355852b868",
                            "docker.elastic.co/beats/metricbeat:7.3.0"
                        ],
                        "sizeBytes": 176753775
                    },
                    {
                        "names": [
                            "docker.elastic.co/beats/metricbeat@sha256:eb04284c9ee1401776e9e27d825fdacfa6a167a4d6932645ecc140884bce70bd",
                            "docker.elastic.co/beats/metricbeat:7.10.1"
                        ],
                        "sizeBytes": 168313470
                    },
                    {
                        "names": [
                            "docker.elastic.co/beats/metricbeat@sha256:63afe1bf33af3b0c6e19cc9ed7fbc38ce47c3a7fe413afb42e96de5b0ea17436",
                            "docker.elastic.co/beats/metricbeat:8.1.2"
                        ],
                        "sizeBytes": 116621600
                    },
                    {
                        "names": [
                            "docker.io/grafana/grafana@sha256:ebe2167bfeba1a7cde5cbed865428ce2802e4e163f635d84fc2c46fab930135f",
                            "docker.io/grafana/grafana:8.4.2"
                        ],
                        "sizeBytes": 88979149
                    },
                    {
                        "names": [
                            "docker.io/rancher/klipper-helm@sha256:961459b8641aa77dae39c96ba8efba88805a8949dcaafa131d1aecc1510128bf",
                            "docker.io/rancher/klipper-helm:v0.6.6-build20211022"
                        ],
                        "sizeBytes": 84453835
                    },
                    {
                        "names": [
                            "quay.io/prometheus/prometheus@sha256:91100b06e86d724d5757e8f5a8920d1e22981067a5acf7eb2f47c3ae62be4ed2",
                            "quay.io/prometheus/prometheus:v2.33.5"
                        ],
                        "sizeBytes": 77557054
                    },
                    {
                        "names": [
                            "docker.io/library/nginx@sha256:2275af0f20d71b293916f1958f8497f987b8d8fd8113df54635f2a5915002bf1",
                            "docker.io/library/nginx:latest"
                        ],
                        "sizeBytes": 56745242
                    },
                    {
                        "names": [
                            "docker.io/rancher/mirrored-library-traefik@sha256:d2b19904395891849a78a41159ee8ae812b9cb10bd40103d3a9557cd76c06758",
                            "docker.io/rancher/mirrored-library-traefik:2.6.1"
                        ],
                        "sizeBytes": 30350231
                    },
                    {
                        "names": [
                            "quay.io/prometheus/alertmanager@sha256:9ab73a421b65b80be072f96a88df756fc5b52a1bc8d983537b8ec5be8b624c5a",
                            "quay.io/prometheus/alertmanager:v0.23.0"
                        ],
                        "sizeBytes": 26476737
                    },
                    {
                        "names": [
                            "docker.io/rancher/mirrored-metrics-server@sha256:48ecad4fe641a09fa4459f93c7ad29d4916f6b9cf7e934d548f1d8eff96e2f35",
                            "docker.io/rancher/mirrored-metrics-server:v0.5.2"
                        ],
                        "sizeBytes": 26022698
                    },
                    {
                        "names": [
                            "quay.io/kiwigrid/k8s-sidecar@sha256:1f025ae37b7b20d63bffd179e5e6f972039dd53d9646388c0a8c456229c7bbcb",
                            "quay.io/kiwigrid/k8s-sidecar:1.15.6"
                        ],
                        "sizeBytes": 24764376
                    },
                    {
                        "names": [
                            "k8s.gcr.io/ingress-nginx/kube-webhook-certgen@sha256:64d8c73dca984af206adf9d6d7e46aa550362b1d7a01f3a0a91b20cc67868660",
                            "k8s.gcr.io/ingress-nginx/kube-webhook-certgen:v1.1.1"
                        ],
                        "sizeBytes": 18899182
                    },
                    {
                        "names": [
                            "quay.io/prometheus-operator/prometheus-operator@sha256:4821c8805185ccac857928569302c1f48ba2d416a67bf71447758bf213240f3f",
                            "quay.io/prometheus-operator/prometheus-operator:v0.55.0"
                        ],
                        "sizeBytes": 14291059
                    },
                    {
                        "names": [
                            "docker.io/rancher/mirrored-coredns-coredns@sha256:d4aec4c63c47a553b334dbf079578cd9dc8616866a467d87e3e6b94e26ea9ed7",
                            "docker.io/rancher/mirrored-coredns-coredns:1.8.6"
                        ],
                        "sizeBytes": 13584486
                    },
                    {
                        "names": [
                            "docker.io/rancher/local-path-provisioner@sha256:1da612c913ce0b4ab82e20844baa9dce1f7065e39412d6a0bb4de99c413f21bf",
                            "docker.io/rancher/local-path-provisioner:v0.0.21"
                        ],
                        "sizeBytes": 11434864
                    },
                    {
                        "names": [
                            "k8s.gcr.io/kube-state-metrics/kube-state-metrics@sha256:69a18fa1e0d0c9f972a64e69ca13b65451b8c5e79ae8dccf3a77968be4a301df",
                            "k8s.gcr.io/kube-state-metrics/kube-state-metrics:v2.4.1"
                        ],
                        "sizeBytes": 11288596
                    },
                    {
                        "names": [
                            "quay.io/prometheus/node-exporter@sha256:f2269e73124dd0f60a7d19a2ce1264d33d08a985aed0ee6b0b89d0be470592cd",
                            "quay.io/prometheus/node-exporter:v1.3.1"
                        ],
                        "sizeBytes": 10347719
                    },
                    {
                        "names": [
                            "quay.io/prometheus-operator/prometheus-config-reloader@sha256:5f20b712ce8562ea564a9b269f30547a629c12b5e7a42203bae9c70141eb4796",
                            "quay.io/prometheus-operator/prometheus-config-reloader:v0.55.0"
                        ],
                        "sizeBytes": 4838723
                    },
                    {
                        "names": [
                            "docker.io/rancher/klipper-lb@sha256:d5ec8350ee1a80ec783effe95a2fefae371d1498ac04865cb6024499882f291c",
                            "docker.io/rancher/klipper-lb:v0.3.4"
                        ],
                        "sizeBytes": 3326144
                    },
                    {
                        "names": [
                            "docker.io/rancher/mirrored-pause@sha256:74c4244427b7312c5b901fe0f67cbc53683d06f4f24c6faee65d4182bf0fa893",
                            "docker.io/rancher/mirrored-pause:3.6"
                        ],
                        "sizeBytes": 301463
                    }
                ]
            }
        }
    ]
}
{
    "kind": "EventList",
    "apiVersion": "v1",
    "metadata": {
        "resourceVersion": "20940"
    },
    "items": []
}
{
    "kind": "ReplicationControllerList",
    "apiVersion": "v1",
    "metadata": {
        "resourceVersion": "20940"
    },
    "items": []
}
{
    "kind": "ServiceList",
    "apiVersion": "v1",
    "metadata": {
        "resourceVersion": "20940"
    },
    "items": [
        {
            "metadata": {
                "name": "kube-dns",
                "namespace": "kube-system",
                "uid": "52dbe5e9-d3c4-40fb-b73c-8103d555cfd7",
                "resourceVersion": "260",
                "creationTimestamp": "2022-03-22T17:40:16Z",
                "labels": {
                    "k8s-app": "kube-dns",
                    "kubernetes.io/cluster-service": "true",
                    "kubernetes.io/name": "CoreDNS",
                    "objectset.rio.cattle.io/hash": "bce283298811743a0386ab510f2f67ef74240c57"
                },
                "annotations": {
                    "objectset.rio.cattle.io/applied": "H4sIAAAAAAAA/4yRwY7TMBCGXwXN2Qlx026zljig3QtCQistcEEcHGdKTRKP5ZkWoSrvjtxmRWEV2Fui+fzpn39OYKP/jIk9BTBw1KCg96EDA4+Yjt4hKBhRbGfFgjmBDYHEiqfA+Zfa7+iEUcrkqXRWZMDS02ufDaAW5/QjYCq+HXsw0Nd8NTlq9eq9D92bt11H4b+KYEcEA44SdoFfhHO0Lr/pDy0W/JMFR1AQE40oezxwpiMlAQO3elM/m7FLNmaBpAPCpGCwLQ7nOvqGCxvjk/ySKH+mgILn1244sGAqeK531vyNzXvdUcL7D4//2GtveQ8GWoerpl7dNo3W23Vtq7q5se1GV7vV7maLu+16ta7cZpvzzu6riEu1TAo4osurzbnfPYABXZXruqxKXeVyKAmD+XJ68l6UlwY39bk9IUcDGPh0/wCTuiYLcXGJ/nj3Bz2iJO9+u/NxnvNfFTAO6ITSwkWmafoVAAD//yO4Hor3AgAA",
                    "objectset.rio.cattle.io/id": "",
                    "objectset.rio.cattle.io/owner-gvk": "k3s.cattle.io/v1, Kind=Addon",
                    "objectset.rio.cattle.io/owner-name": "coredns",
                    "objectset.rio.cattle.io/owner-namespace": "kube-system",
                    "prometheus.io/port": "9153",
                    "prometheus.io/scrape": "true"
                }
            },
            "spec": {
                "ports": [
                    {
                        "name": "dns",
                        "protocol": "UDP",
                        "port": 53,
                        "targetPort": 53
                    },
                    {
                        "name": "dns-tcp",
                        "protocol": "TCP",
                        "port": 53,
                        "targetPort": 53
                    },
                    {
                        "name": "metrics",
                        "protocol": "TCP",
                        "port": 9153,
                        "targetPort": 9153
                    }
                ],
                "selector": {
                    "k8s-app": "kube-dns"
                },
                "clusterIP": "10.43.0.10",
                "clusterIPs": [
                    "10.43.0.10"
                ],
                "type": "ClusterIP",
                "sessionAffinity": "None",
                "ipFamilies": [
                    "IPv4"
                ],
                "ipFamilyPolicy": "SingleStack",
                "internalTrafficPolicy": "Cluster"
            },
            "status": {
                "loadBalancer": {}
            }
        },
        {
            "metadata": {
                "name": "metrics-server",
                "namespace": "kube-system",
                "uid": "4d8c70a4-ea50-45e0-b62f-95f479e8268d",
                "resourceVersion": "313",
                "creationTimestamp": "2022-03-22T17:40:19Z",
                "labels": {
                    "kubernetes.io/cluster-service": "true",
                    "kubernetes.io/name": "Metrics-server",
                    "objectset.rio.cattle.io/hash": "a5d3bc601c871e123fa32b27f549b6ea770bcf4a"
                },
                "annotations": {
                    "objectset.rio.cattle.io/applied": "H4sIAAAAAAAA/4SQQWvjMBCF/8oyZzkbx06cFexh2WMpBFp6KT2M5Umi2pGEZuJSgv97kZNCQpvmJKQ37833dAAM9okiW+9AQ5+Dgta6BjQ8UOytIVCwI8EGBUEfAJ3zgmK943T19SsZYZJJtH5iUKSjifW/bUoAdVX3b45itulb0NAWfKb0ufp1Z13z91/TeHczwuGOQCfEaA1nTLGnOB5H9ttuDmhSRLuvKeN3FtrBoKDDmrqxYxKiIyFORtPtWc5WaJC4T5sux05c9xdcP/BskbegAedNUZvFNDfLKqd8VqyxmNWzaj0v/9QLwqqa1mZdYiL8tjoc36+U4kAmVQo+CoN+PnyGbEUCgxoF0GVZKAjRize+Aw2P/1egQDBuSFbjxMkwvChg6siIj+NXLTnDEL5SDcPwEQAA//8Uey7TawIAAA",
                    "objectset.rio.cattle.io/id": "",
                    "objectset.rio.cattle.io/owner-gvk": "k3s.cattle.io/v1, Kind=Addon",
                    "objectset.rio.cattle.io/owner-name": "metrics-server-service",
                    "objectset.rio.cattle.io/owner-namespace": "kube-system"
                }
            },
            "spec": {
                "ports": [
                    {
                        "name": "https",
                        "protocol": "TCP",
                        "port": 443,
                        "targetPort": "https"
                    }
                ],
                "selector": {
                    "k8s-app": "metrics-server"
                },
                "clusterIP": "10.43.119.17",
                "clusterIPs": [
                    "10.43.119.17"
                ],
                "type": "ClusterIP",
                "sessionAffinity": "None",
                "ipFamilies": [
                    "IPv4"
                ],
                "ipFamilyPolicy": "SingleStack",
                "internalTrafficPolicy": "Cluster"
            },
            "status": {
                "loadBalancer": {}
            }
        },
        {
            "metadata": {
                "name": "dhana-kube-prometheus-stac-kubelet",
                "namespace": "kube-system",
                "uid": "b4edee8f-c07e-495f-8580-478f5b5d7c1e",
                "resourceVersion": "6325",
                "creationTimestamp": "2022-03-26T17:13:23Z",
                "labels": {
                    "app.kubernetes.io/managed-by": "prometheus-operator",
                    "app.kubernetes.io/name": "kubelet",
                    "k8s-app": "kubelet"
                }
            },
            "spec": {
                "ports": [
                    {
                        "name": "https-metrics",
                        "protocol": "TCP",
                        "port": 10250,
                        "targetPort": 10250
                    },
                    {
                        "name": "http-metrics",
                        "protocol": "TCP",
                        "port": 10255,
                        "targetPort": 10255
                    },
                    {
                        "name": "cadvisor",
                        "protocol": "TCP",
                        "port": 4194,
                        "targetPort": 4194
                    }
                ],
                "clusterIP": "None",
                "clusterIPs": [
                    "None"
                ],
                "type": "ClusterIP",
                "sessionAffinity": "None",
                "ipFamilies": [
                    "IPv4",
                    "IPv6"
                ],
                "ipFamilyPolicy": "RequireDualStack",
                "internalTrafficPolicy": "Cluster"
            },
            "status": {
                "loadBalancer": {}
            }
        },
        {
            "metadata": {
                "name": "traefik",
                "namespace": "kube-system",
                "uid": "29bd8573-82e4-4218-a72d-af4a432dc7f6",
                "resourceVersion": "18803",
                "creationTimestamp": "2022-03-23T15:12:58Z",
                "labels": {
                    "app.kubernetes.io/instance": "traefik",
                    "app.kubernetes.io/managed-by": "Helm",
                    "app.kubernetes.io/name": "traefik",
                    "helm.sh/chart": "traefik-10.14.100"
                },
                "annotations": {
                    "meta.helm.sh/release-name": "traefik",
                    "meta.helm.sh/release-namespace": "kube-system"
                }
            },
            "spec": {
                "ports": [
                    {
                        "name": "web",
                        "protocol": "TCP",
                        "port": 80,
                        "targetPort": "web",
                        "nodePort": 32054
                    },
                    {
                        "name": "websecure",
                        "protocol": "TCP",
                        "port": 443,
                        "targetPort": "websecure",
                        "nodePort": 31478
                    }
                ],
                "selector": {
                    "app.kubernetes.io/instance": "traefik",
                    "app.kubernetes.io/name": "traefik"
                },
                "clusterIP": "10.43.47.227",
                "clusterIPs": [
                    "10.43.47.227"
                ],
                "type": "LoadBalancer",
                "sessionAffinity": "None",
                "externalTrafficPolicy": "Cluster",
                "ipFamilies": [
                    "IPv4"
                ],
                "ipFamilyPolicy": "SingleStack",
                "allocateLoadBalancerNodePorts": true,
                "internalTrafficPolicy": "Cluster"
            },
            "status": {
                "loadBalancer": {
                    "ingress": [
                        {
                            "ip": "192.168.1.30"
                        }
                    ]
                }
            }
        }
    ]
}
{
    "kind": "DaemonSetList",
    "apiVersion": "apps/v1",
    "metadata": {
        "resourceVersion": "20940"
    },
    "items": [
        {
            "metadata": {
                "name": "svclb-traefik",
                "namespace": "kube-system",
                "uid": "73be0387-d9d0-4792-8fae-312ec69407f9",
                "resourceVersion": "18732",
                "generation": 1,
                "creationTimestamp": "2022-03-23T15:12:58Z",
                "labels": {
                    "objectset.rio.cattle.io/hash": "f31475152fbf70655d3c016d368e90118938f6ea",
                    "svccontroller.k3s.cattle.io/nodeselector": "false"
                },
                "annotations": {
                    "deprecated.daemonset.template.generation": "1",
                    "objectset.rio.cattle.io/applied": "H4sIAAAAAAAA/8RVwW7jNhD9lWLOkiJZSuwI6CFIcgjadQzb28vCCEbUKGZNkQI5UmME+veCshN7N06yKFr0ZJOceeB7b/T4DNjIP8g6aTTkgE3jzroEAthIXUION0i10QtiCKAmxhIZIX8G1NowsjTa+aUp/iTBjjiy0kQCmRVF0pxJj+E6IYxma5QiC8G7xeYvTTZ87DaQw1mXBL/8JnX564JsJwV92qexJsiBLVIlNz9V7hoUvmfTFhS6rWOqoQ9AWBqYLWVNjrFuINetUgEoLEh9yHeNbg05VGmSjc+T81FVVOP44vy8TEWcXJTpxYQu4ySZXKaT6oIQgu/FiTapO0LTpiRHigQb61FROfIX3FN1nVBFeCD8HqUABspzqsiSFuQg//aD74PlRyblbFt6HYKDBW9EbgeHR5dFOTkfp+FkRFmYjZJJiONRGWKVYZaOSjGuLqBf9QG4hoQX8EDrGWpksf79VVtsmjfk+j4AprpRyDS0HI3iT9h1CvJj6V0nfuDaH90eWza1aTXvlbkSwq+WZkMa8sGnnZwoNdmd3qS74XcPu5hfP8zu50sIoEPV+q1JDH3wWnBzu1g+zOb3y/ujkuX17G3Npyh3s8VRQRJHWRpl42g0GkO/CkDW+OgPLGqxJnu2UbJpyIaqyLs4SqMM9jWzVqmZUVJsIYe7amp4ZsmR5sNkqCJsjOVwEkMA/t+O/KsWM2MZ8kkcwNo4PqxOtlvDRhj1wnsVgCVnWjuM8LP3g0RrJW+vjWZ64mEasMFCKsmSdtaXJeTfYHq7fLi6+XI3hVXfe30+tyPL0n/Dj1Mw/4Mh/hofOJJl6bElw/I0wn9myioANorsy6viv5mqIsGQw9QsxJrKVvkQ2pCn68MxtEZR5LPOamJy/sut0fHumWk81pCct0/SsRts+CeQ+5QIG4Wa3kXeYVxbyVKguipLo929VtvTDSufJ21TItOCLTI9br0yPoykfvw6HOzC8emrxg6lwkIR5IlPwm3jjZl/VzvkEyO3g8KitZY0T9u6IPtCtIQ8DqAkJy2Vp470sPdFOndie05YbiGP+/7vAAAA///e3gyGNQgAAA",
                    "objectset.rio.cattle.io/id": "svccontroller",
                    "objectset.rio.cattle.io/owner-gvk": "/v1, Kind=Service",
                    "objectset.rio.cattle.io/owner-name": "traefik",
                    "objectset.rio.cattle.io/owner-namespace": "kube-system"
                },
                "ownerReferences": [
                    {
                        "apiVersion": "v1",
                        "kind": "Service",
                        "name": "traefik",
                        "uid": "29bd8573-82e4-4218-a72d-af4a432dc7f6",
                        "controller": true
                    }
                ]
            },
            "spec": {
                "selector": {
                    "matchLabels": {
                        "app": "svclb-traefik"
                    }
                },
                "template": {
                    "metadata": {
                        "creationTimestamp": null,
                        "labels": {
                            "app": "svclb-traefik",
                            "svccontroller.k3s.cattle.io/svcname": "traefik"
                        }
                    },
                    "spec": {
                        "containers": [
                            {
                                "name": "lb-port-80",
                                "image": "rancher/klipper-lb:v0.3.4",
                                "ports": [
                                    {
                                        "name": "lb-port-80",
                                        "hostPort": 80,
                                        "containerPort": 80,
                                        "protocol": "TCP"
                                    }
                                ],
                                "env": [
                                    {
                                        "name": "SRC_PORT",
                                        "value": "80"
                                    },
                                    {
                                        "name": "DEST_PROTO",
                                        "value": "TCP"
                                    },
                                    {
                                        "name": "DEST_PORT",
                                        "value": "80"
                                    },
                                    {
                                        "name": "DEST_IPS",
                                        "value": "10.43.47.227"
                                    }
                                ],
                                "resources": {},
                                "terminationMessagePath": "/dev/termination-log",
                                "terminationMessagePolicy": "File",
                                "imagePullPolicy": "IfNotPresent",
                                "securityContext": {
                                    "capabilities": {
                                        "add": [
                                            "NET_ADMIN"
                                        ]
                                    }
                                }
                            },
                            {
                                "name": "lb-port-443",
                                "image": "rancher/klipper-lb:v0.3.4",
                                "ports": [
                                    {
                                        "name": "lb-port-443",
                                        "hostPort": 443,
                                        "containerPort": 443,
                                        "protocol": "TCP"
                                    }
                                ],
                                "env": [
                                    {
                                        "name": "SRC_PORT",
                                        "value": "443"
                                    },
                                    {
                                        "name": "DEST_PROTO",
                                        "value": "TCP"
                                    },
                                    {
                                        "name": "DEST_PORT",
                                        "value": "443"
                                    },
                                    {
                                        "name": "DEST_IPS",
                                        "value": "10.43.47.227"
                                    }
                                ],
                                "resources": {},
                                "terminationMessagePath": "/dev/termination-log",
                                "terminationMessagePolicy": "File",
                                "imagePullPolicy": "IfNotPresent",
                                "securityContext": {
                                    "capabilities": {
                                        "add": [
                                            "NET_ADMIN"
                                        ]
                                    }
                                }
                            }
                        ],
                        "restartPolicy": "Always",
                        "terminationGracePeriodSeconds": 30,
                        "dnsPolicy": "ClusterFirst",
                        "automountServiceAccountToken": false,
                        "securityContext": {},
                        "schedulerName": "default-scheduler",
                        "tolerations": [
                            {
                                "key": "node-role.kubernetes.io/master",
                                "operator": "Exists",
                                "effect": "NoSchedule"
                            },
                            {
                                "key": "node-role.kubernetes.io/control-plane",
                                "operator": "Exists",
                                "effect": "NoSchedule"
                            },
                            {
                                "key": "CriticalAddonsOnly",
                                "operator": "Exists"
                            }
                        ]
                    }
                },
                "updateStrategy": {
                    "type": "RollingUpdate",
                    "rollingUpdate": {
                        "maxUnavailable": 1,
                        "maxSurge": 0
                    }
                },
                "revisionHistoryLimit": 10
            },
            "status": {
                "currentNumberScheduled": 1,
                "numberMisscheduled": 0,
                "desiredNumberScheduled": 1,
                "numberReady": 1,
                "observedGeneration": 1,
                "updatedNumberScheduled": 1,
                "numberAvailable": 1
            }
        }
    ]
}
{
    "kind": "DeploymentList",
    "apiVersion": "apps/v1",
    "metadata": {
        "resourceVersion": "20940"
    },
    "items": [
        {
            "metadata": {
                "name": "local-path-provisioner",
                "namespace": "kube-system",
                "uid": "1ed50e7c-9565-4cd5-9ea8-6a107d19cd4b",
                "resourceVersion": "18025",
                "generation": 1,
                "creationTimestamp": "2022-03-22T17:40:17Z",
                "labels": {
                    "objectset.rio.cattle.io/hash": "183f35c65ffbc3064603f43f1580d8c68a2dabd4"
                },
                "annotations": {
                    "deployment.kubernetes.io/revision": "1",
                    "objectset.rio.cattle.io/applied": "H4sIAAAAAAAA/6xT0W7qSAz9lZWfkwClRSjSPqC2K612S9FW2pcKrczEKVMm49GMyV6E8u9XJtD26pa2D/cpM7F95tg+Zw8Y7L8Uk2UPJWAIadCOIION9RWUcEPB8a4hL5BBQ4IVCkK5B/SeBcWyT3rl1TMZSSRFtFwYFHFUWB5YBYHsbJz/9xTzp3YDJWzG6U2kHWW//WV99fusqth/CuGxISjBsUGXJ+GIT/SlohTQaOVmu6I87ZJQA10GDlfkPmxtjWkNJYym43p8ZSZXdb0y4+HkcjIc15fjenQ1HVZTM5niRYWr6lJBfyAZUNZ5iNxaHT5F6ONn+KRARtlECs4aTFCOMkjkyAhHDTQoZv33C2sM4fxLXZeBUBMcCh1q3yzWfR3ixMmwF7SeYoLyUa9Ng6qex/OdJsGokspzw762T5DBgMQM+tvxUzwn9rDMgHx7QD7Ob3F/8998dnf7sJhd30IGLbot/RG5UTK1JVf9Q/XLeYGiezr1WLwOueu6ZQa2UamUENGbNcXB+5zLdlgMiwu1xqFgsXVuwc6aHZTwZz1nWURKvU8+W3PLbtvQHW+99BNr9Hjk+XYMr1j9j7yvhG6pxEO0HK3srh2mNO/zesHknivKTbRiDTodN8XWGpoZoy/NP+KXH3Nz7JMhA2FH8eT1xz1sSJu+PsIf/Jnuvdup34JmqiLh9ptNkqDL9kB1TUaghDk/mDVVW6fe7GEOVCM7KlTx0ZNQUnupqCK7PDj09EuRG0xy2MM7kMvTdk5S1rHfYVA1/bzWo3a782vquu57AAAA//9huhZeYwUAAA",
                    "objectset.rio.cattle.io/id": "",
                    "objectset.rio.cattle.io/owner-gvk": "k3s.cattle.io/v1, Kind=Addon",
                    "objectset.rio.cattle.io/owner-name": "local-storage",
                    "objectset.rio.cattle.io/owner-namespace": "kube-system"
                }
            },
            "spec": {
                "replicas": 1,
                "selector": {
                    "matchLabels": {
                        "app": "local-path-provisioner"
                    }
                },
                "template": {
                    "metadata": {
                        "creationTimestamp": null,
                        "labels": {
                            "app": "local-path-provisioner"
                        }
                    },
                    "spec": {
                        "volumes": [
                            {
                                "name": "config-volume",
                                "configMap": {
                                    "name": "local-path-config",
                                    "defaultMode": 420
                                }
                            }
                        ],
                        "containers": [
                            {
                                "name": "local-path-provisioner",
                                "image": "rancher/local-path-provisioner:v0.0.21",
                                "command": [
                                    "local-path-provisioner",
                                    "start",
                                    "--config",
                                    "/etc/config/config.json"
                                ],
                                "env": [
                                    {
                                        "name": "POD_NAMESPACE",
                                        "valueFrom": {
                                            "fieldRef": {
                                                "apiVersion": "v1",
                                                "fieldPath": "metadata.namespace"
                                            }
                                        }
                                    }
                                ],
                                "resources": {},
                                "volumeMounts": [
                                    {
                                        "name": "config-volume",
                                        "mountPath": "/etc/config/"
                                    }
                                ],
                                "terminationMessagePath": "/dev/termination-log",
                                "terminationMessagePolicy": "File",
                                "imagePullPolicy": "IfNotPresent"
                            }
                        ],
                        "restartPolicy": "Always",
                        "terminationGracePeriodSeconds": 30,
                        "dnsPolicy": "ClusterFirst",
                        "serviceAccountName": "local-path-provisioner-service-account",
                        "serviceAccount": "local-path-provisioner-service-account",
                        "securityContext": {},
                        "schedulerName": "default-scheduler",
                        "tolerations": [
                            {
                                "key": "CriticalAddonsOnly",
                                "operator": "Exists"
                            },
                            {
                                "key": "node-role.kubernetes.io/control-plane",
                                "operator": "Exists",
                                "effect": "NoSchedule"
                            },
                            {
                                "key": "node-role.kubernetes.io/master",
                                "operator": "Exists",
                                "effect": "NoSchedule"
                            }
                        ],
                        "priorityClassName": "system-node-critical"
                    }
                },
                "strategy": {
                    "type": "RollingUpdate",
                    "rollingUpdate": {
                        "maxUnavailable": "25%",
                        "maxSurge": "25%"
                    }
                },
                "revisionHistoryLimit": 10,
                "progressDeadlineSeconds": 600
            },
            "status": {
                "observedGeneration": 1,
                "replicas": 1,
                "updatedReplicas": 1,
                "readyReplicas": 1,
                "availableReplicas": 1,
                "conditions": [
                    {
                        "type": "Progressing",
                        "status": "True",
                        "lastUpdateTime": "2022-03-23T15:10:58Z",
                        "lastTransitionTime": "2022-03-23T15:10:58Z",
                        "reason": "NewReplicaSetAvailable",
                        "message": "ReplicaSet \"local-path-provisioner-84bb864455\" has successfully progressed."
                    },
                    {
                        "type": "Available",
                        "status": "True",
                        "lastUpdateTime": "2022-04-08T17:11:12Z",
                        "lastTransitionTime": "2022-04-08T17:11:12Z",
                        "reason": "MinimumReplicasAvailable",
                        "message": "Deployment has minimum availability."
                    }
                ]
            }
        },
        {
            "metadata": {
                "name": "coredns",
                "namespace": "kube-system",
                "uid": "27003c54-ca83-4c0a-af63-b88a2048c5b9",
                "resourceVersion": "18721",
                "generation": 1,
                "creationTimestamp": "2022-03-22T17:40:15Z",
                "labels": {
                    "k8s-app": "kube-dns",
                    "kubernetes.io/name": "CoreDNS",
                    "objectset.rio.cattle.io/hash": "bce283298811743a0386ab510f2f67ef74240c57"
                },
                "annotations": {
                    "deployment.kubernetes.io/revision": "1",
                    "objectset.rio.cattle.io/applied": "H4sIAAAAAAAA/6xV227jNhD9lWKeJduKc/EK6MM2TrtBG9fIZV8WxmJMjWzWFIclKW+MQP9ejGQ7TjaXbdEn0+SZ4ZnDM6MHQKc/kw+aLeSAzoX+OoMEVtoWkMOYnOFNRTZCAhVFLDAi5A+A1nLEqNkG+cvzv0jFQLHnNfcUxmiop7mvJQkkr57zN0s+XaxXkMNqGA5O1lny0+/aFj9/LAq276awWBHkoNhTYcMPwYNDJTGrek5p2IRIFTQJGJyTaYtajUKKzu0gXV5ZekuRgmTbXnvOnsaTmzeuXWJYQg5zRUej4dGH0SjLzo6HOBiOTnF+kg3Ko/L0jMqz46PjgTo5EyLflfQG6eBICeVAhlRkL+sKo1r+8VY1jURGj5EWG4F4NkbbxZ0rMFKX4v7O4hq1wbkhyLMmgbhxQuD6CVb2qXJmF3fglDf1bA64K7YRtSUfIP/yAOgXsoBUsS0hgT5F1d+K0RfBS20IZgnoChfCyKNVS/L9SnsvsHQL3v3mWW/UO4VtwLQ2ZspGqw3kcFlOOE49hc7oRq/JUghTz/O2nhK1qT3dLj2FJZsC8mECyxjdbxTl3GGU1+0vCU1cQgKOfYR8NBgNEghqSe1Lfrq9nYpS2uqo0YzJ4OaGFNsiQH46SMCR11zstzIJrpWiEA5uzhKIuiKu4yPwJbcIhU7JvbDTltXJcI/eIj1HVmwgh7uxMHwnJI3KPQ27PX8x7EN2EFhR9FqFFwJnCXjCQv8nySVy86h4Nsp+VPHvBT/6F3p7Clx7Ra2zja50DJ3zK/ZiqexscKWhBf5dU+hOlavlaDCo2nG6hXZI6QRStddxc8420n1bJhrD36Zer7WhBV0EhaadupCXaAIloNDhXBsddUcFi0K6ZnJx+/WXy8n4683F9efL8wtplMKzkzM0BmZNJ/qf1myumeOv2tB2nOTR19QksGZTV3TFtd36qJLldKv7QTfCgftsqRdpFwmPN+xyvp6jr+oQuTpI1f5P38k4E/MUNuw7eUwl1kaa2HJBNwfjcE4Re0+nNwfIwWhb38tDOa+5Vd9gCJOORSdJqkwdIvlUeR21QgPyVn6tFX1USiqaPO++yIb87vv45QFWJOzOt/HtNy20dSTATpBCEi7utThFhKKyJBUhhwnfqCUVtZHyuzRSWurZ0LN6pP08m9QZtPS/Zq5Q6n855Wxnlf20KfXiCp2IriNVTwTYze1k18D7HaHWgSZc0CeW3HvU45Zc92zWNa/4bzuRHtk8jUv3lmMnD4Vmb/23PNjMmqZp/gkAAP//XaWEZDkJAAA",
                    "objectset.rio.cattle.io/id": "",
                    "objectset.rio.cattle.io/owner-gvk": "k3s.cattle.io/v1, Kind=Addon",
                    "objectset.rio.cattle.io/owner-name": "coredns",
                    "objectset.rio.cattle.io/owner-namespace": "kube-system"
                }
            },
            "spec": {
                "replicas": 1,
                "selector": {
                    "matchLabels": {
                        "k8s-app": "kube-dns"
                    }
                },
                "template": {
                    "metadata": {
                        "creationTimestamp": null,
                        "labels": {
                            "k8s-app": "kube-dns"
                        }
                    },
                    "spec": {
                        "volumes": [
                            {
                                "name": "config-volume",
                                "configMap": {
                                    "name": "coredns",
                                    "items": [
                                        {
                                            "key": "Corefile",
                                            "path": "Corefile"
                                        },
                                        {
                                            "key": "NodeHosts",
                                            "path": "NodeHosts"
                                        }
                                    ],
                                    "defaultMode": 420
                                }
                            },
                            {
                                "name": "custom-config-volume",
                                "configMap": {
                                    "name": "coredns-custom",
                                    "defaultMode": 420,
                                    "optional": true
                                }
                            }
                        ],
                        "containers": [
                            {
                                "name": "coredns",
                                "image": "rancher/mirrored-coredns-coredns:1.8.6",
                                "args": [
                                    "-conf",
                                    "/etc/coredns/Corefile"
                                ],
                                "ports": [
                                    {
                                        "name": "dns",
                                        "containerPort": 53,
                                        "protocol": "UDP"
                                    },
                                    {
                                        "name": "dns-tcp",
                                        "containerPort": 53,
                                        "protocol": "TCP"
                                    },
                                    {
                                        "name": "metrics",
                                        "containerPort": 9153,
                                        "protocol": "TCP"
                                    }
                                ],
                                "resources": {
                                    "limits": {
                                        "memory": "170Mi"
                                    },
                                    "requests": {
                                        "cpu": "100m",
                                        "memory": "70Mi"
                                    }
                                },
                                "volumeMounts": [
                                    {
                                        "name": "config-volume",
                                        "readOnly": true,
                                        "mountPath": "/etc/coredns"
                                    },
                                    {
                                        "name": "custom-config-volume",
                                        "readOnly": true,
                                        "mountPath": "/etc/coredns/custom"
                                    }
                                ],
                                "livenessProbe": {
                                    "httpGet": {
                                        "path": "/health",
                                        "port": 8080,
                                        "scheme": "HTTP"
                                    },
                                    "initialDelaySeconds": 60,
                                    "timeoutSeconds": 1,
                                    "periodSeconds": 10,
                                    "successThreshold": 1,
                                    "failureThreshold": 3
                                },
                                "readinessProbe": {
                                    "httpGet": {
                                        "path": "/ready",
                                        "port": 8181,
                                        "scheme": "HTTP"
                                    },
                                    "timeoutSeconds": 1,
                                    "periodSeconds": 2,
                                    "successThreshold": 1,
                                    "failureThreshold": 3
                                },
                                "terminationMessagePath": "/dev/termination-log",
                                "terminationMessagePolicy": "File",
                                "imagePullPolicy": "IfNotPresent",
                                "securityContext": {
                                    "capabilities": {
                                        "add": [
                                            "NET_BIND_SERVICE"
                                        ],
                                        "drop": [
                                            "all"
                                        ]
                                    },
                                    "readOnlyRootFilesystem": true,
                                    "allowPrivilegeEscalation": false
                                }
                            }
                        ],
                        "restartPolicy": "Always",
                        "terminationGracePeriodSeconds": 30,
                        "dnsPolicy": "Default",
                        "nodeSelector": {
                            "beta.kubernetes.io/os": "linux"
                        },
                        "serviceAccountName": "coredns",
                        "serviceAccount": "coredns",
                        "securityContext": {},
                        "schedulerName": "default-scheduler",
                        "tolerations": [
                            {
                                "key": "CriticalAddonsOnly",
                                "operator": "Exists"
                            },
                            {
                                "key": "node-role.kubernetes.io/control-plane",
                                "operator": "Exists",
                                "effect": "NoSchedule"
                            },
                            {
                                "key": "node-role.kubernetes.io/master",
                                "operator": "Exists",
                                "effect": "NoSchedule"
                            }
                        ],
                        "priorityClassName": "system-cluster-critical"
                    }
                },
                "strategy": {
                    "type": "RollingUpdate",
                    "rollingUpdate": {
                        "maxUnavailable": 1,
                        "maxSurge": "25%"
                    }
                },
                "revisionHistoryLimit": 10,
                "progressDeadlineSeconds": 600
            },
            "status": {
                "observedGeneration": 1,
                "replicas": 1,
                "updatedReplicas": 1,
                "readyReplicas": 1,
                "availableReplicas": 1,
                "conditions": [
                    {
                        "type": "Available",
                        "status": "True",
                        "lastUpdateTime": "2022-03-22T17:40:27Z",
                        "lastTransitionTime": "2022-03-22T17:40:27Z",
                        "reason": "MinimumReplicasAvailable",
                        "message": "Deployment has minimum availability."
                    },
                    {
                        "type": "Progressing",
                        "status": "True",
                        "lastUpdateTime": "2022-03-23T15:11:08Z",
                        "lastTransitionTime": "2022-03-23T15:11:08Z",
                        "reason": "NewReplicaSetAvailable",
                        "message": "ReplicaSet \"coredns-96cc4f57d\" has successfully progressed."
                    }
                ]
            }
        },
        {
            "metadata": {
                "name": "traefik",
                "namespace": "kube-system",
                "uid": "d2031b3d-1a02-48bd-b848-4707d2155f0a",
                "resourceVersion": "18750",
                "generation": 1,
                "creationTimestamp": "2022-03-23T15:12:58Z",
                "labels": {
                    "app.kubernetes.io/instance": "traefik",
                    "app.kubernetes.io/managed-by": "Helm",
                    "app.kubernetes.io/name": "traefik",
                    "helm.sh/chart": "traefik-10.14.100"
                },
                "annotations": {
                    "deployment.kubernetes.io/revision": "1",
                    "meta.helm.sh/release-name": "traefik",
                    "meta.helm.sh/release-namespace": "kube-system"
                }
            },
            "spec": {
                "replicas": 1,
                "selector": {
                    "matchLabels": {
                        "app.kubernetes.io/instance": "traefik",
                        "app.kubernetes.io/name": "traefik"
                    }
                },
                "template": {
                    "metadata": {
                        "creationTimestamp": null,
                        "labels": {
                            "app.kubernetes.io/instance": "traefik",
                            "app.kubernetes.io/managed-by": "Helm",
                            "app.kubernetes.io/name": "traefik",
                            "helm.sh/chart": "traefik-10.14.100"
                        },
                        "annotations": {
                            "prometheus.io/path": "/metrics",
                            "prometheus.io/port": "9100",
                            "prometheus.io/scrape": "true"
                        }
                    },
                    "spec": {
                        "volumes": [
                            {
                                "name": "data",
                                "emptyDir": {}
                            },
                            {
                                "name": "tmp",
                                "emptyDir": {}
                            }
                        ],
                        "containers": [
                            {
                                "name": "traefik",
                                "image": "rancher/mirrored-library-traefik:2.6.1",
                                "args": [
                                    "--global.checknewversion",
                                    "--global.sendanonymoususage",
                                    "--entrypoints.metrics.address=:9100/tcp",
                                    "--entrypoints.traefik.address=:9000/tcp",
                                    "--entrypoints.web.address=:8000/tcp",
                                    "--entrypoints.websecure.address=:8443/tcp",
                                    "--api.dashboard=true",
                                    "--ping=true",
                                    "--metrics.prometheus=true",
                                    "--metrics.prometheus.entrypoint=metrics",
                                    "--providers.kubernetescrd",
                                    "--providers.kubernetesingress",
                                    "--providers.kubernetesingress.ingressendpoint.publishedservice=kube-system/traefik",
                                    "--entrypoints.websecure.http.tls=true"
                                ],
                                "ports": [
                                    {
                                        "name": "metrics",
                                        "containerPort": 9100,
                                        "protocol": "TCP"
                                    },
                                    {
                                        "name": "traefik",
                                        "containerPort": 9000,
                                        "protocol": "TCP"
                                    },
                                    {
                                        "name": "web",
                                        "containerPort": 8000,
                                        "protocol": "TCP"
                                    },
                                    {
                                        "name": "websecure",
                                        "containerPort": 8443,
                                        "protocol": "TCP"
                                    }
                                ],
                                "resources": {},
                                "volumeMounts": [
                                    {
                                        "name": "data",
                                        "mountPath": "/data"
                                    },
                                    {
                                        "name": "tmp",
                                        "mountPath": "/tmp"
                                    }
                                ],
                                "livenessProbe": {
                                    "httpGet": {
                                        "path": "/ping",
                                        "port": 9000,
                                        "scheme": "HTTP"
                                    },
                                    "initialDelaySeconds": 10,
                                    "timeoutSeconds": 2,
                                    "periodSeconds": 10,
                                    "successThreshold": 1,
                                    "failureThreshold": 3
                                },
                                "readinessProbe": {
                                    "httpGet": {
                                        "path": "/ping",
                                        "port": 9000,
                                        "scheme": "HTTP"
                                    },
                                    "initialDelaySeconds": 10,
                                    "timeoutSeconds": 2,
                                    "periodSeconds": 10,
                                    "successThreshold": 1,
                                    "failureThreshold": 1
                                },
                                "terminationMessagePath": "/dev/termination-log",
                                "terminationMessagePolicy": "File",
                                "imagePullPolicy": "IfNotPresent",
                                "securityContext": {
                                    "capabilities": {
                                        "drop": [
                                            "ALL"
                                        ]
                                    },
                                    "runAsUser": 65532,
                                    "runAsGroup": 65532,
                                    "runAsNonRoot": true,
                                    "readOnlyRootFilesystem": true
                                }
                            }
                        ],
                        "restartPolicy": "Always",
                        "terminationGracePeriodSeconds": 60,
                        "dnsPolicy": "ClusterFirst",
                        "serviceAccountName": "traefik",
                        "serviceAccount": "traefik",
                        "securityContext": {
                            "fsGroup": 65532
                        },
                        "schedulerName": "default-scheduler",
                        "tolerations": [
                            {
                                "key": "CriticalAddonsOnly",
                                "operator": "Exists"
                            },
                            {
                                "key": "node-role.kubernetes.io/control-plane",
                                "operator": "Exists",
                                "effect": "NoSchedule"
                            },
                            {
                                "key": "node-role.kubernetes.io/master",
                                "operator": "Exists",
                                "effect": "NoSchedule"
                            }
                        ],
                        "priorityClassName": "system-cluster-critical"
                    }
                },
                "strategy": {
                    "type": "RollingUpdate",
                    "rollingUpdate": {
                        "maxUnavailable": 1,
                        "maxSurge": 1
                    }
                },
                "revisionHistoryLimit": 10,
                "progressDeadlineSeconds": 600
            },
            "status": {
                "observedGeneration": 1,
                "replicas": 1,
                "updatedReplicas": 1,
                "readyReplicas": 1,
                "availableReplicas": 1,
                "conditions": [
                    {
                        "type": "Available",
                        "status": "True",
                        "lastUpdateTime": "2022-03-23T15:12:58Z",
                        "lastTransitionTime": "2022-03-23T15:12:58Z",
                        "reason": "MinimumReplicasAvailable",
                        "message": "Deployment has minimum availability."
                    },
                    {
                        "type": "Progressing",
                        "status": "True",
                        "lastUpdateTime": "2022-03-23T15:14:09Z",
                        "lastTransitionTime": "2022-03-23T15:12:58Z",
                        "reason": "NewReplicaSetAvailable",
                        "message": "ReplicaSet \"traefik-56c4b88c4b\" has successfully progressed."
                    }
                ]
            }
        },
        {
            "metadata": {
                "name": "metrics-server",
                "namespace": "kube-system",
                "uid": "c1a00dcd-f2d3-4cc5-8221-7b42f44ec74d",
                "resourceVersion": "18792",
                "generation": 1,
                "creationTimestamp": "2022-03-22T17:40:19Z",
                "labels": {
                    "k8s-app": "metrics-server",
                    "objectset.rio.cattle.io/hash": "e10e245e13e46a725c9dddd4f9eb239f147774fd"
                },
                "annotations": {
                    "deployment.kubernetes.io/revision": "1",
                    "objectset.rio.cattle.io/applied": "H4sIAAAAAAAA/6xV328bNwz+VwY+3zl24jTrAX4IkmwttmbGku2lyIMs0bFmnaSRPDeecf/7QJ2TuUjTbMXeeCL16ePHH7cDk/3vSOxThAZMzny0mUAFax8dNHCJOaRti1GgghbFOCMGmh2YGJMY8SmyfqbFH2iFUUbk08gakYAjn468gkD1oj99ikj1/WYNDaxP+MCzmVTf/eSjm507l+KrENG0CI1SJG+5ZqQNUu0O6b8OwNlYRVl3C6x5y4It9BUEs8BQ0lx/z7XJ+dlDX0FfGV5BAzgZ4/H0FCcnOH1jzo5P7VvnnJsu3+Li+OTtcjI9OzubLp2+98VcYDh/gSJntEqQMaCVRGq3Ruzq59e5930Fgm0ORrDcOyjzv0j9RcIHtGyKYnxEYmg+7sDQvRpQ1xZJaudpdiRthgrqmtF2hHVOJLPpdHpSDjXbgFJnwiUSoauNc4TMtWwz8ux9FKRowvt5dfXwZL5LLIXaIUTHWMfksGYx0nF5qAQM7GtCTqHTxp5NThnuKvCtudf0yES7QjpqPVFSDp/n22zGo9PRMVQQ/AYjMs8pLYqiS+NDR3i7IuRVCg6akwpWIvlHFPVnI9ojR3rxL6igcGpKBEMFbFdYBH53ezu/UV199OJNuMRgtjdoU3QMzZtxBRnJJ/d0NBlXwJ21yHzw+KQC8S2mTv4J/ErbKZuhbk9lnBeCWp6na49kMyVJNgVo4PZiDv1dBYTG+W8SRG9uv12R54Ic/wc9tBM6sljan/DPDlmKbXMHDUzG47YsxTbRFho4G3/wZZZKB3vZXqQo+FDyMSGkT3PyGx/wHq/YmlB2JzRLExgHiX6JYftrSvKDD7if7EaoU28Xz/k6RfV+dvYbI2mVx+O+gk0KXYsfUhf35WrVnO+lHOZrXyxps04d9Hdan0w+FcLBMF8PEQOBYVIsefHWBBUeaeMtnlur2NcvdIykgPT4c/i4gzWqQBd7mLLQWbPVtZk1UhcWXD14FbivdoDLJVot+HW6sSt0XdAhHmAKJUoBRzrSFFGQddFqc1IKdQ4m4v+K3BqWYcc/h7x71H3IFNss20uv+7f/ktp93/8dAAD//zbPSKBzBwAA",
                    "objectset.rio.cattle.io/id": "",
                    "objectset.rio.cattle.io/owner-gvk": "k3s.cattle.io/v1, Kind=Addon",
                    "objectset.rio.cattle.io/owner-name": "metrics-server-deployment",
                    "objectset.rio.cattle.io/owner-namespace": "kube-system"
                }
            },
            "spec": {
                "replicas": 1,
                "selector": {
                    "matchLabels": {
                        "k8s-app": "metrics-server"
                    }
                },
                "template": {
                    "metadata": {
                        "name": "metrics-server",
                        "creationTimestamp": null,
                        "labels": {
                            "k8s-app": "metrics-server"
                        }
                    },
                    "spec": {
                        "volumes": [
                            {
                                "name": "tmp-dir",
                                "emptyDir": {}
                            }
                        ],
                        "containers": [
                            {
                                "name": "metrics-server",
                                "image": "rancher/mirrored-metrics-server:v0.5.2",
                                "args": [
                                    "--cert-dir=/tmp",
                                    "--secure-port=4443",
                                    "--kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname",
                                    "--kubelet-use-node-status-port",
                                    "--metric-resolution=15s"
                                ],
                                "ports": [
                                    {
                                        "name": "https",
                                        "containerPort": 4443,
                                        "protocol": "TCP"
                                    }
                                ],
                                "resources": {
                                    "requests": {
                                        "cpu": "100m",
                                        "memory": "70Mi"
                                    }
                                },
                                "volumeMounts": [
                                    {
                                        "name": "tmp-dir",
                                        "mountPath": "/tmp"
                                    }
                                ],
                                "livenessProbe": {
                                    "httpGet": {
                                        "path": "/livez",
                                        "port": "https",
                                        "scheme": "HTTPS"
                                    },
                                    "initialDelaySeconds": 60,
                                    "timeoutSeconds": 1,
                                    "periodSeconds": 10,
                                    "successThreshold": 1,
                                    "failureThreshold": 3
                                },
                                "readinessProbe": {
                                    "httpGet": {
                                        "path": "/readyz",
                                        "port": "https",
                                        "scheme": "HTTPS"
                                    },
                                    "timeoutSeconds": 1,
                                    "periodSeconds": 2,
                                    "successThreshold": 1,
                                    "failureThreshold": 3
                                },
                                "terminationMessagePath": "/dev/termination-log",
                                "terminationMessagePolicy": "File",
                                "imagePullPolicy": "IfNotPresent",
                                "securityContext": {
                                    "runAsUser": 1000,
                                    "runAsNonRoot": true,
                                    "readOnlyRootFilesystem": true,
                                    "allowPrivilegeEscalation": false
                                }
                            }
                        ],
                        "restartPolicy": "Always",
                        "terminationGracePeriodSeconds": 30,
                        "dnsPolicy": "ClusterFirst",
                        "serviceAccountName": "metrics-server",
                        "serviceAccount": "metrics-server",
                        "securityContext": {},
                        "schedulerName": "default-scheduler",
                        "tolerations": [
                            {
                                "key": "CriticalAddonsOnly",
                                "operator": "Exists"
                            },
                            {
                                "key": "node-role.kubernetes.io/control-plane",
                                "operator": "Exists",
                                "effect": "NoSchedule"
                            },
                            {
                                "key": "node-role.kubernetes.io/master",
                                "operator": "Exists",
                                "effect": "NoSchedule"
                            }
                        ],
                        "priorityClassName": "system-node-critical"
                    }
                },
                "strategy": {
                    "type": "RollingUpdate",
                    "rollingUpdate": {
                        "maxUnavailable": "25%",
                        "maxSurge": "25%"
                    }
                },
                "revisionHistoryLimit": 10,
                "progressDeadlineSeconds": 600
            },
            "status": {
                "observedGeneration": 1,
                "replicas": 1,
                "updatedReplicas": 1,
                "readyReplicas": 1,
                "availableReplicas": 1,
                "conditions": [
                    {
                        "type": "Progressing",
                        "status": "True",
                        "lastUpdateTime": "2022-03-23T15:11:50Z",
                        "lastTransitionTime": "2022-03-23T15:11:50Z",
                        "reason": "NewReplicaSetAvailable",
                        "message": "ReplicaSet \"metrics-server-ff9dbcb6c\" has successfully progressed."
                    },
                    {
                        "type": "Available",
                        "status": "True",
                        "lastUpdateTime": "2022-04-10T08:08:57Z",
                        "lastTransitionTime": "2022-04-10T08:08:57Z",
                        "reason": "MinimumReplicasAvailable",
                        "message": "Deployment has minimum availability."
                    }
                ]
            }
        }
    ]
}
{
    "kind": "ReplicaSetList",
    "apiVersion": "apps/v1",
    "metadata": {
        "resourceVersion": "20940"
    },
    "items": [
        {
            "metadata": {
                "name": "local-path-provisioner-84bb864455",
                "namespace": "kube-system",
                "uid": "c5928506-bda0-4403-9564-3052e8c9f325",
                "resourceVersion": "18023",
                "generation": 1,
                "creationTimestamp": "2022-03-22T17:40:26Z",
                "labels": {
                    "app": "local-path-provisioner",
                    "pod-template-hash": "84bb864455"
                },
                "annotations": {
                    "deployment.kubernetes.io/desired-replicas": "1",
                    "deployment.kubernetes.io/max-replicas": "2",
                    "deployment.kubernetes.io/revision": "1",
                    "objectset.rio.cattle.io/applied": "H4sIAAAAAAAA/6xT0W7qSAz9lZWfkwClRSjSPqC2K612S9FW2pcKrczEKVMm49GMyV6E8u9XJtD26pa2D/cpM7F95tg+Zw8Y7L8Uk2UPJWAIadCOIION9RWUcEPB8a4hL5BBQ4IVCkK5B/SeBcWyT3rl1TMZSSRFtFwYFHFUWB5YBYHsbJz/9xTzp3YDJWzG6U2kHWW//WV99fusqth/CuGxISjBsUGXJ+GIT/SlohTQaOVmu6I87ZJQA10GDlfkPmxtjWkNJYym43p8ZSZXdb0y4+HkcjIc15fjenQ1HVZTM5niRYWr6lJBfyAZUNZ5iNxaHT5F6ONn+KRARtlECs4aTFCOMkjkyAhHDTQoZv33C2sM4fxLXZeBUBMcCh1q3yzWfR3ixMmwF7SeYoLyUa9Ng6qex/OdJsGokspzw762T5DBgMQM+tvxUzwn9rDMgHx7QD7Ob3F/8998dnf7sJhd30IGLbot/RG5UTK1JVf9Q/XLeYGiezr1WLwOueu6ZQa2UamUENGbNcXB+5zLdlgMiwu1xqFgsXVuwc6aHZTwZz1nWURKvU8+W3PLbtvQHW+99BNr9Hjk+XYMr1j9j7yvhG6pxEO0HK3srh2mNO/zesHknivKTbRiDTodN8XWGpoZoy/NP+KXH3Nz7JMhA2FH8eT1xz1sSJu+PsIf/Jnuvdup34JmqiLh9ptNkqDL9kB1TUaghDk/mDVVW6fe7GEOVCM7KlTx0ZNQUnupqCK7PDj09EuRG0xy2MM7kMvTdk5S1rHfYVA1/bzWo3a782vquu57AAAA//9huhZeYwUAAA",
                    "objectset.rio.cattle.io/id": "",
                    "objectset.rio.cattle.io/owner-gvk": "k3s.cattle.io/v1, Kind=Addon",
                    "objectset.rio.cattle.io/owner-name": "local-storage",
                    "objectset.rio.cattle.io/owner-namespace": "kube-system"
                },
                "ownerReferences": [
                    {
                        "apiVersion": "apps/v1",
                        "kind": "Deployment",
                        "name": "local-path-provisioner",
                        "uid": "1ed50e7c-9565-4cd5-9ea8-6a107d19cd4b",
                        "controller": true,
                        "blockOwnerDeletion": true
                    }
                ]
            },
            "spec": {
                "replicas": 1,
                "selector": {
                    "matchLabels": {
                        "app": "local-path-provisioner",
                        "pod-template-hash": "84bb864455"
                    }
                },
                "template": {
                    "metadata": {
                        "creationTimestamp": null,
                        "labels": {
                            "app": "local-path-provisioner",
                            "pod-template-hash": "84bb864455"
                        }
                    },
                    "spec": {
                        "volumes": [
                            {
                                "name": "config-volume",
                                "configMap": {
                                    "name": "local-path-config",
                                    "defaultMode": 420
                                }
                            }
                        ],
                        "containers": [
                            {
                                "name": "local-path-provisioner",
                                "image": "rancher/local-path-provisioner:v0.0.21",
                                "command": [
                                    "local-path-provisioner",
                                    "start",
                                    "--config",
                                    "/etc/config/config.json"
                                ],
                                "env": [
                                    {
                                        "name": "POD_NAMESPACE",
                                        "valueFrom": {
                                            "fieldRef": {
                                                "apiVersion": "v1",
                                                "fieldPath": "metadata.namespace"
                                            }
                                        }
                                    }
                                ],
                                "resources": {},
                                "volumeMounts": [
                                    {
                                        "name": "config-volume",
                                        "mountPath": "/etc/config/"
                                    }
                                ],
                                "terminationMessagePath": "/dev/termination-log",
                                "terminationMessagePolicy": "File",
                                "imagePullPolicy": "IfNotPresent"
                            }
                        ],
                        "restartPolicy": "Always",
                        "terminationGracePeriodSeconds": 30,
                        "dnsPolicy": "ClusterFirst",
                        "serviceAccountName": "local-path-provisioner-service-account",
                        "serviceAccount": "local-path-provisioner-service-account",
                        "securityContext": {},
                        "schedulerName": "default-scheduler",
                        "tolerations": [
                            {
                                "key": "CriticalAddonsOnly",
                                "operator": "Exists"
                            },
                            {
                                "key": "node-role.kubernetes.io/control-plane",
                                "operator": "Exists",
                                "effect": "NoSchedule"
                            },
                            {
                                "key": "node-role.kubernetes.io/master",
                                "operator": "Exists",
                                "effect": "NoSchedule"
                            }
                        ],
                        "priorityClassName": "system-node-critical"
                    }
                }
            },
            "status": {
                "replicas": 1,
                "fullyLabeledReplicas": 1,
                "readyReplicas": 1,
                "availableReplicas": 1,
                "observedGeneration": 1
            }
        },
        {
            "metadata": {
                "name": "coredns-96cc4f57d",
                "namespace": "kube-system",
                "uid": "d71a1e43-4705-47a5-b078-d9af12f93f4b",
                "resourceVersion": "18717",
                "generation": 1,
                "creationTimestamp": "2022-03-22T17:40:26Z",
                "labels": {
                    "k8s-app": "kube-dns",
                    "pod-template-hash": "96cc4f57d"
                },
                "annotations": {
                    "deployment.kubernetes.io/desired-replicas": "1",
                    "deployment.kubernetes.io/max-replicas": "2",
                    "deployment.kubernetes.io/revision": "1",
                    "objectset.rio.cattle.io/applied": "H4sIAAAAAAAA/6xV227jNhD9lWKeJduKc/EK6MM2TrtBG9fIZV8WxmJMjWzWFIclKW+MQP9ejGQ7TjaXbdEn0+SZ4ZnDM6MHQKc/kw+aLeSAzoX+OoMEVtoWkMOYnOFNRTZCAhVFLDAi5A+A1nLEqNkG+cvzv0jFQLHnNfcUxmiop7mvJQkkr57zN0s+XaxXkMNqGA5O1lny0+/aFj9/LAq276awWBHkoNhTYcMPwYNDJTGrek5p2IRIFTQJGJyTaYtajUKKzu0gXV5ZekuRgmTbXnvOnsaTmzeuXWJYQg5zRUej4dGH0SjLzo6HOBiOTnF+kg3Ko/L0jMqz46PjgTo5EyLflfQG6eBICeVAhlRkL+sKo1r+8VY1jURGj5EWG4F4NkbbxZ0rMFKX4v7O4hq1wbkhyLMmgbhxQuD6CVb2qXJmF3fglDf1bA64K7YRtSUfIP/yAOgXsoBUsS0hgT5F1d+K0RfBS20IZgnoChfCyKNVS/L9SnsvsHQL3v3mWW/UO4VtwLQ2ZspGqw3kcFlOOE49hc7oRq/JUghTz/O2nhK1qT3dLj2FJZsC8mECyxjdbxTl3GGU1+0vCU1cQgKOfYR8NBgNEghqSe1Lfrq9nYpS2uqo0YzJ4OaGFNsiQH46SMCR11zstzIJrpWiEA5uzhKIuiKu4yPwJbcIhU7JvbDTltXJcI/eIj1HVmwgh7uxMHwnJI3KPQ27PX8x7EN2EFhR9FqFFwJnCXjCQv8nySVy86h4Nsp+VPHvBT/6F3p7Clx7Ra2zja50DJ3zK/ZiqexscKWhBf5dU+hOlavlaDCo2nG6hXZI6QRStddxc8420n1bJhrD36Zer7WhBV0EhaadupCXaAIloNDhXBsddUcFi0K6ZnJx+/WXy8n4683F9efL8wtplMKzkzM0BmZNJ/qf1myumeOv2tB2nOTR19QksGZTV3TFtd36qJLldKv7QTfCgftsqRdpFwmPN+xyvp6jr+oQuTpI1f5P38k4E/MUNuw7eUwl1kaa2HJBNwfjcE4Re0+nNwfIwWhb38tDOa+5Vd9gCJOORSdJqkwdIvlUeR21QgPyVn6tFX1USiqaPO++yIb87vv45QFWJOzOt/HtNy20dSTATpBCEi7utThFhKKyJBUhhwnfqCUVtZHyuzRSWurZ0LN6pP08m9QZtPS/Zq5Q6n855Wxnlf20KfXiCp2IriNVTwTYze1k18D7HaHWgSZc0CeW3HvU45Zc92zWNa/4bzuRHtk8jUv3lmMnD4Vmb/23PNjMmqZp/gkAAP//XaWEZDkJAAA",
                    "objectset.rio.cattle.io/id": "",
                    "objectset.rio.cattle.io/owner-gvk": "k3s.cattle.io/v1, Kind=Addon",
                    "objectset.rio.cattle.io/owner-name": "coredns",
                    "objectset.rio.cattle.io/owner-namespace": "kube-system"
                },
                "ownerReferences": [
                    {
                        "apiVersion": "apps/v1",
                        "kind": "Deployment",
                        "name": "coredns",
                        "uid": "27003c54-ca83-4c0a-af63-b88a2048c5b9",
                        "controller": true,
                        "blockOwnerDeletion": true
                    }
                ]
            },
            "spec": {
                "replicas": 1,
                "selector": {
                    "matchLabels": {
                        "k8s-app": "kube-dns",
                        "pod-template-hash": "96cc4f57d"
                    }
                },
                "template": {
                    "metadata": {
                        "creationTimestamp": null,
                        "labels": {
                            "k8s-app": "kube-dns",
                            "pod-template-hash": "96cc4f57d"
                        }
                    },
                    "spec": {
                        "volumes": [
                            {
                                "name": "config-volume",
                                "configMap": {
                                    "name": "coredns",
                                    "items": [
                                        {
                                            "key": "Corefile",
                                            "path": "Corefile"
                                        },
                                        {
                                            "key": "NodeHosts",
                                            "path": "NodeHosts"
                                        }
                                    ],
                                    "defaultMode": 420
                                }
                            },
                            {
                                "name": "custom-config-volume",
                                "configMap": {
                                    "name": "coredns-custom",
                                    "defaultMode": 420,
                                    "optional": true
                                }
                            }
                        ],
                        "containers": [
                            {
                                "name": "coredns",
                                "image": "rancher/mirrored-coredns-coredns:1.8.6",
                                "args": [
                                    "-conf",
                                    "/etc/coredns/Corefile"
                                ],
                                "ports": [
                                    {
                                        "name": "dns",
                                        "containerPort": 53,
                                        "protocol": "UDP"
                                    },
                                    {
                                        "name": "dns-tcp",
                                        "containerPort": 53,
                                        "protocol": "TCP"
                                    },
                                    {
                                        "name": "metrics",
                                        "containerPort": 9153,
                                        "protocol": "TCP"
                                    }
                                ],
                                "resources": {
                                    "limits": {
                                        "memory": "170Mi"
                                    },
                                    "requests": {
                                        "cpu": "100m",
                                        "memory": "70Mi"
                                    }
                                },
                                "volumeMounts": [
                                    {
                                        "name": "config-volume",
                                        "readOnly": true,
                                        "mountPath": "/etc/coredns"
                                    },
                                    {
                                        "name": "custom-config-volume",
                                        "readOnly": true,
                                        "mountPath": "/etc/coredns/custom"
                                    }
                                ],
                                "livenessProbe": {
                                    "httpGet": {
                                        "path": "/health",
                                        "port": 8080,
                                        "scheme": "HTTP"
                                    },
                                    "initialDelaySeconds": 60,
                                    "timeoutSeconds": 1,
                                    "periodSeconds": 10,
                                    "successThreshold": 1,
                                    "failureThreshold": 3
                                },
                                "readinessProbe": {
                                    "httpGet": {
                                        "path": "/ready",
                                        "port": 8181,
                                        "scheme": "HTTP"
                                    },
                                    "timeoutSeconds": 1,
                                    "periodSeconds": 2,
                                    "successThreshold": 1,
                                    "failureThreshold": 3
                                },
                                "terminationMessagePath": "/dev/termination-log",
                                "terminationMessagePolicy": "File",
                                "imagePullPolicy": "IfNotPresent",
                                "securityContext": {
                                    "capabilities": {
                                        "add": [
                                            "NET_BIND_SERVICE"
                                        ],
                                        "drop": [
                                            "all"
                                        ]
                                    },
                                    "readOnlyRootFilesystem": true,
                                    "allowPrivilegeEscalation": false
                                }
                            }
                        ],
                        "restartPolicy": "Always",
                        "terminationGracePeriodSeconds": 30,
                        "dnsPolicy": "Default",
                        "nodeSelector": {
                            "beta.kubernetes.io/os": "linux"
                        },
                        "serviceAccountName": "coredns",
                        "serviceAccount": "coredns",
                        "securityContext": {},
                        "schedulerName": "default-scheduler",
                        "tolerations": [
                            {
                                "key": "CriticalAddonsOnly",
                                "operator": "Exists"
                            },
                            {
                                "key": "node-role.kubernetes.io/control-plane",
                                "operator": "Exists",
                                "effect": "NoSchedule"
                            },
                            {
                                "key": "node-role.kubernetes.io/master",
                                "operator": "Exists",
                                "effect": "NoSchedule"
                            }
                        ],
                        "priorityClassName": "system-cluster-critical"
                    }
                }
            },
            "status": {
                "replicas": 1,
                "fullyLabeledReplicas": 1,
                "readyReplicas": 1,
                "availableReplicas": 1,
                "observedGeneration": 1
            }
        },
        {
            "metadata": {
                "name": "traefik-56c4b88c4b",
                "namespace": "kube-system",
                "uid": "0764b677-1c27-42fa-aa98-4ec8635eed04",
                "resourceVersion": "18748",
                "generation": 1,
                "creationTimestamp": "2022-03-23T15:12:58Z",
                "labels": {
                    "app.kubernetes.io/instance": "traefik",
                    "app.kubernetes.io/managed-by": "Helm",
                    "app.kubernetes.io/name": "traefik",
                    "helm.sh/chart": "traefik-10.14.100",
                    "pod-template-hash": "56c4b88c4b"
                },
                "annotations": {
                    "deployment.kubernetes.io/desired-replicas": "1",
                    "deployment.kubernetes.io/max-replicas": "2",
                    "deployment.kubernetes.io/revision": "1",
                    "meta.helm.sh/release-name": "traefik",
                    "meta.helm.sh/release-namespace": "kube-system"
                },
                "ownerReferences": [
                    {
                        "apiVersion": "apps/v1",
                        "kind": "Deployment",
                        "name": "traefik",
                        "uid": "d2031b3d-1a02-48bd-b848-4707d2155f0a",
                        "controller": true,
                        "blockOwnerDeletion": true
                    }
                ]
            },
            "spec": {
                "replicas": 1,
                "selector": {
                    "matchLabels": {
                        "app.kubernetes.io/instance": "traefik",
                        "app.kubernetes.io/name": "traefik",
                        "pod-template-hash": "56c4b88c4b"
                    }
                },
                "template": {
                    "metadata": {
                        "creationTimestamp": null,
                        "labels": {
                            "app.kubernetes.io/instance": "traefik",
                            "app.kubernetes.io/managed-by": "Helm",
                            "app.kubernetes.io/name": "traefik",
                            "helm.sh/chart": "traefik-10.14.100",
                            "pod-template-hash": "56c4b88c4b"
                        },
                        "annotations": {
                            "prometheus.io/path": "/metrics",
                            "prometheus.io/port": "9100",
                            "prometheus.io/scrape": "true"
                        }
                    },
                    "spec": {
                        "volumes": [
                            {
                                "name": "data",
                                "emptyDir": {}
                            },
                            {
                                "name": "tmp",
                                "emptyDir": {}
                            }
                        ],
                        "containers": [
                            {
                                "name": "traefik",
                                "image": "rancher/mirrored-library-traefik:2.6.1",
                                "args": [
                                    "--global.checknewversion",
                                    "--global.sendanonymoususage",
                                    "--entrypoints.metrics.address=:9100/tcp",
                                    "--entrypoints.traefik.address=:9000/tcp",
                                    "--entrypoints.web.address=:8000/tcp",
                                    "--entrypoints.websecure.address=:8443/tcp",
                                    "--api.dashboard=true",
                                    "--ping=true",
                                    "--metrics.prometheus=true",
                                    "--metrics.prometheus.entrypoint=metrics",
                                    "--providers.kubernetescrd",
                                    "--providers.kubernetesingress",
                                    "--providers.kubernetesingress.ingressendpoint.publishedservice=kube-system/traefik",
                                    "--entrypoints.websecure.http.tls=true"
                                ],
                                "ports": [
                                    {
                                        "name": "metrics",
                                        "containerPort": 9100,
                                        "protocol": "TCP"
                                    },
                                    {
                                        "name": "traefik",
                                        "containerPort": 9000,
                                        "protocol": "TCP"
                                    },
                                    {
                                        "name": "web",
                                        "containerPort": 8000,
                                        "protocol": "TCP"
                                    },
                                    {
                                        "name": "websecure",
                                        "containerPort": 8443,
                                        "protocol": "TCP"
                                    }
                                ],
                                "resources": {},
                                "volumeMounts": [
                                    {
                                        "name": "data",
                                        "mountPath": "/data"
                                    },
                                    {
                                        "name": "tmp",
                                        "mountPath": "/tmp"
                                    }
                                ],
                                "livenessProbe": {
                                    "httpGet": {
                                        "path": "/ping",
                                        "port": 9000,
                                        "scheme": "HTTP"
                                    },
                                    "initialDelaySeconds": 10,
                                    "timeoutSeconds": 2,
                                    "periodSeconds": 10,
                                    "successThreshold": 1,
                                    "failureThreshold": 3
                                },
                                "readinessProbe": {
                                    "httpGet": {
                                        "path": "/ping",
                                        "port": 9000,
                                        "scheme": "HTTP"
                                    },
                                    "initialDelaySeconds": 10,
                                    "timeoutSeconds": 2,
                                    "periodSeconds": 10,
                                    "successThreshold": 1,
                                    "failureThreshold": 1
                                },
                                "terminationMessagePath": "/dev/termination-log",
                                "terminationMessagePolicy": "File",
                                "imagePullPolicy": "IfNotPresent",
                                "securityContext": {
                                    "capabilities": {
                                        "drop": [
                                            "ALL"
                                        ]
                                    },
                                    "runAsUser": 65532,
                                    "runAsGroup": 65532,
                                    "runAsNonRoot": true,
                                    "readOnlyRootFilesystem": true
                                }
                            }
                        ],
                        "restartPolicy": "Always",
                        "terminationGracePeriodSeconds": 60,
                        "dnsPolicy": "ClusterFirst",
                        "serviceAccountName": "traefik",
                        "serviceAccount": "traefik",
                        "securityContext": {
                            "fsGroup": 65532
                        },
                        "schedulerName": "default-scheduler",
                        "tolerations": [
                            {
                                "key": "CriticalAddonsOnly",
                                "operator": "Exists"
                            },
                            {
                                "key": "node-role.kubernetes.io/control-plane",
                                "operator": "Exists",
                                "effect": "NoSchedule"
                            },
                            {
                                "key": "node-role.kubernetes.io/master",
                                "operator": "Exists",
                                "effect": "NoSchedule"
                            }
                        ],
                        "priorityClassName": "system-cluster-critical"
                    }
                }
            },
            "status": {
                "replicas": 1,
                "fullyLabeledReplicas": 1,
                "readyReplicas": 1,
                "availableReplicas": 1,
                "observedGeneration": 1
            }
        },
        {
            "metadata": {
                "name": "metrics-server-ff9dbcb6c",
                "namespace": "kube-system",
                "uid": "9fcd8f0b-ec0c-4461-827f-ebe18dca023d",
                "resourceVersion": "18789",
                "generation": 1,
                "creationTimestamp": "2022-03-22T17:40:26Z",
                "labels": {
                    "k8s-app": "metrics-server",
                    "pod-template-hash": "ff9dbcb6c"
                },
                "annotations": {
                    "deployment.kubernetes.io/desired-replicas": "1",
                    "deployment.kubernetes.io/max-replicas": "2",
                    "deployment.kubernetes.io/revision": "1",
                    "objectset.rio.cattle.io/applied": "H4sIAAAAAAAA/6xV328bNwz+VwY+3zl24jTrAX4IkmwttmbGku2lyIMs0bFmnaSRPDeecf/7QJ2TuUjTbMXeeCL16ePHH7cDk/3vSOxThAZMzny0mUAFax8dNHCJOaRti1GgghbFOCMGmh2YGJMY8SmyfqbFH2iFUUbk08gakYAjn468gkD1oj99ikj1/WYNDaxP+MCzmVTf/eSjm507l+KrENG0CI1SJG+5ZqQNUu0O6b8OwNlYRVl3C6x5y4It9BUEs8BQ0lx/z7XJ+dlDX0FfGV5BAzgZ4/H0FCcnOH1jzo5P7VvnnJsu3+Li+OTtcjI9OzubLp2+98VcYDh/gSJntEqQMaCVRGq3Ruzq59e5930Fgm0ORrDcOyjzv0j9RcIHtGyKYnxEYmg+7sDQvRpQ1xZJaudpdiRthgrqmtF2hHVOJLPpdHpSDjXbgFJnwiUSoauNc4TMtWwz8ux9FKRowvt5dfXwZL5LLIXaIUTHWMfksGYx0nF5qAQM7GtCTqHTxp5NThnuKvCtudf0yES7QjpqPVFSDp/n22zGo9PRMVQQ/AYjMs8pLYqiS+NDR3i7IuRVCg6akwpWIvlHFPVnI9ojR3rxL6igcGpKBEMFbFdYBH53ezu/UV199OJNuMRgtjdoU3QMzZtxBRnJJ/d0NBlXwJ21yHzw+KQC8S2mTv4J/ErbKZuhbk9lnBeCWp6na49kMyVJNgVo4PZiDv1dBYTG+W8SRG9uv12R54Ic/wc9tBM6sljan/DPDlmKbXMHDUzG47YsxTbRFho4G3/wZZZKB3vZXqQo+FDyMSGkT3PyGx/wHq/YmlB2JzRLExgHiX6JYftrSvKDD7if7EaoU28Xz/k6RfV+dvYbI2mVx+O+gk0KXYsfUhf35WrVnO+lHOZrXyxps04d9Hdan0w+FcLBMF8PEQOBYVIsefHWBBUeaeMtnlur2NcvdIykgPT4c/i4gzWqQBd7mLLQWbPVtZk1UhcWXD14FbivdoDLJVot+HW6sSt0XdAhHmAKJUoBRzrSFFGQddFqc1IKdQ4m4v+K3BqWYcc/h7x71H3IFNss20uv+7f/ktp93/8dAAD//zbPSKBzBwAA",
                    "objectset.rio.cattle.io/id": "",
                    "objectset.rio.cattle.io/owner-gvk": "k3s.cattle.io/v1, Kind=Addon",
                    "objectset.rio.cattle.io/owner-name": "metrics-server-deployment",
                    "objectset.rio.cattle.io/owner-namespace": "kube-system"
                },
                "ownerReferences": [
                    {
                        "apiVersion": "apps/v1",
                        "kind": "Deployment",
                        "name": "metrics-server",
                        "uid": "c1a00dcd-f2d3-4cc5-8221-7b42f44ec74d",
                        "controller": true,
                        "blockOwnerDeletion": true
                    }
                ]
            },
            "spec": {
                "replicas": 1,
                "selector": {
                    "matchLabels": {
                        "k8s-app": "metrics-server",
                        "pod-template-hash": "ff9dbcb6c"
                    }
                },
                "template": {
                    "metadata": {
                        "name": "metrics-server",
                        "creationTimestamp": null,
                        "labels": {
                            "k8s-app": "metrics-server",
                            "pod-template-hash": "ff9dbcb6c"
                        }
                    },
                    "spec": {
                        "volumes": [
                            {
                                "name": "tmp-dir",
                                "emptyDir": {}
                            }
                        ],
                        "containers": [
                            {
                                "name": "metrics-server",
                                "image": "rancher/mirrored-metrics-server:v0.5.2",
                                "args": [
                                    "--cert-dir=/tmp",
                                    "--secure-port=4443",
                                    "--kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname",
                                    "--kubelet-use-node-status-port",
                                    "--metric-resolution=15s"
                                ],
                                "ports": [
                                    {
                                        "name": "https",
                                        "containerPort": 4443,
                                        "protocol": "TCP"
                                    }
                                ],
                                "resources": {
                                    "requests": {
                                        "cpu": "100m",
                                        "memory": "70Mi"
                                    }
                                },
                                "volumeMounts": [
                                    {
                                        "name": "tmp-dir",
                                        "mountPath": "/tmp"
                                    }
                                ],
                                "livenessProbe": {
                                    "httpGet": {
                                        "path": "/livez",
                                        "port": "https",
                                        "scheme": "HTTPS"
                                    },
                                    "initialDelaySeconds": 60,
                                    "timeoutSeconds": 1,
                                    "periodSeconds": 10,
                                    "successThreshold": 1,
                                    "failureThreshold": 3
                                },
                                "readinessProbe": {
                                    "httpGet": {
                                        "path": "/readyz",
                                        "port": "https",
                                        "scheme": "HTTPS"
                                    },
                                    "timeoutSeconds": 1,
                                    "periodSeconds": 2,
                                    "successThreshold": 1,
                                    "failureThreshold": 3
                                },
                                "terminationMessagePath": "/dev/termination-log",
                                "terminationMessagePolicy": "File",
                                "imagePullPolicy": "IfNotPresent",
                                "securityContext": {
                                    "runAsUser": 1000,
                                    "runAsNonRoot": true,
                                    "readOnlyRootFilesystem": true,
                                    "allowPrivilegeEscalation": false
                                }
                            }
                        ],
                        "restartPolicy": "Always",
                        "terminationGracePeriodSeconds": 30,
                        "dnsPolicy": "ClusterFirst",
                        "serviceAccountName": "metrics-server",
                        "serviceAccount": "metrics-server",
                        "securityContext": {},
                        "schedulerName": "default-scheduler",
                        "tolerations": [
                            {
                                "key": "CriticalAddonsOnly",
                                "operator": "Exists"
                            },
                            {
                                "key": "node-role.kubernetes.io/control-plane",
                                "operator": "Exists",
                                "effect": "NoSchedule"
                            },
                            {
                                "key": "node-role.kubernetes.io/master",
                                "operator": "Exists",
                                "effect": "NoSchedule"
                            }
                        ],
                        "priorityClassName": "system-node-critical"
                    }
                }
            },
            "status": {
                "replicas": 1,
                "fullyLabeledReplicas": 1,
                "readyReplicas": 1,
                "availableReplicas": 1,
                "observedGeneration": 1
            }
        }
    ]
}
{
    "kind": "PodList",
    "apiVersion": "v1",
    "metadata": {
        "resourceVersion": "20940"
    },
    "items": [
        {
            "metadata": {
                "name": "helm-install-traefik-crd--1-fgrt5",
                "generateName": "helm-install-traefik-crd--1-",
                "namespace": "kube-system",
                "uid": "10dc1de1-015e-4461-83d9-6d8a9bad14d3",
                "resourceVersion": "743",
                "creationTimestamp": "2022-03-22T17:40:26Z",
                "labels": {
                    "controller-uid": "258893ed-a29a-44f2-b2b6-ac1732cdd902",
                    "helmcharts.helm.cattle.io/chart": "traefik-crd",
                    "job-name": "helm-install-traefik-crd"
                },
                "annotations": {
                    "helmcharts.helm.cattle.io/configHash": "SHA256=E3B0C44298FC1C149AFBF4C8996FB92427AE41E4649B934CA495991B7852B855"
                },
                "ownerReferences": [
                    {
                        "apiVersion": "batch/v1",
                        "kind": "Job",
                        "name": "helm-install-traefik-crd",
                        "uid": "258893ed-a29a-44f2-b2b6-ac1732cdd902",
                        "controller": true,
                        "blockOwnerDeletion": true
                    }
                ]
            },
            "spec": {
                "volumes": [
                    {
                        "name": "values",
                        "configMap": {
                            "name": "chart-values-traefik-crd",
                            "defaultMode": 420
                        }
                    },
                    {
                        "name": "content",
                        "configMap": {
                            "name": "chart-content-traefik-crd",
                            "defaultMode": 420
                        }
                    },
                    {
                        "name": "kube-api-access-xcnkp",
                        "projected": {
                            "sources": [
                                {
                                    "serviceAccountToken": {
                                        "expirationSeconds": 3607,
                                        "path": "token"
                                    }
                                },
                                {
                                    "configMap": {
                                        "name": "kube-root-ca.crt",
                                        "items": [
                                            {
                                                "key": "ca.crt",
                                                "path": "ca.crt"
                                            }
                                        ]
                                    }
                                },
                                {
                                    "downwardAPI": {
                                        "items": [
                                            {
                                                "path": "namespace",
                                                "fieldRef": {
                                                    "apiVersion": "v1",
                                                    "fieldPath": "metadata.namespace"
                                                }
                                            }
                                        ]
                                    }
                                }
                            ],
                            "defaultMode": 420
                        }
                    }
                ],
                "containers": [
                    {
                        "name": "helm",
                        "image": "rancher/klipper-helm:v0.6.6-build20211022",
                        "args": [
                            "install"
                        ],
                        "env": [
                            {
                                "name": "NAME",
                                "value": "traefik-crd"
                            },
                            {
                                "name": "VERSION"
                            },
                            {
                                "name": "REPO"
                            },
                            {
                                "name": "HELM_DRIVER",
                                "value": "secret"
                            },
                            {
                                "name": "CHART_NAMESPACE",
                                "value": "kube-system"
                            },
                            {
                                "name": "CHART",
                                "value": "https://%{KUBERNETES_API}%/static/charts/traefik-crd-10.14.100.tgz"
                            },
                            {
                                "name": "HELM_VERSION"
                            },
                            {
                                "name": "TARGET_NAMESPACE",
                                "value": "kube-system"
                            },
                            {
                                "name": "NO_PROXY",
                                "value": ".svc,.cluster.local,10.42.0.0/16,10.43.0.0/16"
                            }
                        ],
                        "resources": {},
                        "volumeMounts": [
                            {
                                "name": "values",
                                "mountPath": "/config"
                            },
                            {
                                "name": "content",
                                "mountPath": "/chart"
                            },
                            {
                                "name": "kube-api-access-xcnkp",
                                "readOnly": true,
                                "mountPath": "/var/run/secrets/kubernetes.io/serviceaccount"
                            }
                        ],
                        "terminationMessagePath": "/dev/termination-log",
                        "terminationMessagePolicy": "File",
                        "imagePullPolicy": "IfNotPresent"
                    }
                ],
                "restartPolicy": "OnFailure",
                "terminationGracePeriodSeconds": 30,
                "dnsPolicy": "ClusterFirst",
                "nodeSelector": {
                    "kubernetes.io/os": "linux"
                },
                "serviceAccountName": "helm-traefik-crd",
                "serviceAccount": "helm-traefik-crd",
                "nodeName": "localhost.localdomain",
                "securityContext": {},
                "schedulerName": "default-scheduler",
                "tolerations": [
                    {
                        "key": "node.kubernetes.io/not-ready",
                        "operator": "Exists",
                        "effect": "NoExecute",
                        "tolerationSeconds": 300
                    },
                    {
                        "key": "node.kubernetes.io/unreachable",
                        "operator": "Exists",
                        "effect": "NoExecute",
                        "tolerationSeconds": 300
                    }
                ],
                "priority": 0,
                "enableServiceLinks": true,
                "preemptionPolicy": "PreemptLowerPriority"
            },
            "status": {
                "phase": "Succeeded",
                "conditions": [
                    {
                        "type": "Initialized",
                        "status": "True",
                        "lastProbeTime": null,
                        "lastTransitionTime": "2022-03-22T17:40:27Z",
                        "reason": "PodCompleted"
                    },
                    {
                        "type": "Ready",
                        "status": "False",
                        "lastProbeTime": null,
                        "lastTransitionTime": "2022-03-23T15:12:38Z",
                        "reason": "PodCompleted"
                    },
                    {
                        "type": "ContainersReady",
                        "status": "False",
                        "lastProbeTime": null,
                        "lastTransitionTime": "2022-03-23T15:12:38Z",
                        "reason": "PodCompleted"
                    },
                    {
                        "type": "PodScheduled",
                        "status": "True",
                        "lastProbeTime": null,
                        "lastTransitionTime": "2022-03-22T17:40:27Z"
                    }
                ],
                "hostIP": "192.168.1.200",
                "podIP": "10.42.0.10",
                "podIPs": [
                    {
                        "ip": "10.42.0.10"
                    }
                ],
                "startTime": "2022-03-22T17:40:27Z",
                "containerStatuses": [
                    {
                        "name": "helm",
                        "state": {
                            "terminated": {
                                "exitCode": 0,
                                "reason": "Completed",
                                "startedAt": "2022-03-23T15:12:25Z",
                                "finishedAt": "2022-03-23T15:12:37Z",
                                "containerID": "containerd://d51aae9c36ac806df32384e5ae3c5cc68951e04cbce88d805aef3a42babea2c4"
                            }
                        },
                        "lastState": {},
                        "ready": false,
                        "restartCount": 0,
                        "image": "docker.io/rancher/klipper-helm:v0.6.6-build20211022",
                        "imageID": "docker.io/rancher/klipper-helm@sha256:961459b8641aa77dae39c96ba8efba88805a8949dcaafa131d1aecc1510128bf",
                        "containerID": "containerd://d51aae9c36ac806df32384e5ae3c5cc68951e04cbce88d805aef3a42babea2c4",
                        "started": false
                    }
                ],
                "qosClass": "BestEffort"
            }
        },
        {
            "metadata": {
                "name": "metrics-server-ff9dbcb6c-j797w",
                "generateName": "metrics-server-ff9dbcb6c-",
                "namespace": "kube-system",
                "uid": "7e730f1e-800d-4106-8f0c-d9ae3dc06184",
                "resourceVersion": "18791",
                "creationTimestamp": "2022-03-22T17:40:27Z",
                "labels": {
                    "k8s-app": "metrics-server",
                    "pod-template-hash": "ff9dbcb6c"
                },
                "ownerReferences": [
                    {
                        "apiVersion": "apps/v1",
                        "kind": "ReplicaSet",
                        "name": "metrics-server-ff9dbcb6c",
                        "uid": "9fcd8f0b-ec0c-4461-827f-ebe18dca023d",
                        "controller": true,
                        "blockOwnerDeletion": true
                    }
                ]
            },
            "spec": {
                "volumes": [
                    {
                        "name": "tmp-dir",
                        "emptyDir": {}
                    },
                    {
                        "name": "kube-api-access-q4flz",
                        "projected": {
                            "sources": [
                                {
                                    "serviceAccountToken": {
                                        "expirationSeconds": 3607,
                                        "path": "token"
                                    }
                                },
                                {
                                    "configMap": {
                                        "name": "kube-root-ca.crt",
                                        "items": [
                                            {
                                                "key": "ca.crt",
                                                "path": "ca.crt"
                                            }
                                        ]
                                    }
                                },
                                {
                                    "downwardAPI": {
                                        "items": [
                                            {
                                                "path": "namespace",
                                                "fieldRef": {
                                                    "apiVersion": "v1",
                                                    "fieldPath": "metadata.namespace"
                                                }
                                            }
                                        ]
                                    }
                                }
                            ],
                            "defaultMode": 420
                        }
                    }
                ],
                "containers": [
                    {
                        "name": "metrics-server",
                        "image": "rancher/mirrored-metrics-server:v0.5.2",
                        "args": [
                            "--cert-dir=/tmp",
                            "--secure-port=4443",
                            "--kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname",
                            "--kubelet-use-node-status-port",
                            "--metric-resolution=15s"
                        ],
                        "ports": [
                            {
                                "name": "https",
                                "containerPort": 4443,
                                "protocol": "TCP"
                            }
                        ],
                        "resources": {
                            "requests": {
                                "cpu": "100m",
                                "memory": "70Mi"
                            }
                        },
                        "volumeMounts": [
                            {
                                "name": "tmp-dir",
                                "mountPath": "/tmp"
                            },
                            {
                                "name": "kube-api-access-q4flz",
                                "readOnly": true,
                                "mountPath": "/var/run/secrets/kubernetes.io/serviceaccount"
                            }
                        ],
                        "livenessProbe": {
                            "httpGet": {
                                "path": "/livez",
                                "port": "https",
                                "scheme": "HTTPS"
                            },
                            "initialDelaySeconds": 60,
                            "timeoutSeconds": 1,
                            "periodSeconds": 10,
                            "successThreshold": 1,
                            "failureThreshold": 3
                        },
                        "readinessProbe": {
                            "httpGet": {
                                "path": "/readyz",
                                "port": "https",
                                "scheme": "HTTPS"
                            },
                            "timeoutSeconds": 1,
                            "periodSeconds": 2,
                            "successThreshold": 1,
                            "failureThreshold": 3
                        },
                        "terminationMessagePath": "/dev/termination-log",
                        "terminationMessagePolicy": "File",
                        "imagePullPolicy": "IfNotPresent",
                        "securityContext": {
                            "runAsUser": 1000,
                            "runAsNonRoot": true,
                            "readOnlyRootFilesystem": true,
                            "allowPrivilegeEscalation": false
                        }
                    }
                ],
                "restartPolicy": "Always",
                "terminationGracePeriodSeconds": 30,
                "dnsPolicy": "ClusterFirst",
                "serviceAccountName": "metrics-server",
                "serviceAccount": "metrics-server",
                "nodeName": "localhost.localdomain",
                "securityContext": {},
                "schedulerName": "default-scheduler",
                "tolerations": [
                    {
                        "key": "CriticalAddonsOnly",
                        "operator": "Exists"
                    },
                    {
                        "key": "node-role.kubernetes.io/control-plane",
                        "operator": "Exists",
                        "effect": "NoSchedule"
                    },
                    {
                        "key": "node-role.kubernetes.io/master",
                        "operator": "Exists",
                        "effect": "NoSchedule"
                    },
                    {
                        "key": "node.kubernetes.io/not-ready",
                        "operator": "Exists",
                        "effect": "NoExecute",
                        "tolerationSeconds": 300
                    },
                    {
                        "key": "node.kubernetes.io/unreachable",
                        "operator": "Exists",
                        "effect": "NoExecute",
                        "tolerationSeconds": 300
                    }
                ],
                "priorityClassName": "system-node-critical",
                "priority": 2000001000,
                "enableServiceLinks": true,
                "preemptionPolicy": "PreemptLowerPriority"
            },
            "status": {
                "phase": "Running",
                "conditions": [
                    {
                        "type": "Initialized",
                        "status": "True",
                        "lastProbeTime": null,
                        "lastTransitionTime": "2022-03-22T17:40:28Z"
                    },
                    {
                        "type": "Ready",
                        "status": "True",
                        "lastProbeTime": null,
                        "lastTransitionTime": "2022-04-10T08:08:56Z"
                    },
                    {
                        "type": "ContainersReady",
                        "status": "True",
                        "lastProbeTime": null,
                        "lastTransitionTime": "2022-04-10T08:08:56Z"
                    },
                    {
                        "type": "PodScheduled",
                        "status": "True",
                        "lastProbeTime": null,
                        "lastTransitionTime": "2022-03-22T17:40:27Z"
                    }
                ],
                "hostIP": "192.168.1.30",
                "podIP": "10.42.0.72",
                "podIPs": [
                    {
                        "ip": "10.42.0.72"
                    }
                ],
                "startTime": "2022-03-22T17:40:28Z",
                "containerStatuses": [
                    {
                        "name": "metrics-server",
                        "state": {
                            "running": {
                                "startedAt": "2022-04-10T08:07:36Z"
                            }
                        },
                        "lastState": {
                            "terminated": {
                                "exitCode": 2,
                                "reason": "Error",
                                "startedAt": "2022-04-08T17:41:24Z",
                                "finishedAt": "2022-04-08T17:41:27Z",
                                "containerID": "containerd://92cc435ee50d3e11c65ffd8359aad347b2810ca97395f687d477a9a85b5737a0"
                            }
                        },
                        "ready": true,
                        "restartCount": 84,
                        "image": "docker.io/rancher/mirrored-metrics-server:v0.5.2",
                        "imageID": "docker.io/rancher/mirrored-metrics-server@sha256:48ecad4fe641a09fa4459f93c7ad29d4916f6b9cf7e934d548f1d8eff96e2f35",
                        "containerID": "containerd://4943a14da1c66088d8dc651ea9d3250178153638425a795e11fdd48435cc7647",
                        "started": true
                    }
                ],
                "qosClass": "Burstable"
            }
        },
        {
            "metadata": {
                "name": "local-path-provisioner-84bb864455-m7442",
                "generateName": "local-path-provisioner-84bb864455-",
                "namespace": "kube-system",
                "uid": "fe77eae1-dfd7-4b03-95b0-021a0e962dec",
                "resourceVersion": "18796",
                "creationTimestamp": "2022-03-22T17:40:27Z",
                "labels": {
                    "app": "local-path-provisioner",
                    "pod-template-hash": "84bb864455"
                },
                "ownerReferences": [
                    {
                        "apiVersion": "apps/v1",
                        "kind": "ReplicaSet",
                        "name": "local-path-provisioner-84bb864455",
                        "uid": "c5928506-bda0-4403-9564-3052e8c9f325",
                        "controller": true,
                        "blockOwnerDeletion": true
                    }
                ]
            },
            "spec": {
                "volumes": [
                    {
                        "name": "config-volume",
                        "configMap": {
                            "name": "local-path-config",
                            "defaultMode": 420
                        }
                    },
                    {
                        "name": "kube-api-access-98kb9",
                        "projected": {
                            "sources": [
                                {
                                    "serviceAccountToken": {
                                        "expirationSeconds": 3607,
                                        "path": "token"
                                    }
                                },
                                {
                                    "configMap": {
                                        "name": "kube-root-ca.crt",
                                        "items": [
                                            {
                                                "key": "ca.crt",
                                                "path": "ca.crt"
                                            }
                                        ]
                                    }
                                },
                                {
                                    "downwardAPI": {
                                        "items": [
                                            {
                                                "path": "namespace",
                                                "fieldRef": {
                                                    "apiVersion": "v1",
                                                    "fieldPath": "metadata.namespace"
                                                }
                                            }
                                        ]
                                    }
                                }
                            ],
                            "defaultMode": 420
                        }
                    }
                ],
                "containers": [
                    {
                        "name": "local-path-provisioner",
                        "image": "rancher/local-path-provisioner:v0.0.21",
                        "command": [
                            "local-path-provisioner",
                            "start",
                            "--config",
                            "/etc/config/config.json"
                        ],
                        "env": [
                            {
                                "name": "POD_NAMESPACE",
                                "valueFrom": {
                                    "fieldRef": {
                                        "apiVersion": "v1",
                                        "fieldPath": "metadata.namespace"
                                    }
                                }
                            }
                        ],
                        "resources": {},
                        "volumeMounts": [
                            {
                                "name": "config-volume",
                                "mountPath": "/etc/config/"
                            },
                            {
                                "name": "kube-api-access-98kb9",
                                "readOnly": true,
                                "mountPath": "/var/run/secrets/kubernetes.io/serviceaccount"
                            }
                        ],
                        "terminationMessagePath": "/dev/termination-log",
                        "terminationMessagePolicy": "File",
                        "imagePullPolicy": "IfNotPresent"
                    }
                ],
                "restartPolicy": "Always",
                "terminationGracePeriodSeconds": 30,
                "dnsPolicy": "ClusterFirst",
                "serviceAccountName": "local-path-provisioner-service-account",
                "serviceAccount": "local-path-provisioner-service-account",
                "nodeName": "localhost.localdomain",
                "securityContext": {},
                "schedulerName": "default-scheduler",
                "tolerations": [
                    {
                        "key": "CriticalAddonsOnly",
                        "operator": "Exists"
                    },
                    {
                        "key": "node-role.kubernetes.io/control-plane",
                        "operator": "Exists",
                        "effect": "NoSchedule"
                    },
                    {
                        "key": "node-role.kubernetes.io/master",
                        "operator": "Exists",
                        "effect": "NoSchedule"
                    },
                    {
                        "key": "node.kubernetes.io/not-ready",
                        "operator": "Exists",
                        "effect": "NoExecute",
                        "tolerationSeconds": 300
                    },
                    {
                        "key": "node.kubernetes.io/unreachable",
                        "operator": "Exists",
                        "effect": "NoExecute",
                        "tolerationSeconds": 300
                    }
                ],
                "priorityClassName": "system-node-critical",
                "priority": 2000001000,
                "enableServiceLinks": true,
                "preemptionPolicy": "PreemptLowerPriority"
            },
            "status": {
                "phase": "Running",
                "conditions": [
                    {
                        "type": "Initialized",
                        "status": "True",
                        "lastProbeTime": null,
                        "lastTransitionTime": "2022-03-22T17:40:28Z"
                    },
                    {
                        "type": "Ready",
                        "status": "True",
                        "lastProbeTime": null,
                        "lastTransitionTime": "2022-04-10T08:07:38Z"
                    },
                    {
                        "type": "ContainersReady",
                        "status": "True",
                        "lastProbeTime": null,
                        "lastTransitionTime": "2022-04-10T08:07:38Z"
                    },
                    {
                        "type": "PodScheduled",
                        "status": "True",
                        "lastProbeTime": null,
                        "lastTransitionTime": "2022-03-22T17:40:27Z"
                    }
                ],
                "hostIP": "192.168.1.30",
                "podIP": "10.42.0.71",
                "podIPs": [
                    {
                        "ip": "10.42.0.71"
                    }
                ],
                "startTime": "2022-03-22T17:40:28Z",
                "containerStatuses": [
                    {
                        "name": "local-path-provisioner",
                        "state": {
                            "running": {
                                "startedAt": "2022-04-10T08:07:37Z"
                            }
                        },
                        "lastState": {
                            "terminated": {
                                "exitCode": 255,
                                "reason": "Unknown",
                                "startedAt": "2022-04-08T15:17:21Z",
                                "finishedAt": "2022-04-10T08:07:15Z",
                                "containerID": "containerd://1b42ecda8b962bdbd3ac1662784f0580b37c871aee3796151a06e3e073a34b35"
                            }
                        },
                        "ready": true,
                        "restartCount": 13,
                        "image": "docker.io/rancher/local-path-provisioner:v0.0.21",
                        "imageID": "docker.io/rancher/local-path-provisioner@sha256:1da612c913ce0b4ab82e20844baa9dce1f7065e39412d6a0bb4de99c413f21bf",
                        "containerID": "containerd://3b9522c0815b348915eb3c121dbede0f5936f1ff4d7e7037e8b30be152487d4c",
                        "started": true
                    }
                ],
                "qosClass": "BestEffort"
            }
        },
        {
            "metadata": {
                "name": "svclb-traefik-74xqk",
                "generateName": "svclb-traefik-",
                "namespace": "kube-system",
                "uid": "1b4db5ea-7827-4226-b322-dfb3437221f9",
                "resourceVersion": "18802",
                "creationTimestamp": "2022-03-23T15:12:58Z",
                "labels": {
                    "app": "svclb-traefik",
                    "controller-revision-hash": "678d7565fc",
                    "pod-template-generation": "1",
                    "svccontroller.k3s.cattle.io/svcname": "traefik"
                },
                "ownerReferences": [
                    {
                        "apiVersion": "apps/v1",
                        "kind": "DaemonSet",
                        "name": "svclb-traefik",
                        "uid": "73be0387-d9d0-4792-8fae-312ec69407f9",
                        "controller": true,
                        "blockOwnerDeletion": true
                    }
                ]
            },
            "spec": {
                "containers": [
                    {
                        "name": "lb-port-80",
                        "image": "rancher/klipper-lb:v0.3.4",
                        "ports": [
                            {
                                "name": "lb-port-80",
                                "hostPort": 80,
                                "containerPort": 80,
                                "protocol": "TCP"
                            }
                        ],
                        "env": [
                            {
                                "name": "SRC_PORT",
                                "value": "80"
                            },
                            {
                                "name": "DEST_PROTO",
                                "value": "TCP"
                            },
                            {
                                "name": "DEST_PORT",
                                "value": "80"
                            },
                            {
                                "name": "DEST_IPS",
                                "value": "10.43.47.227"
                            }
                        ],
                        "resources": {},
                        "terminationMessagePath": "/dev/termination-log",
                        "terminationMessagePolicy": "File",
                        "imagePullPolicy": "IfNotPresent",
                        "securityContext": {
                            "capabilities": {
                                "add": [
                                    "NET_ADMIN"
                                ]
                            }
                        }
                    },
                    {
                        "name": "lb-port-443",
                        "image": "rancher/klipper-lb:v0.3.4",
                        "ports": [
                            {
                                "name": "lb-port-443",
                                "hostPort": 443,
                                "containerPort": 443,
                                "protocol": "TCP"
                            }
                        ],
                        "env": [
                            {
                                "name": "SRC_PORT",
                                "value": "443"
                            },
                            {
                                "name": "DEST_PROTO",
                                "value": "TCP"
                            },
                            {
                                "name": "DEST_PORT",
                                "value": "443"
                            },
                            {
                                "name": "DEST_IPS",
                                "value": "10.43.47.227"
                            }
                        ],
                        "resources": {},
                        "terminationMessagePath": "/dev/termination-log",
                        "terminationMessagePolicy": "File",
                        "imagePullPolicy": "IfNotPresent",
                        "securityContext": {
                            "capabilities": {
                                "add": [
                                    "NET_ADMIN"
                                ]
                            }
                        }
                    }
                ],
                "restartPolicy": "Always",
                "terminationGracePeriodSeconds": 30,
                "dnsPolicy": "ClusterFirst",
                "serviceAccountName": "default",
                "serviceAccount": "default",
                "automountServiceAccountToken": false,
                "nodeName": "localhost.localdomain",
                "securityContext": {},
                "affinity": {
                    "nodeAffinity": {
                        "requiredDuringSchedulingIgnoredDuringExecution": {
                            "nodeSelectorTerms": [
                                {
                                    "matchFields": [
                                        {
                                            "key": "metadata.name",
                                            "operator": "In",
                                            "values": [
                                                "localhost.localdomain"
                                            ]
                                        }
                                    ]
                                }
                            ]
                        }
                    }
                },
                "schedulerName": "default-scheduler",
                "tolerations": [
                    {
                        "key": "node-role.kubernetes.io/master",
                        "operator": "Exists",
                        "effect": "NoSchedule"
                    },
                    {
                        "key": "node-role.kubernetes.io/control-plane",
                        "operator": "Exists",
                        "effect": "NoSchedule"
                    },
                    {
                        "key": "CriticalAddonsOnly",
                        "operator": "Exists"
                    },
                    {
                        "key": "node.kubernetes.io/not-ready",
                        "operator": "Exists",
                        "effect": "NoExecute"
                    },
                    {
                        "key": "node.kubernetes.io/unreachable",
                        "operator": "Exists",
                        "effect": "NoExecute"
                    },
                    {
                        "key": "node.kubernetes.io/disk-pressure",
                        "operator": "Exists",
                        "effect": "NoSchedule"
                    },
                    {
                        "key": "node.kubernetes.io/memory-pressure",
                        "operator": "Exists",
                        "effect": "NoSchedule"
                    },
                    {
                        "key": "node.kubernetes.io/pid-pressure",
                        "operator": "Exists",
                        "effect": "NoSchedule"
                    },
                    {
                        "key": "node.kubernetes.io/unschedulable",
                        "operator": "Exists",
                        "effect": "NoSchedule"
                    }
                ],
                "priority": 0,
                "enableServiceLinks": true,
                "preemptionPolicy": "PreemptLowerPriority"
            },
            "status": {
                "phase": "Running",
                "conditions": [
                    {
                        "type": "Initialized",
                        "status": "True",
                        "lastProbeTime": null,
                        "lastTransitionTime": "2022-03-23T15:12:59Z"
                    },
                    {
                        "type": "Ready",
                        "status": "True",
                        "lastProbeTime": null,
                        "lastTransitionTime": "2022-04-10T08:07:42Z"
                    },
                    {
                        "type": "ContainersReady",
                        "status": "True",
                        "lastProbeTime": null,
                        "lastTransitionTime": "2022-04-10T08:07:42Z"
                    },
                    {
                        "type": "PodScheduled",
                        "status": "True",
                        "lastProbeTime": null,
                        "lastTransitionTime": "2022-03-23T15:12:59Z"
                    }
                ],
                "hostIP": "192.168.1.30",
                "podIP": "10.42.0.76",
                "podIPs": [
                    {
                        "ip": "10.42.0.76"
                    }
                ],
                "startTime": "2022-03-23T15:12:59Z",
                "containerStatuses": [
                    {
                        "name": "lb-port-443",
                        "state": {
                            "running": {
                                "startedAt": "2022-04-10T08:07:42Z"
                            }
                        },
                        "lastState": {
                            "terminated": {
                                "exitCode": 255,
                                "reason": "Unknown",
                                "startedAt": "2022-04-08T15:05:26Z",
                                "finishedAt": "2022-04-10T08:07:16Z",
                                "containerID": "containerd://e9f1266d1d8916cdfc3a86ca88e05387afe2f2323e91b41dd9c77408371086f7"
                            }
                        },
                        "ready": true,
                        "restartCount": 6,
                        "image": "docker.io/rancher/klipper-lb:v0.3.4",
                        "imageID": "docker.io/rancher/klipper-lb@sha256:d5ec8350ee1a80ec783effe95a2fefae371d1498ac04865cb6024499882f291c",
                        "containerID": "containerd://fa0aeaaf17d7ad02a8ae8b83a0ecd1aa02375f5ad3f704cac1af056baa34734a",
                        "started": true
                    },
                    {
                        "name": "lb-port-80",
                        "state": {
                            "running": {
                                "startedAt": "2022-04-10T08:07:40Z"
                            }
                        },
                        "lastState": {
                            "terminated": {
                                "exitCode": 255,
                                "reason": "Unknown",
                                "startedAt": "2022-04-08T15:05:21Z",
                                "finishedAt": "2022-04-10T08:07:16Z",
                                "containerID": "containerd://c84252cb61b18c303ff1d233c5af3c945a5d1a76d6f96e850cc880c4515beec0"
                            }
                        },
                        "ready": true,
                        "restartCount": 6,
                        "image": "docker.io/rancher/klipper-lb:v0.3.4",
                        "imageID": "docker.io/rancher/klipper-lb@sha256:d5ec8350ee1a80ec783effe95a2fefae371d1498ac04865cb6024499882f291c",
                        "containerID": "containerd://d62bebc6ff052a2facbfc1ea31e19ddbc1bc81acf1e60381776f5713e7504698",
                        "started": true
                    }
                ],
                "qosClass": "BestEffort"
            }
        },
        {
            "metadata": {
                "name": "traefik-56c4b88c4b-6lv59",
                "generateName": "traefik-56c4b88c4b-",
                "namespace": "kube-system",
                "uid": "09cd68b2-c0c7-47ec-b1a8-9cc8b39b280a",
                "resourceVersion": "18805",
                "creationTimestamp": "2022-03-23T15:12:58Z",
                "labels": {
                    "app.kubernetes.io/instance": "traefik",
                    "app.kubernetes.io/managed-by": "Helm",
                    "app.kubernetes.io/name": "traefik",
                    "helm.sh/chart": "traefik-10.14.100",
                    "pod-template-hash": "56c4b88c4b"
                },
                "annotations": {
                    "prometheus.io/path": "/metrics",
                    "prometheus.io/port": "9100",
                    "prometheus.io/scrape": "true"
                },
                "ownerReferences": [
                    {
                        "apiVersion": "apps/v1",
                        "kind": "ReplicaSet",
                        "name": "traefik-56c4b88c4b",
                        "uid": "0764b677-1c27-42fa-aa98-4ec8635eed04",
                        "controller": true,
                        "blockOwnerDeletion": true
                    }
                ]
            },
            "spec": {
                "volumes": [
                    {
                        "name": "data",
                        "emptyDir": {}
                    },
                    {
                        "name": "tmp",
                        "emptyDir": {}
                    },
                    {
                        "name": "kube-api-access-2hnxq",
                        "projected": {
                            "sources": [
                                {
                                    "serviceAccountToken": {
                                        "expirationSeconds": 3607,
                                        "path": "token"
                                    }
                                },
                                {
                                    "configMap": {
                                        "name": "kube-root-ca.crt",
                                        "items": [
                                            {
                                                "key": "ca.crt",
                                                "path": "ca.crt"
                                            }
                                        ]
                                    }
                                },
                                {
                                    "downwardAPI": {
                                        "items": [
                                            {
                                                "path": "namespace",
                                                "fieldRef": {
                                                    "apiVersion": "v1",
                                                    "fieldPath": "metadata.namespace"
                                                }
                                            }
                                        ]
                                    }
                                }
                            ],
                            "defaultMode": 420
                        }
                    }
                ],
                "containers": [
                    {
                        "name": "traefik",
                        "image": "rancher/mirrored-library-traefik:2.6.1",
                        "args": [
                            "--global.checknewversion",
                            "--global.sendanonymoususage",
                            "--entrypoints.metrics.address=:9100/tcp",
                            "--entrypoints.traefik.address=:9000/tcp",
                            "--entrypoints.web.address=:8000/tcp",
                            "--entrypoints.websecure.address=:8443/tcp",
                            "--api.dashboard=true",
                            "--ping=true",
                            "--metrics.prometheus=true",
                            "--metrics.prometheus.entrypoint=metrics",
                            "--providers.kubernetescrd",
                            "--providers.kubernetesingress",
                            "--providers.kubernetesingress.ingressendpoint.publishedservice=kube-system/traefik",
                            "--entrypoints.websecure.http.tls=true"
                        ],
                        "ports": [
                            {
                                "name": "metrics",
                                "containerPort": 9100,
                                "protocol": "TCP"
                            },
                            {
                                "name": "traefik",
                                "containerPort": 9000,
                                "protocol": "TCP"
                            },
                            {
                                "name": "web",
                                "containerPort": 8000,
                                "protocol": "TCP"
                            },
                            {
                                "name": "websecure",
                                "containerPort": 8443,
                                "protocol": "TCP"
                            }
                        ],
                        "resources": {},
                        "volumeMounts": [
                            {
                                "name": "data",
                                "mountPath": "/data"
                            },
                            {
                                "name": "tmp",
                                "mountPath": "/tmp"
                            },
                            {
                                "name": "kube-api-access-2hnxq",
                                "readOnly": true,
                                "mountPath": "/var/run/secrets/kubernetes.io/serviceaccount"
                            }
                        ],
                        "livenessProbe": {
                            "httpGet": {
                                "path": "/ping",
                                "port": 9000,
                                "scheme": "HTTP"
                            },
                            "initialDelaySeconds": 10,
                            "timeoutSeconds": 2,
                            "periodSeconds": 10,
                            "successThreshold": 1,
                            "failureThreshold": 3
                        },
                        "readinessProbe": {
                            "httpGet": {
                                "path": "/ping",
                                "port": 9000,
                                "scheme": "HTTP"
                            },
                            "initialDelaySeconds": 10,
                            "timeoutSeconds": 2,
                            "periodSeconds": 10,
                            "successThreshold": 1,
                            "failureThreshold": 1
                        },
                        "terminationMessagePath": "/dev/termination-log",
                        "terminationMessagePolicy": "File",
                        "imagePullPolicy": "IfNotPresent",
                        "securityContext": {
                            "capabilities": {
                                "drop": [
                                    "ALL"
                                ]
                            },
                            "runAsUser": 65532,
                            "runAsGroup": 65532,
                            "runAsNonRoot": true,
                            "readOnlyRootFilesystem": true
                        }
                    }
                ],
                "restartPolicy": "Always",
                "terminationGracePeriodSeconds": 60,
                "dnsPolicy": "ClusterFirst",
                "serviceAccountName": "traefik",
                "serviceAccount": "traefik",
                "nodeName": "localhost.localdomain",
                "securityContext": {
                    "fsGroup": 65532
                },
                "schedulerName": "default-scheduler",
                "tolerations": [
                    {
                        "key": "CriticalAddonsOnly",
                        "operator": "Exists"
                    },
                    {
                        "key": "node-role.kubernetes.io/control-plane",
                        "operator": "Exists",
                        "effect": "NoSchedule"
                    },
                    {
                        "key": "node-role.kubernetes.io/master",
                        "operator": "Exists",
                        "effect": "NoSchedule"
                    },
                    {
                        "key": "node.kubernetes.io/not-ready",
                        "operator": "Exists",
                        "effect": "NoExecute",
                        "tolerationSeconds": 300
                    },
                    {
                        "key": "node.kubernetes.io/unreachable",
                        "operator": "Exists",
                        "effect": "NoExecute",
                        "tolerationSeconds": 300
                    }
                ],
                "priorityClassName": "system-cluster-critical",
                "priority": 2000000000,
                "enableServiceLinks": true,
                "preemptionPolicy": "PreemptLowerPriority"
            },
            "status": {
                "phase": "Running",
                "conditions": [
                    {
                        "type": "Initialized",
                        "status": "True",
                        "lastProbeTime": null,
                        "lastTransitionTime": "2022-03-23T15:12:58Z"
                    },
                    {
                        "type": "Ready",
                        "status": "True",
                        "lastProbeTime": null,
                        "lastTransitionTime": "2022-04-10T08:07:56Z"
                    },
                    {
                        "type": "ContainersReady",
                        "status": "True",
                        "lastProbeTime": null,
                        "lastTransitionTime": "2022-04-10T08:07:56Z"
                    },
                    {
                        "type": "PodScheduled",
                        "status": "True",
                        "lastProbeTime": null,
                        "lastTransitionTime": "2022-03-23T15:12:58Z"
                    }
                ],
                "hostIP": "192.168.1.30",
                "podIP": "10.42.0.74",
                "podIPs": [
                    {
                        "ip": "10.42.0.74"
                    }
                ],
                "startTime": "2022-03-23T15:12:58Z",
                "containerStatuses": [
                    {
                        "name": "traefik",
                        "state": {
                            "running": {
                                "startedAt": "2022-04-10T08:07:37Z"
                            }
                        },
                        "lastState": {
                            "terminated": {
                                "exitCode": 255,
                                "reason": "Unknown",
                                "startedAt": "2022-04-08T17:10:03Z",
                                "finishedAt": "2022-04-10T08:07:16Z",
                                "containerID": "containerd://f95aff57e0ec787c87ebbf2ba48f14a0696d16132e4e9cebeb6b3ea9992322a8"
                            }
                        },
                        "ready": true,
                        "restartCount": 31,
                        "image": "docker.io/rancher/mirrored-library-traefik:2.6.1",
                        "imageID": "docker.io/rancher/mirrored-library-traefik@sha256:d2b19904395891849a78a41159ee8ae812b9cb10bd40103d3a9557cd76c06758",
                        "containerID": "containerd://badffa04c07bae643bb8b73024464bfb1d994b12a71415ca5816a3ff883bbe4f",
                        "started": true
                    }
                ],
                "qosClass": "BestEffort"
            }
        },
        {
            "metadata": {
                "name": "coredns-96cc4f57d-qlxzq",
                "generateName": "coredns-96cc4f57d-",
                "namespace": "kube-system",
                "uid": "32abdc44-c48a-478c-9810-f3e3d34ddba7",
                "resourceVersion": "18809",
                "creationTimestamp": "2022-03-22T17:40:27Z",
                "labels": {
                    "k8s-app": "kube-dns",
                    "pod-template-hash": "96cc4f57d"
                },
                "ownerReferences": [
                    {
                        "apiVersion": "apps/v1",
                        "kind": "ReplicaSet",
                        "name": "coredns-96cc4f57d",
                        "uid": "d71a1e43-4705-47a5-b078-d9af12f93f4b",
                        "controller": true,
                        "blockOwnerDeletion": true
                    }
                ]
            },
            "spec": {
                "volumes": [
                    {
                        "name": "config-volume",
                        "configMap": {
                            "name": "coredns",
                            "items": [
                                {
                                    "key": "Corefile",
                                    "path": "Corefile"
                                },
                                {
                                    "key": "NodeHosts",
                                    "path": "NodeHosts"
                                }
                            ],
                            "defaultMode": 420
                        }
                    },
                    {
                        "name": "custom-config-volume",
                        "configMap": {
                            "name": "coredns-custom",
                            "defaultMode": 420,
                            "optional": true
                        }
                    },
                    {
                        "name": "kube-api-access-brrrc",
                        "projected": {
                            "sources": [
                                {
                                    "serviceAccountToken": {
                                        "expirationSeconds": 3607,
                                        "path": "token"
                                    }
                                },
                                {
                                    "configMap": {
                                        "name": "kube-root-ca.crt",
                                        "items": [
                                            {
                                                "key": "ca.crt",
                                                "path": "ca.crt"
                                            }
                                        ]
                                    }
                                },
                                {
                                    "downwardAPI": {
                                        "items": [
                                            {
                                                "path": "namespace",
                                                "fieldRef": {
                                                    "apiVersion": "v1",
                                                    "fieldPath": "metadata.namespace"
                                                }
                                            }
                                        ]
                                    }
                                }
                            ],
                            "defaultMode": 420
                        }
                    }
                ],
                "containers": [
                    {
                        "name": "coredns",
                        "image": "rancher/mirrored-coredns-coredns:1.8.6",
                        "args": [
                            "-conf",
                            "/etc/coredns/Corefile"
                        ],
                        "ports": [
                            {
                                "name": "dns",
                                "containerPort": 53,
                                "protocol": "UDP"
                            },
                            {
                                "name": "dns-tcp",
                                "containerPort": 53,
                                "protocol": "TCP"
                            },
                            {
                                "name": "metrics",
                                "containerPort": 9153,
                                "protocol": "TCP"
                            }
                        ],
                        "resources": {
                            "limits": {
                                "memory": "170Mi"
                            },
                            "requests": {
                                "cpu": "100m",
                                "memory": "70Mi"
                            }
                        },
                        "volumeMounts": [
                            {
                                "name": "config-volume",
                                "readOnly": true,
                                "mountPath": "/etc/coredns"
                            },
                            {
                                "name": "custom-config-volume",
                                "readOnly": true,
                                "mountPath": "/etc/coredns/custom"
                            },
                            {
                                "name": "kube-api-access-brrrc",
                                "readOnly": true,
                                "mountPath": "/var/run/secrets/kubernetes.io/serviceaccount"
                            }
                        ],
                        "livenessProbe": {
                            "httpGet": {
                                "path": "/health",
                                "port": 8080,
                                "scheme": "HTTP"
                            },
                            "initialDelaySeconds": 60,
                            "timeoutSeconds": 1,
                            "periodSeconds": 10,
                            "successThreshold": 1,
                            "failureThreshold": 3
                        },
                        "readinessProbe": {
                            "httpGet": {
                                "path": "/ready",
                                "port": 8181,
                                "scheme": "HTTP"
                            },
                            "timeoutSeconds": 1,
                            "periodSeconds": 2,
                            "successThreshold": 1,
                            "failureThreshold": 3
                        },
                        "terminationMessagePath": "/dev/termination-log",
                        "terminationMessagePolicy": "File",
                        "imagePullPolicy": "IfNotPresent",
                        "securityContext": {
                            "capabilities": {
                                "add": [
                                    "NET_BIND_SERVICE"
                                ],
                                "drop": [
                                    "all"
                                ]
                            },
                            "readOnlyRootFilesystem": true,
                            "allowPrivilegeEscalation": false
                        }
                    }
                ],
                "restartPolicy": "Always",
                "terminationGracePeriodSeconds": 30,
                "dnsPolicy": "Default",
                "nodeSelector": {
                    "beta.kubernetes.io/os": "linux"
                },
                "serviceAccountName": "coredns",
                "serviceAccount": "coredns",
                "nodeName": "localhost.localdomain",
                "securityContext": {},
                "schedulerName": "default-scheduler",
                "tolerations": [
                    {
                        "key": "CriticalAddonsOnly",
                        "operator": "Exists"
                    },
                    {
                        "key": "node-role.kubernetes.io/control-plane",
                        "operator": "Exists",
                        "effect": "NoSchedule"
                    },
                    {
                        "key": "node-role.kubernetes.io/master",
                        "operator": "Exists",
                        "effect": "NoSchedule"
                    },
                    {
                        "key": "node.kubernetes.io/not-ready",
                        "operator": "Exists",
                        "effect": "NoExecute",
                        "tolerationSeconds": 300
                    },
                    {
                        "key": "node.kubernetes.io/unreachable",
                        "operator": "Exists",
                        "effect": "NoExecute",
                        "tolerationSeconds": 300
                    }
                ],
                "priorityClassName": "system-cluster-critical",
                "priority": 2000000000,
                "enableServiceLinks": true,
                "preemptionPolicy": "PreemptLowerPriority"
            },
            "status": {
                "phase": "Running",
                "conditions": [
                    {
                        "type": "Initialized",
                        "status": "True",
                        "lastProbeTime": null,
                        "lastTransitionTime": "2022-03-22T17:40:28Z"
                    },
                    {
                        "type": "Ready",
                        "status": "True",
                        "lastProbeTime": null,
                        "lastTransitionTime": "2022-04-10T08:07:41Z"
                    },
                    {
                        "type": "ContainersReady",
                        "status": "True",
                        "lastProbeTime": null,
                        "lastTransitionTime": "2022-04-10T08:07:41Z"
                    },
                    {
                        "type": "PodScheduled",
                        "status": "True",
                        "lastProbeTime": null,
                        "lastTransitionTime": "2022-03-22T17:40:27Z"
                    }
                ],
                "hostIP": "192.168.1.30",
                "podIP": "10.42.0.78",
                "podIPs": [
                    {
                        "ip": "10.42.0.78"
                    }
                ],
                "startTime": "2022-03-22T17:40:28Z",
                "containerStatuses": [
                    {
                        "name": "coredns",
                        "state": {
                            "running": {
                                "startedAt": "2022-04-10T08:07:38Z"
                            }
                        },
                        "lastState": {
                            "terminated": {
                                "exitCode": 255,
                                "reason": "Unknown",
                                "startedAt": "2022-04-08T17:09:58Z",
                                "finishedAt": "2022-04-10T08:07:16Z",
                                "containerID": "containerd://cda8c953c11aee927e7ee7e10bc4119a1d3e2f8893ed4ff88f748fb5ff9706f7"
                            }
                        },
                        "ready": true,
                        "restartCount": 34,
                        "image": "docker.io/rancher/mirrored-coredns-coredns:1.8.6",
                        "imageID": "docker.io/rancher/mirrored-coredns-coredns@sha256:d4aec4c63c47a553b334dbf079578cd9dc8616866a467d87e3e6b94e26ea9ed7",
                        "containerID": "containerd://b986c2b3d3e96f925f2fb4f9c81b7b59b302cf6b007ede25003470dca53d0722",
                        "started": true
                    }
                ],
                "qosClass": "Burstable"
            }
        }
    ]
}
==== START logs for container helm of pod kube-system/helm-install-traefik-crd--1-fgrt5 ====
unable to retrieve container logs for containerd://d51aae9c36ac806df32384e5ae3c5cc68951e04cbce88d805aef3a42babea2c4==== END logs for container helm of pod kube-system/helm-install-traefik-crd--1-fgrt5 ====
==== START logs for container metrics-server of pod kube-system/metrics-server-ff9dbcb6c-j797w ====
I0410 08:07:40.974608       1 secure_serving.go:202] Serving securely on [::]:4443
I0410 08:07:40.975040       1 requestheader_controller.go:169] Starting RequestHeaderAuthRequestController
I0410 08:07:40.975070       1 shared_informer.go:240] Waiting for caches to sync for RequestHeaderAuthRequestController
I0410 08:07:40.975128       1 dynamic_serving_content.go:130] Starting serving-cert::/tmp/apiserver.crt::/tmp/apiserver.key
I0410 08:07:40.975178       1 tlsconfig.go:240] Starting DynamicServingCertificateController
I0410 08:07:41.002311       1 configmap_cafile_content.go:202] Starting client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0410 08:07:41.002332       1 shared_informer.go:240] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0410 08:07:41.002349       1 configmap_cafile_content.go:202] Starting client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file
I0410 08:07:41.002353       1 shared_informer.go:240] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file
I0410 08:07:41.046651       1 server.go:188] "Failed probe" probe="metric-storage-ready" err="not metrics to serve"
I0410 08:07:41.080288       1 shared_informer.go:247] Caches are synced for RequestHeaderAuthRequestController 
I0410 08:07:41.103454       1 shared_informer.go:247] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file 
I0410 08:07:41.204402       1 shared_informer.go:247] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file 
I0410 08:07:41.232454       1 server.go:188] "Failed probe" probe="metric-storage-ready" err="not metrics to serve"
I0410 08:07:43.030317       1 server.go:188] "Failed probe" probe="metric-storage-ready" err="not metrics to serve"
I0410 08:07:43.771490       1 server.go:188] "Failed probe" probe="metric-storage-ready" err="not metrics to serve"
E0410 08:07:43.786929       1 scraper.go:139] "Failed to scrape node" err="Get \"https://172.17.100.150:10250/stats/summary?only_cpu_and_memory=true\": dial tcp 172.17.100.150:10250: connect: no route to host" node="localhost.localdomain"
I0410 08:07:44.981532       1 server.go:188] "Failed probe" probe="metric-storage-ready" err="not metrics to serve"
I0410 08:07:46.984565       1 server.go:188] "Failed probe" probe="metric-storage-ready" err="not metrics to serve"
I0410 08:07:48.986766       1 server.go:188] "Failed probe" probe="metric-storage-ready" err="not metrics to serve"
I0410 08:07:50.980529       1 server.go:188] "Failed probe" probe="metric-storage-ready" err="not metrics to serve"
I0410 08:07:52.983319       1 server.go:188] "Failed probe" probe="metric-storage-ready" err="not metrics to serve"
I0410 08:07:54.981862       1 server.go:188] "Failed probe" probe="metric-storage-ready" err="not metrics to serve"
I0410 08:07:56.984923       1 server.go:188] "Failed probe" probe="metric-storage-ready" err="not metrics to serve"
E0410 08:07:58.766034       1 scraper.go:139] "Failed to scrape node" err="Get \"https://172.17.100.150:10250/stats/summary?only_cpu_and_memory=true\": dial tcp 172.17.100.150:10250: connect: no route to host" node="localhost.localdomain"
I0410 08:07:58.984080       1 server.go:188] "Failed probe" probe="metric-storage-ready" err="not metrics to serve"
I0410 08:08:00.982615       1 server.go:188] "Failed probe" probe="metric-storage-ready" err="not metrics to serve"
I0410 08:08:02.982893       1 server.go:188] "Failed probe" probe="metric-storage-ready" err="not metrics to serve"
I0410 08:08:04.989424       1 server.go:188] "Failed probe" probe="metric-storage-ready" err="not metrics to serve"
I0410 08:08:06.984977       1 server.go:188] "Failed probe" probe="metric-storage-ready" err="not metrics to serve"
I0410 08:08:08.981592       1 server.go:188] "Failed probe" probe="metric-storage-ready" err="not metrics to serve"
I0410 08:08:10.981727       1 server.go:188] "Failed probe" probe="metric-storage-ready" err="not metrics to serve"
I0410 08:08:12.981575       1 server.go:188] "Failed probe" probe="metric-storage-ready" err="not metrics to serve"
E0410 08:08:13.760247       1 scraper.go:139] "Failed to scrape node" err="Get \"https://172.17.100.150:10250/stats/summary?only_cpu_and_memory=true\": dial tcp 172.17.100.150:10250: connect: no route to host" node="localhost.localdomain"
I0410 08:08:14.984074       1 server.go:188] "Failed probe" probe="metric-storage-ready" err="not metrics to serve"
I0410 08:08:16.980892       1 server.go:188] "Failed probe" probe="metric-storage-ready" err="not metrics to serve"
I0410 08:08:18.981160       1 server.go:188] "Failed probe" probe="metric-storage-ready" err="not metrics to serve"
I0410 08:08:20.987625       1 server.go:188] "Failed probe" probe="metric-storage-ready" err="not metrics to serve"
I0410 08:08:22.989568       1 server.go:188] "Failed probe" probe="metric-storage-ready" err="not metrics to serve"
I0410 08:08:24.985660       1 server.go:188] "Failed probe" probe="metric-storage-ready" err="not metrics to serve"
I0410 08:08:26.980693       1 server.go:188] "Failed probe" probe="metric-storage-ready" err="not metrics to serve"
E0410 08:08:28.766104       1 scraper.go:139] "Failed to scrape node" err="Get \"https://172.17.100.150:10250/stats/summary?only_cpu_and_memory=true\": dial tcp 172.17.100.150:10250: connect: no route to host" node="localhost.localdomain"
I0410 08:08:28.983552       1 server.go:188] "Failed probe" probe="metric-storage-ready" err="not metrics to serve"
I0410 08:08:30.983850       1 server.go:188] "Failed probe" probe="metric-storage-ready" err="not metrics to serve"
I0410 08:08:32.998266       1 server.go:188] "Failed probe" probe="metric-storage-ready" err="not metrics to serve"
I0410 08:08:34.991704       1 server.go:188] "Failed probe" probe="metric-storage-ready" err="not metrics to serve"
I0410 08:08:36.982403       1 server.go:188] "Failed probe" probe="metric-storage-ready" err="not metrics to serve"
I0410 08:08:38.984125       1 server.go:188] "Failed probe" probe="metric-storage-ready" err="not metrics to serve"
I0410 08:08:40.991222       1 server.go:188] "Failed probe" probe="metric-storage-ready" err="not metrics to serve"
I0410 08:08:43.011539       1 server.go:188] "Failed probe" probe="metric-storage-ready" err="not metrics to serve"
I0410 08:08:44.981033       1 server.go:188] "Failed probe" probe="metric-storage-ready" err="not metrics to serve"
I0410 08:08:46.981084       1 server.go:188] "Failed probe" probe="metric-storage-ready" err="not metrics to serve"
I0410 08:08:48.995582       1 server.go:188] "Failed probe" probe="metric-storage-ready" err="not metrics to serve"
I0410 08:08:50.981966       1 server.go:188] "Failed probe" probe="metric-storage-ready" err="not metrics to serve"
I0410 08:08:52.989951       1 server.go:188] "Failed probe" probe="metric-storage-ready" err="not metrics to serve"
I0410 08:08:54.984303       1 server.go:188] "Failed probe" probe="metric-storage-ready" err="not metrics to serve"
==== END logs for container metrics-server of pod kube-system/metrics-server-ff9dbcb6c-j797w ====
==== START logs for container local-path-provisioner of pod kube-system/local-path-provisioner-84bb864455-m7442 ====
I0410 08:07:40.643440       1 controller.go:773] Starting provisioner controller rancher.io/local-path_local-path-provisioner-84bb864455-m7442_db563268-ec40-4edc-b90d-e89474472798!
I0410 08:07:41.018141       1 controller.go:822] Started provisioner controller rancher.io/local-path_local-path-provisioner-84bb864455-m7442_db563268-ec40-4edc-b90d-e89474472798!
==== END logs for container local-path-provisioner of pod kube-system/local-path-provisioner-84bb864455-m7442 ====
==== START logs for container lb-port-80 of pod kube-system/svclb-traefik-74xqk ====
+ trap exit TERM INT
+ echo 10.43.47.227
+ grep -Eq :
+ cat /proc/sys/net/ipv4/ip_forward
+ '[' 1 '!=' 1 ]
+ iptables -t nat -I PREROUTING '!' -s 10.43.47.227/32 -p TCP --dport 80 -j DNAT --to 10.43.47.227:80
+ iptables -t nat -I POSTROUTING -d 10.43.47.227/32 -p TCP -j MASQUERADE
+ '[' '!' -e /pause ]
+ mkfifo /pause
==== END logs for container lb-port-80 of pod kube-system/svclb-traefik-74xqk ====
==== START logs for container lb-port-443 of pod kube-system/svclb-traefik-74xqk ====
+ trap exit TERM INT
+ echo 10.43.47.227
+ grep -Eq :
+ cat /proc/sys/net/ipv4/ip_forward
+ '[' 1 '!=' 1 ]
+ iptables -t nat -I PREROUTING '!' -s 10.43.47.227/32 -p TCP --dport 443 -j DNAT --to 10.43.47.227:443
+ iptables -t nat -I POSTROUTING -d 10.43.47.227/32 -p TCP -j MASQUERADE
+ '[' '!' -e /pause ]
+ mkfifo /pause
==== END logs for container lb-port-443 of pod kube-system/svclb-traefik-74xqk ====
==== START logs for container traefik of pod kube-system/traefik-56c4b88c4b-6lv59 ====
time="2022-04-10T08:07:43Z" level=info msg="Configuration loaded from flags."
==== END logs for container traefik of pod kube-system/traefik-56c4b88c4b-6lv59 ====
==== START logs for container coredns of pod kube-system/coredns-96cc4f57d-qlxzq ====
[WARNING] No files matching import glob pattern: /etc/coredns/custom/*.server
.:53
[WARNING] No files matching import glob pattern: /etc/coredns/custom/*.server
[INFO] plugin/reload: Running configuration MD5 = 442b35f70385f5c97f2491a0ce8a27f6
CoreDNS-1.8.6
linux/amd64, go1.17.1, 13a9191
[WARNING] No files matching import glob pattern: /etc/coredns/custom/*.server
[WARNING] No files matching import glob pattern: /etc/coredns/custom/*.server
[WARNING] No files matching import glob pattern: /etc/coredns/custom/*.server
[WARNING] No files matching import glob pattern: /etc/coredns/custom/*.server
[WARNING] No files matching import glob pattern: /etc/coredns/custom/*.server
[WARNING] No files matching import glob pattern: /etc/coredns/custom/*.server
[WARNING] No files matching import glob pattern: /etc/coredns/custom/*.server
[WARNING] No files matching import glob pattern: /etc/coredns/custom/*.server
[WARNING] No files matching import glob pattern: /etc/coredns/custom/*.server
[WARNING] No files matching import glob pattern: /etc/coredns/custom/*.server
[WARNING] No files matching import glob pattern: /etc/coredns/custom/*.server
[WARNING] No files matching import glob pattern: /etc/coredns/custom/*.server
[WARNING] No files matching import glob pattern: /etc/coredns/custom/*.server
[WARNING] No files matching import glob pattern: /etc/coredns/custom/*.server
[WARNING] No files matching import glob pattern: /etc/coredns/custom/*.server
[WARNING] No files matching import glob pattern: /etc/coredns/custom/*.server
[WARNING] No files matching import glob pattern: /etc/coredns/custom/*.server
[WARNING] No files matching import glob pattern: /etc/coredns/custom/*.server
[WARNING] No files matching import glob pattern: /etc/coredns/custom/*.server
[WARNING] No files matching import glob pattern: /etc/coredns/custom/*.server
[WARNING] No files matching import glob pattern: /etc/coredns/custom/*.server
[WARNING] No files matching import glob pattern: /etc/coredns/custom/*.server
[WARNING] No files matching import glob pattern: /etc/coredns/custom/*.server
[WARNING] No files matching import glob pattern: /etc/coredns/custom/*.server
[WARNING] No files matching import glob pattern: /etc/coredns/custom/*.server
[WARNING] No files matching import glob pattern: /etc/coredns/custom/*.server
[WARNING] No files matching import glob pattern: /etc/coredns/custom/*.server
[WARNING] No files matching import glob pattern: /etc/coredns/custom/*.server
[WARNING] No files matching import glob pattern: /etc/coredns/custom/*.server
[WARNING] No files matching import glob pattern: /etc/coredns/custom/*.server
[WARNING] No files matching import glob pattern: /etc/coredns/custom/*.server
[WARNING] No files matching import glob pattern: /etc/coredns/custom/*.server
[WARNING] No files matching import glob pattern: /etc/coredns/custom/*.server
[WARNING] No files matching import glob pattern: /etc/coredns/custom/*.server
[WARNING] No files matching import glob pattern: /etc/coredns/custom/*.server
[WARNING] No files matching import glob pattern: /etc/coredns/custom/*.server
[WARNING] No files matching import glob pattern: /etc/coredns/custom/*.server
[WARNING] No files matching import glob pattern: /etc/coredns/custom/*.server
[WARNING] No files matching import glob pattern: /etc/coredns/custom/*.server
[WARNING] No files matching import glob pattern: /etc/coredns/custom/*.server
[WARNING] No files matching import glob pattern: /etc/coredns/custom/*.server
[WARNING] No files matching import glob pattern: /etc/coredns/custom/*.server
[WARNING] No files matching import glob pattern: /etc/coredns/custom/*.server
[WARNING] No files matching import glob pattern: /etc/coredns/custom/*.server
[WARNING] No files matching import glob pattern: /etc/coredns/custom/*.server
[WARNING] No files matching import glob pattern: /etc/coredns/custom/*.server
[WARNING] No files matching import glob pattern: /etc/coredns/custom/*.server
[WARNING] No files matching import glob pattern: /etc/coredns/custom/*.server
[WARNING] No files matching import glob pattern: /etc/coredns/custom/*.server
[WARNING] No files matching import glob pattern: /etc/coredns/custom/*.server
[WARNING] No files matching import glob pattern: /etc/coredns/custom/*.server
[WARNING] No files matching import glob pattern: /etc/coredns/custom/*.server
[WARNING] No files matching import glob pattern: /etc/coredns/custom/*.server
[WARNING] No files matching import glob pattern: /etc/coredns/custom/*.server
[WARNING] No files matching import glob pattern: /etc/coredns/custom/*.server
[WARNING] No files matching import glob pattern: /etc/coredns/custom/*.server
[WARNING] No files matching import glob pattern: /etc/coredns/custom/*.server
[WARNING] No files matching import glob pattern: /etc/coredns/custom/*.server
[WARNING] No files matching import glob pattern: /etc/coredns/custom/*.server
[WARNING] No files matching import glob pattern: /etc/coredns/custom/*.server
[WARNING] No files matching import glob pattern: /etc/coredns/custom/*.server
[WARNING] No files matching import glob pattern: /etc/coredns/custom/*.server
[WARNING] No files matching import glob pattern: /etc/coredns/custom/*.server
[WARNING] No files matching import glob pattern: /etc/coredns/custom/*.server
[WARNING] No files matching import glob pattern: /etc/coredns/custom/*.server
[WARNING] No files matching import glob pattern: /etc/coredns/custom/*.server
[WARNING] No files matching import glob pattern: /etc/coredns/custom/*.server
[WARNING] No files matching import glob pattern: /etc/coredns/custom/*.server
[WARNING] No files matching import glob pattern: /etc/coredns/custom/*.server
[WARNING] No files matching import glob pattern: /etc/coredns/custom/*.server
[WARNING] No files matching import glob pattern: /etc/coredns/custom/*.server
[WARNING] No files matching import glob pattern: /etc/coredns/custom/*.server
[WARNING] No files matching import glob pattern: /etc/coredns/custom/*.server
[WARNING] No files matching import glob pattern: /etc/coredns/custom/*.server
[WARNING] No files matching import glob pattern: /etc/coredns/custom/*.server
[WARNING] No files matching import glob pattern: /etc/coredns/custom/*.server
[WARNING] No files matching import glob pattern: /etc/coredns/custom/*.server
[WARNING] No files matching import glob pattern: /etc/coredns/custom/*.server
[WARNING] No files matching import glob pattern: /etc/coredns/custom/*.server
[WARNING] No files matching import glob pattern: /etc/coredns/custom/*.server
[WARNING] No files matching import glob pattern: /etc/coredns/custom/*.server
[WARNING] No files matching import glob pattern: /etc/coredns/custom/*.server
[WARNING] No files matching import glob pattern: /etc/coredns/custom/*.server
[WARNING] No files matching import glob pattern: /etc/coredns/custom/*.server
[WARNING] No files matching import glob pattern: /etc/coredns/custom/*.server
[WARNING] No files matching import glob pattern: /etc/coredns/custom/*.server
[WARNING] No files matching import glob pattern: /etc/coredns/custom/*.server
[WARNING] No files matching import glob pattern: /etc/coredns/custom/*.server
[WARNING] No files matching import glob pattern: /etc/coredns/custom/*.server
[WARNING] No files matching import glob pattern: /etc/coredns/custom/*.server
[WARNING] No files matching import glob pattern: /etc/coredns/custom/*.server
[WARNING] No files matching import glob pattern: /etc/coredns/custom/*.server
[WARNING] No files matching import glob pattern: /etc/coredns/custom/*.server
[WARNING] No files matching import glob pattern: /etc/coredns/custom/*.server
[WARNING] No files matching import glob pattern: /etc/coredns/custom/*.server
[WARNING] No files matching import glob pattern: /etc/coredns/custom/*.server
[WARNING] No files matching import glob pattern: /etc/coredns/custom/*.server
[WARNING] No files matching import glob pattern: /etc/coredns/custom/*.server
[WARNING] No files matching import glob pattern: /etc/coredns/custom/*.server
[WARNING] No files matching import glob pattern: /etc/coredns/custom/*.server
[WARNING] No files matching import glob pattern: /etc/coredns/custom/*.server
[WARNING] No files matching import glob pattern: /etc/coredns/custom/*.server
[WARNING] No files matching import glob pattern: /etc/coredns/custom/*.server
[WARNING] No files matching import glob pattern: /etc/coredns/custom/*.server
[WARNING] No files matching import glob pattern: /etc/coredns/custom/*.server
[WARNING] No files matching import glob pattern: /etc/coredns/custom/*.server
[WARNING] No files matching import glob pattern: /etc/coredns/custom/*.server
[WARNING] No files matching import glob pattern: /etc/coredns/custom/*.server
[WARNING] No files matching import glob pattern: /etc/coredns/custom/*.server
[WARNING] No files matching import glob pattern: /etc/coredns/custom/*.server
[WARNING] No files matching import glob pattern: /etc/coredns/custom/*.server
[WARNING] No files matching import glob pattern: /etc/coredns/custom/*.server
[WARNING] No files matching import glob pattern: /etc/coredns/custom/*.server
[WARNING] No files matching import glob pattern: /etc/coredns/custom/*.server
[WARNING] No files matching import glob pattern: /etc/coredns/custom/*.server
[WARNING] No files matching import glob pattern: /etc/coredns/custom/*.server
[WARNING] No files matching import glob pattern: /etc/coredns/custom/*.server
[WARNING] No files matching import glob pattern: /etc/coredns/custom/*.server
[WARNING] No files matching import glob pattern: /etc/coredns/custom/*.server
[WARNING] No files matching import glob pattern: /etc/coredns/custom/*.server
[WARNING] No files matching import glob pattern: /etc/coredns/custom/*.server
[WARNING] No files matching import glob pattern: /etc/coredns/custom/*.server
[WARNING] No files matching import glob pattern: /etc/coredns/custom/*.server
[WARNING] No files matching import glob pattern: /etc/coredns/custom/*.server
[WARNING] No files matching import glob pattern: /etc/coredns/custom/*.server
[WARNING] No files matching import glob pattern: /etc/coredns/custom/*.server
[WARNING] No files matching import glob pattern: /etc/coredns/custom/*.server
[WARNING] No files matching import glob pattern: /etc/coredns/custom/*.server
[WARNING] No files matching import glob pattern: /etc/coredns/custom/*.server
[WARNING] No files matching import glob pattern: /etc/coredns/custom/*.server
[WARNING] No files matching import glob pattern: /etc/coredns/custom/*.server
[WARNING] No files matching import glob pattern: /etc/coredns/custom/*.server
[WARNING] No files matching import glob pattern: /etc/coredns/custom/*.server
[WARNING] No files matching import glob pattern: /etc/coredns/custom/*.server
[WARNING] No files matching import glob pattern: /etc/coredns/custom/*.server
[WARNING] No files matching import glob pattern: /etc/coredns/custom/*.server
[WARNING] No files matching import glob pattern: /etc/coredns/custom/*.server
[WARNING] No files matching import glob pattern: /etc/coredns/custom/*.server
[WARNING] No files matching import glob pattern: /etc/coredns/custom/*.server
[WARNING] No files matching import glob pattern: /etc/coredns/custom/*.server
[WARNING] No files matching import glob pattern: /etc/coredns/custom/*.server
[WARNING] No files matching import glob pattern: /etc/coredns/custom/*.server
[WARNING] No files matching import glob pattern: /etc/coredns/custom/*.server
[WARNING] No files matching import glob pattern: /etc/coredns/custom/*.server
[WARNING] No files matching import glob pattern: /etc/coredns/custom/*.server
[WARNING] No files matching import glob pattern: /etc/coredns/custom/*.server
[WARNING] No files matching import glob pattern: /etc/coredns/custom/*.server
[WARNING] No files matching import glob pattern: /etc/coredns/custom/*.server
[WARNING] No files matching import glob pattern: /etc/coredns/custom/*.server
[WARNING] No files matching import glob pattern: /etc/coredns/custom/*.server
[WARNING] No files matching import glob pattern: /etc/coredns/custom/*.server
[WARNING] No files matching import glob pattern: /etc/coredns/custom/*.server
[WARNING] No files matching import glob pattern: /etc/coredns/custom/*.server
[WARNING] No files matching import glob pattern: /etc/coredns/custom/*.server
[WARNING] No files matching import glob pattern: /etc/coredns/custom/*.server
[WARNING] No files matching import glob pattern: /etc/coredns/custom/*.server
[WARNING] No files matching import glob pattern: /etc/coredns/custom/*.server
[WARNING] No files matching import glob pattern: /etc/coredns/custom/*.server
[WARNING] No files matching import glob pattern: /etc/coredns/custom/*.server
[WARNING] No files matching import glob pattern: /etc/coredns/custom/*.server
[WARNING] No files matching import glob pattern: /etc/coredns/custom/*.server
[WARNING] No files matching import glob pattern: /etc/coredns/custom/*.server
[WARNING] No files matching import glob pattern: /etc/coredns/custom/*.server
[WARNING] No files matching import glob pattern: /etc/coredns/custom/*.server
[WARNING] No files matching import glob pattern: /etc/coredns/custom/*.server
[WARNING] No files matching import glob pattern: /etc/coredns/custom/*.server
[WARNING] No files matching import glob pattern: /etc/coredns/custom/*.server
[WARNING] No files matching import glob pattern: /etc/coredns/custom/*.server
[WARNING] No files matching import glob pattern: /etc/coredns/custom/*.server
[WARNING] No files matching import glob pattern: /etc/coredns/custom/*.server
[WARNING] No files matching import glob pattern: /etc/coredns/custom/*.server
[WARNING] No files matching import glob pattern: /etc/coredns/custom/*.server
[WARNING] No files matching import glob pattern: /etc/coredns/custom/*.server
[WARNING] No files matching import glob pattern: /etc/coredns/custom/*.server
[WARNING] No files matching import glob pattern: /etc/coredns/custom/*.server
[WARNING] No files matching import glob pattern: /etc/coredns/custom/*.server
[WARNING] No files matching import glob pattern: /etc/coredns/custom/*.server
[WARNING] No files matching import glob pattern: /etc/coredns/custom/*.server
[WARNING] No files matching import glob pattern: /etc/coredns/custom/*.server
[WARNING] No files matching import glob pattern: /etc/coredns/custom/*.server
[WARNING] No files matching import glob pattern: /etc/coredns/custom/*.server
[WARNING] No files matching import glob pattern: /etc/coredns/custom/*.server
[WARNING] No files matching import glob pattern: /etc/coredns/custom/*.server
[WARNING] No files matching import glob pattern: /etc/coredns/custom/*.server
[WARNING] No files matching import glob pattern: /etc/coredns/custom/*.server
[WARNING] No files matching import glob pattern: /etc/coredns/custom/*.server
[WARNING] No files matching import glob pattern: /etc/coredns/custom/*.server
[WARNING] No files matching import glob pattern: /etc/coredns/custom/*.server
[WARNING] No files matching import glob pattern: /etc/coredns/custom/*.server
[WARNING] No files matching import glob pattern: /etc/coredns/custom/*.server
[WARNING] No files matching import glob pattern: /etc/coredns/custom/*.server
[WARNING] No files matching import glob pattern: /etc/coredns/custom/*.server
[WARNING] No files matching import glob pattern: /etc/coredns/custom/*.server
[WARNING] No files matching import glob pattern: /etc/coredns/custom/*.server
[WARNING] No files matching import glob pattern: /etc/coredns/custom/*.server
[WARNING] No files matching import glob pattern: /etc/coredns/custom/*.server
[WARNING] No files matching import glob pattern: /etc/coredns/custom/*.server
[WARNING] No files matching import glob pattern: /etc/coredns/custom/*.server
[WARNING] No files matching import glob pattern: /etc/coredns/custom/*.server
[WARNING] No files matching import glob pattern: /etc/coredns/custom/*.server
[WARNING] No files matching import glob pattern: /etc/coredns/custom/*.server
[WARNING] No files matching import glob pattern: /etc/coredns/custom/*.server
[WARNING] No files matching import glob pattern: /etc/coredns/custom/*.server
[WARNING] No files matching import glob pattern: /etc/coredns/custom/*.server
[WARNING] No files matching import glob pattern: /etc/coredns/custom/*.server
[WARNING] No files matching import glob pattern: /etc/coredns/custom/*.server
[WARNING] No files matching import glob pattern: /etc/coredns/custom/*.server
[WARNING] No files matching import glob pattern: /etc/coredns/custom/*.server
[WARNING] No files matching import glob pattern: /etc/coredns/custom/*.server
[WARNING] No files matching import glob pattern: /etc/coredns/custom/*.server
[WARNING] No files matching import glob pattern: /etc/coredns/custom/*.server
[WARNING] No files matching import glob pattern: /etc/coredns/custom/*.server
[WARNING] No files matching import glob pattern: /etc/coredns/custom/*.server
[WARNING] No files matching import glob pattern: /etc/coredns/custom/*.server
[WARNING] No files matching import glob pattern: /etc/coredns/custom/*.server
[WARNING] No files matching import glob pattern: /etc/coredns/custom/*.server
[WARNING] No files matching import glob pattern: /etc/coredns/custom/*.server
[WARNING] No files matching import glob pattern: /etc/coredns/custom/*.server
[WARNING] No files matching import glob pattern: /etc/coredns/custom/*.server
[WARNING] No files matching import glob pattern: /etc/coredns/custom/*.server
[WARNING] No files matching import glob pattern: /etc/coredns/custom/*.server
[WARNING] No files matching import glob pattern: /etc/coredns/custom/*.server
[WARNING] No files matching import glob pattern: /etc/coredns/custom/*.server
[WARNING] No files matching import glob pattern: /etc/coredns/custom/*.server
[WARNING] No files matching import glob pattern: /etc/coredns/custom/*.server
[WARNING] No files matching import glob pattern: /etc/coredns/custom/*.server
==== END logs for container coredns of pod kube-system/coredns-96cc4f57d-qlxzq ====
{
    "kind": "EventList",
    "apiVersion": "v1",
    "metadata": {
        "resourceVersion": "20940"
    },
    "items": [
        {
            "metadata": {
                "name": "dhana.16e48079b922c7f8",
                "namespace": "default",
                "uid": "c9697f5f-e342-4137-8783-639be5b50a88",
                "resourceVersion": "20671",
                "creationTimestamp": "2022-04-10T09:57:03Z"
            },
            "involvedObject": {
                "kind": "Deployment",
                "namespace": "default",
                "name": "dhana",
                "uid": "a8f03198-3545-471b-8a4d-3027b165d82d",
                "apiVersion": "apps/v1",
                "resourceVersion": "20668"
            },
            "reason": "ScalingReplicaSet",
            "message": "Scaled up replica set dhana-666f9cc654 to 1",
            "source": {
                "component": "deployment-controller"
            },
            "firstTimestamp": "2022-04-10T09:57:03Z",
            "lastTimestamp": "2022-04-10T09:57:03Z",
            "count": 1,
            "type": "Normal",
            "eventTime": null,
            "reportingComponent": "",
            "reportingInstance": ""
        },
        {
            "metadata": {
                "name": "dhana-666f9cc654.16e48079c2bbf598",
                "namespace": "default",
                "uid": "e59ec41a-4f93-4628-b2fe-a56d94edb4a5",
                "resourceVersion": "20674",
                "creationTimestamp": "2022-04-10T09:57:03Z"
            },
            "involvedObject": {
                "kind": "ReplicaSet",
                "namespace": "default",
                "name": "dhana-666f9cc654",
                "uid": "66cbd24b-e6c1-47c2-88bf-92d6cf243113",
                "apiVersion": "apps/v1",
                "resourceVersion": "20669"
            },
            "reason": "SuccessfulCreate",
            "message": "Created pod: dhana-666f9cc654-bfdvx",
            "source": {
                "component": "replicaset-controller"
            },
            "firstTimestamp": "2022-04-10T09:57:03Z",
            "lastTimestamp": "2022-04-10T09:57:03Z",
            "count": 1,
            "type": "Normal",
            "eventTime": null,
            "reportingComponent": "",
            "reportingInstance": ""
        },
        {
            "metadata": {
                "name": "dhana-666f9cc654-bfdvx.16e48079c3b929bb",
                "namespace": "default",
                "uid": "628baee0-a34a-4fac-ba7e-08a8bd16bbc3",
                "resourceVersion": "20677",
                "creationTimestamp": "2022-04-10T09:57:03Z"
            },
            "involvedObject": {
                "kind": "Pod",
                "namespace": "default",
                "name": "dhana-666f9cc654-bfdvx",
                "uid": "a9a868ff-7c7b-4dd9-a7da-ce34e93e4b77",
                "apiVersion": "v1",
                "resourceVersion": "20673"
            },
            "reason": "Scheduled",
            "message": "Successfully assigned default/dhana-666f9cc654-bfdvx to localhost.localdomain",
            "source": {},
            "firstTimestamp": null,
            "lastTimestamp": null,
            "type": "Normal",
            "eventTime": "2022-04-10T09:57:03.987529Z",
            "action": "Binding",
            "reportingComponent": "default-scheduler",
            "reportingInstance": "default-scheduler-localhost.localdomain"
        },
        {
            "metadata": {
                "name": "dhana-666f9cc654-bfdvx.16e48079f44383b1",
                "namespace": "default",
                "uid": "9536c7e4-d9d5-42ea-9697-3160c6f8f782",
                "resourceVersion": "20681",
                "creationTimestamp": "2022-04-10T09:57:04Z"
            },
            "involvedObject": {
                "kind": "Pod",
                "namespace": "default",
                "name": "dhana-666f9cc654-bfdvx",
                "uid": "a9a868ff-7c7b-4dd9-a7da-ce34e93e4b77",
                "apiVersion": "v1",
                "resourceVersion": "20676",
                "fieldPath": "spec.containers{nginx}"
            },
            "reason": "Pulling",
            "message": "Pulling image \"nginx\"",
            "source": {
                "component": "kubelet",
                "host": "localhost.localdomain"
            },
            "firstTimestamp": "2022-04-10T09:57:04Z",
            "lastTimestamp": "2022-04-10T09:57:04Z",
            "count": 1,
            "type": "Normal",
            "eventTime": null,
            "reportingComponent": "",
            "reportingInstance": ""
        },
        {
            "metadata": {
                "name": "dhana-666f9cc654-bfdvx.16e4808410f431f3",
                "namespace": "default",
                "uid": "a456ec1c-ff3f-4d55-a7c2-f7ad4bc2b7ec",
                "resourceVersion": "20691",
                "creationTimestamp": "2022-04-10T09:57:48Z"
            },
            "involvedObject": {
                "kind": "Pod",
                "namespace": "default",
                "name": "dhana-666f9cc654-bfdvx",
                "uid": "a9a868ff-7c7b-4dd9-a7da-ce34e93e4b77",
                "apiVersion": "v1",
                "resourceVersion": "20676",
                "fieldPath": "spec.containers{nginx}"
            },
            "reason": "Pulled",
            "message": "Successfully pulled image \"nginx\" in 43.430998018s",
            "source": {
                "component": "kubelet",
                "host": "localhost.localdomain"
            },
            "firstTimestamp": "2022-04-10T09:57:48Z",
            "lastTimestamp": "2022-04-10T09:57:48Z",
            "count": 1,
            "type": "Normal",
            "eventTime": null,
            "reportingComponent": "",
            "reportingInstance": ""
        },
        {
            "metadata": {
                "name": "dhana-666f9cc654-bfdvx.16e48084854f906a",
                "namespace": "default",
                "uid": "31900873-3b69-4f1f-a5e0-26a19359e4a5",
                "resourceVersion": "20693",
                "creationTimestamp": "2022-04-10T09:57:50Z"
            },
            "involvedObject": {
                "kind": "Pod",
                "namespace": "default",
                "name": "dhana-666f9cc654-bfdvx",
                "uid": "a9a868ff-7c7b-4dd9-a7da-ce34e93e4b77",
                "apiVersion": "v1",
                "resourceVersion": "20676",
                "fieldPath": "spec.containers{nginx}"
            },
            "reason": "Created",
            "message": "Created container nginx",
            "source": {
                "component": "kubelet",
                "host": "localhost.localdomain"
            },
            "firstTimestamp": "2022-04-10T09:57:50Z",
            "lastTimestamp": "2022-04-10T09:57:50Z",
            "count": 1,
            "type": "Normal",
            "eventTime": null,
            "reportingComponent": "",
            "reportingInstance": ""
        },
        {
            "metadata": {
                "name": "dhana-666f9cc654-bfdvx.16e480849d65f657",
                "namespace": "default",
                "uid": "736873c9-ea80-4d8d-afa1-01c070b0a7db",
                "resourceVersion": "20694",
                "creationTimestamp": "2022-04-10T09:57:50Z"
            },
            "involvedObject": {
                "kind": "Pod",
                "namespace": "default",
                "name": "dhana-666f9cc654-bfdvx",
                "uid": "a9a868ff-7c7b-4dd9-a7da-ce34e93e4b77",
                "apiVersion": "v1",
                "resourceVersion": "20676",
                "fieldPath": "spec.containers{nginx}"
            },
            "reason": "Started",
            "message": "Started container nginx",
            "source": {
                "component": "kubelet",
                "host": "localhost.localdomain"
            },
            "firstTimestamp": "2022-04-10T09:57:50Z",
            "lastTimestamp": "2022-04-10T09:57:50Z",
            "count": 1,
            "type": "Normal",
            "eventTime": null,
            "reportingComponent": "",
            "reportingInstance": ""
        }
    ]
}
{
    "kind": "ReplicationControllerList",
    "apiVersion": "v1",
    "metadata": {
        "resourceVersion": "20940"
    },
    "items": []
}
{
    "kind": "ServiceList",
    "apiVersion": "v1",
    "metadata": {
        "resourceVersion": "20940"
    },
    "items": [
        {
            "metadata": {
                "name": "kubernetes",
                "namespace": "default",
                "uid": "68ac09c0-68c3-4f47-acd7-fa7bf29abeda",
                "resourceVersion": "216",
                "creationTimestamp": "2022-03-22T17:40:08Z",
                "labels": {
                    "component": "apiserver",
                    "provider": "kubernetes"
                }
            },
            "spec": {
                "ports": [
                    {
                        "name": "https",
                        "protocol": "TCP",
                        "port": 443,
                        "targetPort": 6443
                    }
                ],
                "clusterIP": "10.43.0.1",
                "clusterIPs": [
                    "10.43.0.1"
                ],
                "type": "ClusterIP",
                "sessionAffinity": "None",
                "ipFamilies": [
                    "IPv4"
                ],
                "ipFamilyPolicy": "SingleStack",
                "internalTrafficPolicy": "Cluster"
            },
            "status": {
                "loadBalancer": {}
            }
        }
    ]
}
{
    "kind": "DaemonSetList",
    "apiVersion": "apps/v1",
    "metadata": {
        "resourceVersion": "20940"
    },
    "items": []
}
{
    "kind": "DeploymentList",
    "apiVersion": "apps/v1",
    "metadata": {
        "resourceVersion": "20940"
    },
    "items": [
        {
            "metadata": {
                "name": "dhana",
                "namespace": "default",
                "uid": "a8f03198-3545-471b-8a4d-3027b165d82d",
                "resourceVersion": "20697",
                "generation": 1,
                "creationTimestamp": "2022-04-10T09:57:03Z",
                "labels": {
                    "app": "dhana"
                },
                "annotations": {
                    "deployment.kubernetes.io/revision": "1"
                }
            },
            "spec": {
                "replicas": 1,
                "selector": {
                    "matchLabels": {
                        "app": "dhana"
                    }
                },
                "template": {
                    "metadata": {
                        "creationTimestamp": null,
                        "labels": {
                            "app": "dhana"
                        }
                    },
                    "spec": {
                        "containers": [
                            {
                                "name": "nginx",
                                "image": "nginx",
                                "resources": {},
                                "terminationMessagePath": "/dev/termination-log",
                                "terminationMessagePolicy": "File",
                                "imagePullPolicy": "Always"
                            }
                        ],
                        "restartPolicy": "Always",
                        "terminationGracePeriodSeconds": 30,
                        "dnsPolicy": "ClusterFirst",
                        "securityContext": {},
                        "schedulerName": "default-scheduler"
                    }
                },
                "strategy": {
                    "type": "RollingUpdate",
                    "rollingUpdate": {
                        "maxUnavailable": "25%",
                        "maxSurge": "25%"
                    }
                },
                "revisionHistoryLimit": 10,
                "progressDeadlineSeconds": 600
            },
            "status": {
                "observedGeneration": 1,
                "replicas": 1,
                "updatedReplicas": 1,
                "readyReplicas": 1,
                "availableReplicas": 1,
                "conditions": [
                    {
                        "type": "Available",
                        "status": "True",
                        "lastUpdateTime": "2022-04-10T09:57:51Z",
                        "lastTransitionTime": "2022-04-10T09:57:51Z",
                        "reason": "MinimumReplicasAvailable",
                        "message": "Deployment has minimum availability."
                    },
                    {
                        "type": "Progressing",
                        "status": "True",
                        "lastUpdateTime": "2022-04-10T09:57:51Z",
                        "lastTransitionTime": "2022-04-10T09:57:03Z",
                        "reason": "NewReplicaSetAvailable",
                        "message": "ReplicaSet \"dhana-666f9cc654\" has successfully progressed."
                    }
                ]
            }
        }
    ]
}
{
    "kind": "ReplicaSetList",
    "apiVersion": "apps/v1",
    "metadata": {
        "resourceVersion": "20940"
    },
    "items": [
        {
            "metadata": {
                "name": "dhana-666f9cc654",
                "namespace": "default",
                "uid": "66cbd24b-e6c1-47c2-88bf-92d6cf243113",
                "resourceVersion": "20696",
                "generation": 1,
                "creationTimestamp": "2022-04-10T09:57:03Z",
                "labels": {
                    "app": "dhana",
                    "pod-template-hash": "666f9cc654"
                },
                "annotations": {
                    "deployment.kubernetes.io/desired-replicas": "1",
                    "deployment.kubernetes.io/max-replicas": "2",
                    "deployment.kubernetes.io/revision": "1"
                },
                "ownerReferences": [
                    {
                        "apiVersion": "apps/v1",
                        "kind": "Deployment",
                        "name": "dhana",
                        "uid": "a8f03198-3545-471b-8a4d-3027b165d82d",
                        "controller": true,
                        "blockOwnerDeletion": true
                    }
                ]
            },
            "spec": {
                "replicas": 1,
                "selector": {
                    "matchLabels": {
                        "app": "dhana",
                        "pod-template-hash": "666f9cc654"
                    }
                },
                "template": {
                    "metadata": {
                        "creationTimestamp": null,
                        "labels": {
                            "app": "dhana",
                            "pod-template-hash": "666f9cc654"
                        }
                    },
                    "spec": {
                        "containers": [
                            {
                                "name": "nginx",
                                "image": "nginx",
                                "resources": {},
                                "terminationMessagePath": "/dev/termination-log",
                                "terminationMessagePolicy": "File",
                                "imagePullPolicy": "Always"
                            }
                        ],
                        "restartPolicy": "Always",
                        "terminationGracePeriodSeconds": 30,
                        "dnsPolicy": "ClusterFirst",
                        "securityContext": {},
                        "schedulerName": "default-scheduler"
                    }
                }
            },
            "status": {
                "replicas": 1,
                "fullyLabeledReplicas": 1,
                "readyReplicas": 1,
                "availableReplicas": 1,
                "observedGeneration": 1
            }
        }
    ]
}
{
    "kind": "PodList",
    "apiVersion": "v1",
    "metadata": {
        "resourceVersion": "20940"
    },
    "items": [
        {
            "metadata": {
                "name": "dhana-666f9cc654-bfdvx",
                "generateName": "dhana-666f9cc654-",
                "namespace": "default",
                "uid": "a9a868ff-7c7b-4dd9-a7da-ce34e93e4b77",
                "resourceVersion": "20695",
                "creationTimestamp": "2022-04-10T09:57:03Z",
                "labels": {
                    "app": "dhana",
                    "pod-template-hash": "666f9cc654"
                },
                "ownerReferences": [
                    {
                        "apiVersion": "apps/v1",
                        "kind": "ReplicaSet",
                        "name": "dhana-666f9cc654",
                        "uid": "66cbd24b-e6c1-47c2-88bf-92d6cf243113",
                        "controller": true,
                        "blockOwnerDeletion": true
                    }
                ]
            },
            "spec": {
                "volumes": [
                    {
                        "name": "kube-api-access-j9zb6",
                        "projected": {
                            "sources": [
                                {
                                    "serviceAccountToken": {
                                        "expirationSeconds": 3607,
                                        "path": "token"
                                    }
                                },
                                {
                                    "configMap": {
                                        "name": "kube-root-ca.crt",
                                        "items": [
                                            {
                                                "key": "ca.crt",
                                                "path": "ca.crt"
                                            }
                                        ]
                                    }
                                },
                                {
                                    "downwardAPI": {
                                        "items": [
                                            {
                                                "path": "namespace",
                                                "fieldRef": {
                                                    "apiVersion": "v1",
                                                    "fieldPath": "metadata.namespace"
                                                }
                                            }
                                        ]
                                    }
                                }
                            ],
                            "defaultMode": 420
                        }
                    }
                ],
                "containers": [
                    {
                        "name": "nginx",
                        "image": "nginx",
                        "resources": {},
                        "volumeMounts": [
                            {
                                "name": "kube-api-access-j9zb6",
                                "readOnly": true,
                                "mountPath": "/var/run/secrets/kubernetes.io/serviceaccount"
                            }
                        ],
                        "terminationMessagePath": "/dev/termination-log",
                        "terminationMessagePolicy": "File",
                        "imagePullPolicy": "Always"
                    }
                ],
                "restartPolicy": "Always",
                "terminationGracePeriodSeconds": 30,
                "dnsPolicy": "ClusterFirst",
                "serviceAccountName": "default",
                "serviceAccount": "default",
                "nodeName": "localhost.localdomain",
                "securityContext": {},
                "schedulerName": "default-scheduler",
                "tolerations": [
                    {
                        "key": "node.kubernetes.io/not-ready",
                        "operator": "Exists",
                        "effect": "NoExecute",
                        "tolerationSeconds": 300
                    },
                    {
                        "key": "node.kubernetes.io/unreachable",
                        "operator": "Exists",
                        "effect": "NoExecute",
                        "tolerationSeconds": 300
                    }
                ],
                "priority": 0,
                "enableServiceLinks": true,
                "preemptionPolicy": "PreemptLowerPriority"
            },
            "status": {
                "phase": "Running",
                "conditions": [
                    {
                        "type": "Initialized",
                        "status": "True",
                        "lastProbeTime": null,
                        "lastTransitionTime": "2022-04-10T09:57:03Z"
                    },
                    {
                        "type": "Ready",
                        "status": "True",
                        "lastProbeTime": null,
                        "lastTransitionTime": "2022-04-10T09:57:51Z"
                    },
                    {
                        "type": "ContainersReady",
                        "status": "True",
                        "lastProbeTime": null,
                        "lastTransitionTime": "2022-04-10T09:57:51Z"
                    },
                    {
                        "type": "PodScheduled",
                        "status": "True",
                        "lastProbeTime": null,
                        "lastTransitionTime": "2022-04-10T09:57:03Z"
                    }
                ],
                "hostIP": "192.168.1.30",
                "podIP": "10.42.0.83",
                "podIPs": [
                    {
                        "ip": "10.42.0.83"
                    }
                ],
                "startTime": "2022-04-10T09:57:03Z",
                "containerStatuses": [
                    {
                        "name": "nginx",
                        "state": {
                            "running": {
                                "startedAt": "2022-04-10T09:57:50Z"
                            }
                        },
                        "lastState": {},
                        "ready": true,
                        "restartCount": 0,
                        "image": "docker.io/library/nginx:latest",
                        "imageID": "docker.io/library/nginx@sha256:2275af0f20d71b293916f1958f8497f987b8d8fd8113df54635f2a5915002bf1",
                        "containerID": "containerd://9d13517996b7f50c4c961e5242e4cf2780516d441f24589be16551d2d9102c2a",
                        "started": true
                    }
                ],
                "qosClass": "BestEffort"
            }
        }
    ]
}
==== START logs for container nginx of pod default/dhana-666f9cc654-bfdvx ====
/docker-entrypoint.sh: /docker-entrypoint.d/ is not empty, will attempt to perform configuration
/docker-entrypoint.sh: Looking for shell scripts in /docker-entrypoint.d/
/docker-entrypoint.sh: Launching /docker-entrypoint.d/10-listen-on-ipv6-by-default.sh
10-listen-on-ipv6-by-default.sh: info: Getting the checksum of /etc/nginx/conf.d/default.conf
10-listen-on-ipv6-by-default.sh: info: Enabled listen on IPv6 in /etc/nginx/conf.d/default.conf
/docker-entrypoint.sh: Launching /docker-entrypoint.d/20-envsubst-on-templates.sh
/docker-entrypoint.sh: Launching /docker-entrypoint.d/30-tune-worker-processes.sh
/docker-entrypoint.sh: Configuration complete; ready for start up
2022/04/10 09:57:50 [notice] 1#1: using the "epoll" event method
2022/04/10 09:57:50 [notice] 1#1: nginx/1.21.6
2022/04/10 09:57:50 [notice] 1#1: built by gcc 10.2.1 20210110 (Debian 10.2.1-6) 
2022/04/10 09:57:50 [notice] 1#1: OS: Linux 3.10.0-1160.el7.x86_64
2022/04/10 09:57:50 [notice] 1#1: getrlimit(RLIMIT_NOFILE): 1048576:1048576
2022/04/10 09:57:50 [notice] 1#1: start worker processes
2022/04/10 09:57:50 [notice] 1#1: start worker process 31
==== END logs for container nginx of pod default/dhana-666f9cc654-bfdvx ====
]0;root@localhost:~[root@localhost ~]# kubectl cluster-info dump[K[K[K[K[K
[0;32mKubernetes control plane[0m is running at [0;33mhttps://127.0.0.1:6443[0m
[0;32mCoreDNS[0m is running at [0;33mhttps://127.0.0.1:6443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy[0m
[0;32mMetrics-server[0m is running at [0;33mhttps://127.0.0.1:6443/api/v1/namespaces/kube-system/services/https:metrics-server:https/proxy[0m

To further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.
]0;root@localhost:~[root@localhost ~]# kubectl get ns
NAME              STATUS   AGE
default           Active   18d
kube-system       Active   18d
kube-public       Active   18d
kube-node-lease   Active   18d
ame               Active   41h
]0;root@localhost:~[root@localhost ~]# kubectl top node or pods
error: node [NAME | -l label]
See 'kubectl top node -h' for help and examples
]0;root@localhost:~[root@localhost ~]# kubectl top node or pods[C[C[2P pods[5P pods
NAME                     CPU(cores)   MEMORY(bytes)   
dhana-666f9cc654-bfdvx   0m           1Mi             
]0;root@localhost:~[root@localhost ~]# kubectl top  pods[K
NAME                     CPU(cores)   MEMORY(bytes)   
dhana-666f9cc654-bfdvx   0m           1Mi             
]0;root@localhost:~[root@localhost ~]# kubectl scale –replica=3 tomcat.yaml[K[K[K[K[K[K[K[K[K[K[K[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[1@#
]0;root@localhost:~[root@localhost ~]# ll
total 164
-rw-------.  1 root root  2227 Mar 21 19:14 anaconda-ks.cfg
-rw-r--r--.  1 root root  1565 Apr 10 15:28 deploy-nginx.yml
-rwx------.  1 root root 11156 Mar 23 22:44 [0m[01;32mget_helm.sh[0m
-rw-r--r--.  1 root root  2320 Mar 21 19:43 initial-setup-ks.cfg
drwxr-xr-x. 10 root root  4096 Apr 10 13:59 [01;34mkubectx[0m
-rw-r--r--.  1 root root 93490 Mar 22 23:05 kube-prometheus-stack.yml
drwxr-xr-x. 31 root root  4096 Mar 21 21:21 [01;34mkubernetes[0m
drwxr-xr-x.  9 root root  4096 Mar 21 14:30 [01;34mlocal-repo[0m
-rw-r--r--.  1 root root  7565 Apr  8 21:00 Metricbeat-kube-not.yml
-rw-r--r--.  1 root root   104 Mar 20 14:31 mongo-configmap.yaml
-rw-r--r--.  1 root root  1114 Mar 20 14:32 mongo-express.yaml
-rw-r--r--.  1 root root   158 Mar 20 14:32 mongo-secret.yaml
-rw-r--r--.  1 root root   862 Mar 20 14:31 mongo.yaml
drwxr-xr-x.  4 root root  4096 Mar 21 21:26 [01;34mprome-grafana[0m
drwxr-xr-x.  4 root root  4096 Mar 21 22:01 [01;34mPrometheus-Grafana[0m
-rw-r--r--.  1 root root  4077 Apr  8 22:23 values-metricbeat.yaml
]0;root@localhost:~[root@localhost ~]# ll#kubectl scale –replica=3  deploy-nginx.yml[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[1P
Error: required flag(s) "replicas" not set
required flag(s) "replicas" not set
]0;root@localhost:~[root@localhost ~]# kubectl scale –replica=3  deploy-nginx.yml[C[1@s
Error: required flag(s) "replicas" not set
required flag(s) "replicas" not set
]0;root@localhost:~[root@localhost ~]# kubectl scale –replicas=3  deploy-nginx.yml[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[K[Kscale --help
Set a new size for a deployment, replica set, replication controller, or stateful set.

 Scale also allows users to specify one or more preconditions for the scale action.

 If --current-replicas or --resource-version is specified, it is validated before the scale is attempted, and it is
guaranteed that the precondition holds true when the scale is sent to the server.

Examples:
  # Scale a replica set named 'foo' to 3
  kubectl scale --replicas=3 rs/foo
  
  # Scale a resource identified by type and name specified in "foo.yaml" to 3
  kubectl scale --replicas=3 -f foo.yaml
  
  # If the deployment named mysql's current size is 2, scale mysql to 3
  kubectl scale --current-replicas=2 --replicas=3 deployment/mysql
  
  # Scale multiple replication controllers
  kubectl scale --replicas=5 rc/foo rc/bar rc/baz
  
  # Scale stateful set named 'web' to 3
  kubectl scale --replicas=3 statefulset/web

Options:
      --all=false: Select all resources in the namespace of the specified resource types
      --allow-missing-template-keys=true: If true, ignore any errors in templates when a field or map key is missing in
the template. Only applies to golang and jsonpath output formats.
      --current-replicas=-1: Precondition for current size. Requires that the current size of the resource match this
value in order to scale. -1 (default) for no condition.
      --dry-run='none': Must be "none", "server", or "client". If client strategy, only print the object that would be
sent, without sending it. If server strategy, submit server-side request without persisting the resource.
  -f, --filename=[]: Filename, directory, or URL to files identifying the resource to set a new size
  -k, --kustomize='': Process the kustomization directory. This flag can't be used together with -f or -R.
  -o, --output='': Output format. One of:
json|yaml|name|go-template|go-template-file|template|templatefile|jsonpath|jsonpath-as-json|jsonpath-file.
  -R, --recursive=false: Process the directory used in -f, --filename recursively. Useful when you want to manage
related manifests organized within the same directory.
      --replicas=0: The new desired number of replicas. Required.
      --resource-version='': Precondition for resource version. Requires that the current resource version match this
value in order to scale.
  -l, --selector='': Selector (label query) to filter on, supports '=', '==', and '!='.(e.g. -l key1=value1,key2=value2)
      --show-managed-fields=false: If true, keep the managedFields when printing objects in JSON or YAML format.
      --template='': Template string or path to template file to use when -o=go-template, -o=go-template-file. The
template format is golang templates [http://golang.org/pkg/text/template/#pkg-overview].
      --timeout=0s: The length of time to wait before giving up on a scale operation, zero means don't wait. Any other
values should contain a corresponding time unit (e.g. 1s, 2m, 3h).

Usage:
  kubectl scale [--resource-version=version] [--current-replicas=count] --replicas=COUNT (-f FILENAME | TYPE NAME)
[options]

Use "kubectl options" for a list of global command-line options (applies to all commands).
]0;root@localhost:~[root@localhost ~]# 